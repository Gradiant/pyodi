{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Python Object Detection Insights A library for exploring your object detection dataset. The goal of this library is to provide simple and intuitive visualizations from your dataset and automatically find the best parameters for generating a specific grid of anchors that can fit you data characteristics Commands pyodi paint-annotations - Paint COCO format annotations and predictions pyodi ground-truth - Explore your dataset ground truth characteristics. pyodi evaluation - Evaluate the predictions of your model against your ground truth. pyodi train-config generation - Automatically generate a train_config_file using ground_truth_file . pyodi train-config evaluation - Evaluate the fitness between ground_truth_file and train_config_file .. pyodi coco merge - Automatically merge COCO annotation files. pyodi coco split - Creates a new dataset by splitting images into crops and adapting the annotations file pyodi crops split - Creates a new dataset by splitting images into crops and adapting the annotations file pyodi crops merge - Translate COCO ground truth or COCO predictions crops split into original image coordinates","title":"Home"},{"location":"#welcome-to-python-object-detection-insights","text":"A library for exploring your object detection dataset. The goal of this library is to provide simple and intuitive visualizations from your dataset and automatically find the best parameters for generating a specific grid of anchors that can fit you data characteristics","title":"Welcome to Python Object Detection Insights"},{"location":"#commands","text":"pyodi paint-annotations - Paint COCO format annotations and predictions pyodi ground-truth - Explore your dataset ground truth characteristics. pyodi evaluation - Evaluate the predictions of your model against your ground truth. pyodi train-config generation - Automatically generate a train_config_file using ground_truth_file . pyodi train-config evaluation - Evaluate the fitness between ground_truth_file and train_config_file .. pyodi coco merge - Automatically merge COCO annotation files. pyodi coco split - Creates a new dataset by splitting images into crops and adapting the annotations file pyodi crops split - Creates a new dataset by splitting images into crops and adapting the annotations file pyodi crops merge - Translate COCO ground truth or COCO predictions crops split into original image coordinates","title":"Commands"},{"location":"install/","text":"Installation From pypi pip install pyodi From source git clone https://github.com/Gradiant/pyodi.git cd pyodi/ pip install . # or \"python setup.py install\"","title":"Installation"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#from-pypi","text":"pip install pyodi","title":"From pypi"},{"location":"install/#from-source","text":"git clone https://github.com/Gradiant/pyodi.git cd pyodi/ pip install . # or \"python setup.py install\"","title":"From source"},{"location":"reference/apps/coco-merge/","text":"Coco Merge App. The pyodi coco app can be used to merge COCO annotation files. Example usage: pyodi coco merge coco_1.json coco_2.json output.json This app merges COCO annotation files by replacing original image and annotations ids with new ones and adding all existent categories. API REFERENCE coco_merge ( input_extend , input_add , output_file , indent = None ) Merge COCO annotation files. Parameters: Name Type Description Default input_extend str Path to input file to be extended. required input_add str Path to input file to be added. required output_file Path to output file with merged annotations. required indent Optional [ int ] Argument passed to json.dump . See https://docs.python.org/3/library/json.html#json.dump. None Source code in pyodi/apps/coco/coco_merge.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 @logger . catch ( reraise = True ) def coco_merge ( input_extend : str , input_add : str , output_file : str , indent : Optional [ int ] = None , ) -> str : \"\"\"Merge COCO annotation files. Args: input_extend: Path to input file to be extended. input_add: Path to input file to be added. output_file : Path to output file with merged annotations. indent: Argument passed to `json.dump`. See https://docs.python.org/3/library/json.html#json.dump. \"\"\" with open ( input_extend , \"r\" ) as f : data_extend = json . load ( f ) with open ( input_add , \"r\" ) as f : data_add = json . load ( f ) output : Dict [ str , Any ] = { k : data_extend [ k ] for k in data_extend if k not in ( \"images\" , \"annotations\" ) } output [ \"images\" ], output [ \"annotations\" ] = [], [] for i , data in enumerate ([ data_extend , data_add ]): logger . info ( \"Input {} : {} images, {} annotations\" . format ( i + 1 , len ( data [ \"images\" ]), len ( data [ \"annotations\" ]) ) ) cat_id_map = {} for new_cat in data [ \"categories\" ]: new_id = None for output_cat in output [ \"categories\" ]: if new_cat [ \"name\" ] == output_cat [ \"name\" ]: new_id = output_cat [ \"id\" ] break if new_id is not None : cat_id_map [ new_cat [ \"id\" ]] = new_id else : new_cat_id = max ( c [ \"id\" ] for c in output [ \"categories\" ]) + 1 cat_id_map [ new_cat [ \"id\" ]] = new_cat_id new_cat [ \"id\" ] = new_cat_id output [ \"categories\" ] . append ( new_cat ) img_id_map = {} for image in data [ \"images\" ]: n_imgs = len ( output [ \"images\" ]) img_id_map [ image [ \"id\" ]] = n_imgs image [ \"id\" ] = n_imgs output [ \"images\" ] . append ( image ) for annotation in data [ \"annotations\" ]: n_anns = len ( output [ \"annotations\" ]) annotation [ \"id\" ] = n_anns annotation [ \"image_id\" ] = img_id_map [ annotation [ \"image_id\" ]] annotation [ \"category_id\" ] = cat_id_map [ annotation [ \"category_id\" ]] output [ \"annotations\" ] . append ( annotation ) logger . info ( \"Result: {} images, {} annotations\" . format ( len ( output [ \"images\" ]), len ( output [ \"annotations\" ]) ) ) with open ( output_file , \"w\" ) as f : json . dump ( output , f , indent = indent ) return output_file","title":"merge"},{"location":"reference/apps/coco-merge/#pyodi.apps.coco.coco_merge--coco-merge-app","text":"The pyodi coco app can be used to merge COCO annotation files. Example usage: pyodi coco merge coco_1.json coco_2.json output.json This app merges COCO annotation files by replacing original image and annotations ids with new ones and adding all existent categories.","title":"Coco Merge App."},{"location":"reference/apps/coco-merge/#pyodi.apps.coco.coco_merge--api-reference","text":"","title":"API REFERENCE"},{"location":"reference/apps/coco-merge/#pyodi.apps.coco.coco_merge.coco_merge","text":"Merge COCO annotation files. Parameters: Name Type Description Default input_extend str Path to input file to be extended. required input_add str Path to input file to be added. required output_file Path to output file with merged annotations. required indent Optional [ int ] Argument passed to json.dump . See https://docs.python.org/3/library/json.html#json.dump. None Source code in pyodi/apps/coco/coco_merge.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 @logger . catch ( reraise = True ) def coco_merge ( input_extend : str , input_add : str , output_file : str , indent : Optional [ int ] = None , ) -> str : \"\"\"Merge COCO annotation files. Args: input_extend: Path to input file to be extended. input_add: Path to input file to be added. output_file : Path to output file with merged annotations. indent: Argument passed to `json.dump`. See https://docs.python.org/3/library/json.html#json.dump. \"\"\" with open ( input_extend , \"r\" ) as f : data_extend = json . load ( f ) with open ( input_add , \"r\" ) as f : data_add = json . load ( f ) output : Dict [ str , Any ] = { k : data_extend [ k ] for k in data_extend if k not in ( \"images\" , \"annotations\" ) } output [ \"images\" ], output [ \"annotations\" ] = [], [] for i , data in enumerate ([ data_extend , data_add ]): logger . info ( \"Input {} : {} images, {} annotations\" . format ( i + 1 , len ( data [ \"images\" ]), len ( data [ \"annotations\" ]) ) ) cat_id_map = {} for new_cat in data [ \"categories\" ]: new_id = None for output_cat in output [ \"categories\" ]: if new_cat [ \"name\" ] == output_cat [ \"name\" ]: new_id = output_cat [ \"id\" ] break if new_id is not None : cat_id_map [ new_cat [ \"id\" ]] = new_id else : new_cat_id = max ( c [ \"id\" ] for c in output [ \"categories\" ]) + 1 cat_id_map [ new_cat [ \"id\" ]] = new_cat_id new_cat [ \"id\" ] = new_cat_id output [ \"categories\" ] . append ( new_cat ) img_id_map = {} for image in data [ \"images\" ]: n_imgs = len ( output [ \"images\" ]) img_id_map [ image [ \"id\" ]] = n_imgs image [ \"id\" ] = n_imgs output [ \"images\" ] . append ( image ) for annotation in data [ \"annotations\" ]: n_anns = len ( output [ \"annotations\" ]) annotation [ \"id\" ] = n_anns annotation [ \"image_id\" ] = img_id_map [ annotation [ \"image_id\" ]] annotation [ \"category_id\" ] = cat_id_map [ annotation [ \"category_id\" ]] output [ \"annotations\" ] . append ( annotation ) logger . info ( \"Result: {} images, {} annotations\" . format ( len ( output [ \"images\" ]), len ( output [ \"annotations\" ]) ) ) with open ( output_file , \"w\" ) as f : json . dump ( output , f , indent = indent ) return output_file","title":"coco_merge()"},{"location":"reference/apps/coco-split/","text":"Coco Split App. The pyodi coco split app can be used to split COCO annotation files in train and val annotations files. There are two modes: 'random' or 'property'. The 'random' mode splits randomly the COCO file, while the 'property' mode allows to customize the split operation based in the properties of the COCO annotations file. Example usage: pyodi coco random-split ./coco.json ./random_coco_split --val-percentage 0 .1 pyodi coco property-split ./coco.json ./property_coco_split ./split_config.json The split config file is a json file that has 2 keys: 'discard' and 'val', both with dictionary values. The keys of the dictionaries will be the properties of the images that we want to match, and the values can be either the regex string to match or, for human readability, a dictionary with keys (you can choose whatever you want) and values (the regex string). Split config example: { \"discard\" : { \"file_name\" : \"people_video|crowd_video|multiple_people_video\" , \"source\" : \"Youtube People Dataset|Bad Dataset\" , }, \"val\" : { \"file_name\" : { \"My Val Ground Vehicle Dataset\" : \"val_car_video|val_bus_video|val_moto_video|val_bike_video\" , \"My Val Flying Vehicle Dataset\" : \"val_plane_video|val_drone_video|val_helicopter_video\" , }, \"source\" : \"Val Dataset\" , } } API REFERENCE property_split ( annotations_file , output_filename , split_config_file ) Split the annotations file in training and validation subsets by properties. Parameters: Name Type Description Default annotations_file str Path to annotations file. required output_filename str Output filename. required split_config_file str Path to configuration file. required Returns: Type Description List [ str ] Output filenames. Source code in pyodi/apps/coco/coco_split.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @logger . catch ( reraise = True ) # noqa: C901 def property_split ( annotations_file : str , output_filename : str , split_config_file : str , ) -> List [ str ]: \"\"\"Split the annotations file in training and validation subsets by properties. Args: annotations_file: Path to annotations file. output_filename: Output filename. split_config_file: Path to configuration file. Returns: Output filenames. \"\"\" logger . info ( \"Loading files...\" ) split_config = json . load ( open ( Path ( split_config_file ))) split_list = [] # Transform split_config from human readable format to a more code efficient format for section in split_config : # sections: val / discard for property_name , property_value in split_config [ section ] . items (): if isinstance ( property_value , dict ): property_value = \"|\" . join ( property_value . values ()) split_list . append ( dict ( split = section , property_name = property_name , property_regex = property_value , ) ) data = json . load ( open ( annotations_file )) train_images , val_images = [], [] train_annotations , val_annotations = [], [] n_train_imgs , n_val_imgs = 0 , 0 n_train_anns , n_val_anns = 0 , 0 old_to_new_train_ids = dict () old_to_new_val_ids = dict () logger . info ( \"Gathering images...\" ) for img in data [ \"images\" ]: i = 0 while i < len ( split_list ) and not re . match ( split_list [ i ][ \"property_regex\" ], img [ split_list [ i ][ \"property_name\" ]] ): i += 1 if i < len ( split_list ): # discard or val if split_list [ i ][ \"split\" ] == \"val\" : old_to_new_val_ids [ img [ \"id\" ]] = n_val_imgs img [ \"id\" ] = n_val_imgs val_images . append ( img ) n_val_imgs += 1 else : # train old_to_new_train_ids [ img [ \"id\" ]] = n_train_imgs img [ \"id\" ] = n_train_imgs train_images . append ( img ) n_train_imgs += 1 logger . info ( \"Gathering annotations...\" ) for ann in data [ \"annotations\" ]: if ann [ \"image_id\" ] in old_to_new_val_ids : ann [ \"image_id\" ] = old_to_new_val_ids [ ann [ \"image_id\" ]] ann [ \"id\" ] = n_val_anns val_annotations . append ( ann ) n_val_anns += 1 elif ann [ \"image_id\" ] in old_to_new_train_ids : ann [ \"image_id\" ] = old_to_new_train_ids [ ann [ \"image_id\" ]] ann [ \"id\" ] = n_train_anns train_annotations . append ( ann ) n_train_anns += 1 logger . info ( \"Spliting data...\" ) train_split = { \"images\" : train_images , \"annotations\" : train_annotations , \"info\" : data . get ( \"info\" , {}), \"licenses\" : data . get ( \"licenses\" , []), \"categories\" : data [ \"categories\" ], } val_split = { \"images\" : val_images , \"annotations\" : val_annotations , \"info\" : data . get ( \"info\" , {}), \"licenses\" : data . get ( \"licenses\" , []), \"categories\" : data [ \"categories\" ], } logger . info ( \"Writing splited files...\" ) output_files = [] for split_type , split in zip ([ \"train\" , \"val\" ], [ train_split , val_split ]): output_files . append ( output_filename + f \"_ { split_type } .json\" ) with open ( output_files [ - 1 ], \"w\" ) as f : json . dump ( split , f , indent = 2 ) return output_files random_split ( annotations_file , output_filename , val_percentage = 0.25 , seed = 47 ) Split the annotations file in training and validation subsets randomly. Parameters: Name Type Description Default annotations_file str Path to annotations file. required output_filename str Output filename. required val_percentage float Percentage of validation images. Defaults to 0.25. 0.25 seed int Seed for the random generator. Defaults to 47. 47 Returns: Type Description List [ str ] Output filenames. Source code in pyodi/apps/coco/coco_split.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 @logger . catch ( reraise = True ) def random_split ( annotations_file : str , output_filename : str , val_percentage : float = 0.25 , seed : int = 47 , ) -> List [ str ]: \"\"\"Split the annotations file in training and validation subsets randomly. Args: annotations_file: Path to annotations file. output_filename: Output filename. val_percentage: Percentage of validation images. Defaults to 0.25. seed: Seed for the random generator. Defaults to 47. Returns: Output filenames. \"\"\" data = json . load ( open ( annotations_file )) train_images , val_images , val_ids = [], [], [] np . random . seed ( seed ) rand_values = np . random . rand ( len ( data [ \"images\" ])) logger . info ( \"Gathering images...\" ) for i , image in enumerate ( data [ \"images\" ]): if rand_values [ i ] < val_percentage : val_images . append ( copy ( image )) val_ids . append ( image [ \"id\" ]) else : train_images . append ( copy ( image )) train_annotations , val_annotations = [], [] logger . info ( \"Gathering annotations...\" ) for annotation in data [ \"annotations\" ]: if annotation [ \"image_id\" ] in val_ids : val_annotations . append ( copy ( annotation )) else : train_annotations . append ( copy ( annotation )) train_split = { \"images\" : train_images , \"annotations\" : train_annotations , \"info\" : data . get ( \"info\" , {}), \"licenses\" : data . get ( \"licenses\" , []), \"categories\" : data [ \"categories\" ], } val_split = { \"images\" : val_images , \"annotations\" : val_annotations , \"info\" : data . get ( \"info\" , {}), \"licenses\" : data . get ( \"licenses\" , []), \"categories\" : data [ \"categories\" ], } logger . info ( \"Saving splits to file...\" ) output_files = [] for split_type , split in zip ([ \"train\" , \"val\" ], [ train_split , val_split ]): output_files . append ( output_filename + f \"_ { split_type } .json\" ) with open ( output_files [ - 1 ], \"w\" ) as f : json . dump ( split , f , indent = 2 ) return output_files","title":"split"},{"location":"reference/apps/coco-split/#pyodi.apps.coco.coco_split--coco-split-app","text":"The pyodi coco split app can be used to split COCO annotation files in train and val annotations files. There are two modes: 'random' or 'property'. The 'random' mode splits randomly the COCO file, while the 'property' mode allows to customize the split operation based in the properties of the COCO annotations file. Example usage: pyodi coco random-split ./coco.json ./random_coco_split --val-percentage 0 .1 pyodi coco property-split ./coco.json ./property_coco_split ./split_config.json The split config file is a json file that has 2 keys: 'discard' and 'val', both with dictionary values. The keys of the dictionaries will be the properties of the images that we want to match, and the values can be either the regex string to match or, for human readability, a dictionary with keys (you can choose whatever you want) and values (the regex string). Split config example: { \"discard\" : { \"file_name\" : \"people_video|crowd_video|multiple_people_video\" , \"source\" : \"Youtube People Dataset|Bad Dataset\" , }, \"val\" : { \"file_name\" : { \"My Val Ground Vehicle Dataset\" : \"val_car_video|val_bus_video|val_moto_video|val_bike_video\" , \"My Val Flying Vehicle Dataset\" : \"val_plane_video|val_drone_video|val_helicopter_video\" , }, \"source\" : \"Val Dataset\" , } }","title":"Coco Split App."},{"location":"reference/apps/coco-split/#pyodi.apps.coco.coco_split--api-reference","text":"","title":"API REFERENCE"},{"location":"reference/apps/coco-split/#pyodi.apps.coco.coco_split.property_split","text":"Split the annotations file in training and validation subsets by properties. Parameters: Name Type Description Default annotations_file str Path to annotations file. required output_filename str Output filename. required split_config_file str Path to configuration file. required Returns: Type Description List [ str ] Output filenames. Source code in pyodi/apps/coco/coco_split.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @logger . catch ( reraise = True ) # noqa: C901 def property_split ( annotations_file : str , output_filename : str , split_config_file : str , ) -> List [ str ]: \"\"\"Split the annotations file in training and validation subsets by properties. Args: annotations_file: Path to annotations file. output_filename: Output filename. split_config_file: Path to configuration file. Returns: Output filenames. \"\"\" logger . info ( \"Loading files...\" ) split_config = json . load ( open ( Path ( split_config_file ))) split_list = [] # Transform split_config from human readable format to a more code efficient format for section in split_config : # sections: val / discard for property_name , property_value in split_config [ section ] . items (): if isinstance ( property_value , dict ): property_value = \"|\" . join ( property_value . values ()) split_list . append ( dict ( split = section , property_name = property_name , property_regex = property_value , ) ) data = json . load ( open ( annotations_file )) train_images , val_images = [], [] train_annotations , val_annotations = [], [] n_train_imgs , n_val_imgs = 0 , 0 n_train_anns , n_val_anns = 0 , 0 old_to_new_train_ids = dict () old_to_new_val_ids = dict () logger . info ( \"Gathering images...\" ) for img in data [ \"images\" ]: i = 0 while i < len ( split_list ) and not re . match ( split_list [ i ][ \"property_regex\" ], img [ split_list [ i ][ \"property_name\" ]] ): i += 1 if i < len ( split_list ): # discard or val if split_list [ i ][ \"split\" ] == \"val\" : old_to_new_val_ids [ img [ \"id\" ]] = n_val_imgs img [ \"id\" ] = n_val_imgs val_images . append ( img ) n_val_imgs += 1 else : # train old_to_new_train_ids [ img [ \"id\" ]] = n_train_imgs img [ \"id\" ] = n_train_imgs train_images . append ( img ) n_train_imgs += 1 logger . info ( \"Gathering annotations...\" ) for ann in data [ \"annotations\" ]: if ann [ \"image_id\" ] in old_to_new_val_ids : ann [ \"image_id\" ] = old_to_new_val_ids [ ann [ \"image_id\" ]] ann [ \"id\" ] = n_val_anns val_annotations . append ( ann ) n_val_anns += 1 elif ann [ \"image_id\" ] in old_to_new_train_ids : ann [ \"image_id\" ] = old_to_new_train_ids [ ann [ \"image_id\" ]] ann [ \"id\" ] = n_train_anns train_annotations . append ( ann ) n_train_anns += 1 logger . info ( \"Spliting data...\" ) train_split = { \"images\" : train_images , \"annotations\" : train_annotations , \"info\" : data . get ( \"info\" , {}), \"licenses\" : data . get ( \"licenses\" , []), \"categories\" : data [ \"categories\" ], } val_split = { \"images\" : val_images , \"annotations\" : val_annotations , \"info\" : data . get ( \"info\" , {}), \"licenses\" : data . get ( \"licenses\" , []), \"categories\" : data [ \"categories\" ], } logger . info ( \"Writing splited files...\" ) output_files = [] for split_type , split in zip ([ \"train\" , \"val\" ], [ train_split , val_split ]): output_files . append ( output_filename + f \"_ { split_type } .json\" ) with open ( output_files [ - 1 ], \"w\" ) as f : json . dump ( split , f , indent = 2 ) return output_files","title":"property_split()"},{"location":"reference/apps/coco-split/#pyodi.apps.coco.coco_split.random_split","text":"Split the annotations file in training and validation subsets randomly. Parameters: Name Type Description Default annotations_file str Path to annotations file. required output_filename str Output filename. required val_percentage float Percentage of validation images. Defaults to 0.25. 0.25 seed int Seed for the random generator. Defaults to 47. 47 Returns: Type Description List [ str ] Output filenames. Source code in pyodi/apps/coco/coco_split.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 @logger . catch ( reraise = True ) def random_split ( annotations_file : str , output_filename : str , val_percentage : float = 0.25 , seed : int = 47 , ) -> List [ str ]: \"\"\"Split the annotations file in training and validation subsets randomly. Args: annotations_file: Path to annotations file. output_filename: Output filename. val_percentage: Percentage of validation images. Defaults to 0.25. seed: Seed for the random generator. Defaults to 47. Returns: Output filenames. \"\"\" data = json . load ( open ( annotations_file )) train_images , val_images , val_ids = [], [], [] np . random . seed ( seed ) rand_values = np . random . rand ( len ( data [ \"images\" ])) logger . info ( \"Gathering images...\" ) for i , image in enumerate ( data [ \"images\" ]): if rand_values [ i ] < val_percentage : val_images . append ( copy ( image )) val_ids . append ( image [ \"id\" ]) else : train_images . append ( copy ( image )) train_annotations , val_annotations = [], [] logger . info ( \"Gathering annotations...\" ) for annotation in data [ \"annotations\" ]: if annotation [ \"image_id\" ] in val_ids : val_annotations . append ( copy ( annotation )) else : train_annotations . append ( copy ( annotation )) train_split = { \"images\" : train_images , \"annotations\" : train_annotations , \"info\" : data . get ( \"info\" , {}), \"licenses\" : data . get ( \"licenses\" , []), \"categories\" : data [ \"categories\" ], } val_split = { \"images\" : val_images , \"annotations\" : val_annotations , \"info\" : data . get ( \"info\" , {}), \"licenses\" : data . get ( \"licenses\" , []), \"categories\" : data [ \"categories\" ], } logger . info ( \"Saving splits to file...\" ) output_files = [] for split_type , split in zip ([ \"train\" , \"val\" ], [ train_split , val_split ]): output_files . append ( output_filename + f \"_ { split_type } .json\" ) with open ( output_files [ - 1 ], \"w\" ) as f : json . dump ( split , f , indent = 2 ) return output_files","title":"random_split()"},{"location":"reference/apps/crops-merge/","text":"Crops Merge App. API REFERENCE crops_merge ( ground_truth_file , output_file , predictions_file = None , apply_nms = True , score_thr = 0.0 , iou_thr = 0.5 ) Merge and translate ground_truth_file or predictions to ground_truth 's old_images coordinates. Parameters: Name Type Description Default ground_truth_file str Path to COCO ground truth file of crops. Generated with crops_split . required output_file str Path where the merged annotations will be saved. required predictions_file Optional [ str ] Path to COCO predictions file over ground_truth_file . If not None, the annotations of predictions_file will be merged instead of ground_truth_file's. None apply_nms bool Whether to apply Non Maximum Supression to the merged predictions of each image. Defaults to True. True score_thr float Predictions bellow score_thr will be filtered. Only used if apply_nms . Defaults to 0.0. 0.0 iou_thr float None of the filtered predictions will have an iou above iou_thr to any other. Only used if apply_nms . Defaults to 0.5. 0.5 Source code in pyodi/apps/crops/crops_merge.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 @logger . catch ( reraise = True ) def crops_merge ( ground_truth_file : str , output_file : str , predictions_file : Optional [ str ] = None , apply_nms : bool = True , score_thr : float = 0.0 , iou_thr : float = 0.5 , ) -> str : \"\"\"Merge and translate `ground_truth_file` or `predictions` to `ground_truth`'s `old_images` coordinates. Args: ground_truth_file: Path to COCO ground truth file of crops. Generated with `crops_split`. output_file: Path where the merged annotations will be saved. predictions_file: Path to COCO predictions file over `ground_truth_file`. If not None, the annotations of predictions_file will be merged instead of ground_truth_file's. apply_nms: Whether to apply Non Maximum Supression to the merged predictions of each image. Defaults to True. score_thr: Predictions bellow `score_thr` will be filtered. Only used if `apply_nms`. Defaults to 0.0. iou_thr: None of the filtered predictions will have an iou above `iou_thr` to any other. Only used if `apply_nms`. Defaults to 0.5. \"\"\" ground_truth = json . load ( open ( ground_truth_file )) crop_id_to_filename = { x [ \"id\" ]: x [ \"file_name\" ] for x in ground_truth [ \"images\" ]} stem_to_original_id = { Path ( x [ \"file_name\" ]) . stem : x [ \"id\" ] for x in ground_truth [ \"old_images\" ] } stem_to_original_shape = { Path ( x [ \"file_name\" ]) . stem : ( x [ \"width\" ], x [ \"height\" ]) for x in ground_truth [ \"old_images\" ] } if predictions_file is not None : annotations = json . load ( open ( predictions_file )) else : annotations = ground_truth [ \"annotations\" ] for n , crop in enumerate ( annotations ): if not n % 10 : logger . info ( n ) filename = crop_id_to_filename [ crop [ \"image_id\" ]] parts = Path ( filename ) . stem . split ( \"_\" ) stem = \"_\" . join ( parts [: - 2 ]) original_id = stem_to_original_id [ stem ] crop [ \"image_id\" ] = original_id # Corners are encoded in crop's filename # See crops_split.py crop_row = int ( parts [ - 1 ]) crop_col = int ( parts [ - 2 ]) crop [ \"bbox\" ][ 0 ] += crop_col crop [ \"bbox\" ][ 1 ] += crop_row crop [ \"original_image_shape\" ] = stem_to_original_shape [ stem ] if apply_nms : annotations = nms_predictions ( annotations , score_thr = score_thr , iou_thr = iou_thr ) output_file = str ( Path ( output_file ) . parent / f \" { Path ( output_file ) . stem } _ { score_thr } _ { iou_thr } .json\" ) if predictions_file is not None : new_ground_truth = annotations else : new_ground_truth = { \"info\" : ground_truth . get ( \"info\" , []), \"licenses\" : ground_truth . get ( \"licenses\" , []), \"categories\" : ground_truth . get ( \"categories\" , []), \"images\" : ground_truth . get ( \"old_images\" ), \"annotations\" : annotations , } with open ( output_file , \"w\" ) as f : json . dump ( new_ground_truth , f , indent = 2 ) return output_file","title":"merge"},{"location":"reference/apps/crops-merge/#pyodi.apps.crops.crops_merge--crops-merge-app","text":"","title":"Crops Merge App."},{"location":"reference/apps/crops-merge/#pyodi.apps.crops.crops_merge--api-reference","text":"","title":"API REFERENCE"},{"location":"reference/apps/crops-merge/#pyodi.apps.crops.crops_merge.crops_merge","text":"Merge and translate ground_truth_file or predictions to ground_truth 's old_images coordinates. Parameters: Name Type Description Default ground_truth_file str Path to COCO ground truth file of crops. Generated with crops_split . required output_file str Path where the merged annotations will be saved. required predictions_file Optional [ str ] Path to COCO predictions file over ground_truth_file . If not None, the annotations of predictions_file will be merged instead of ground_truth_file's. None apply_nms bool Whether to apply Non Maximum Supression to the merged predictions of each image. Defaults to True. True score_thr float Predictions bellow score_thr will be filtered. Only used if apply_nms . Defaults to 0.0. 0.0 iou_thr float None of the filtered predictions will have an iou above iou_thr to any other. Only used if apply_nms . Defaults to 0.5. 0.5 Source code in pyodi/apps/crops/crops_merge.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 @logger . catch ( reraise = True ) def crops_merge ( ground_truth_file : str , output_file : str , predictions_file : Optional [ str ] = None , apply_nms : bool = True , score_thr : float = 0.0 , iou_thr : float = 0.5 , ) -> str : \"\"\"Merge and translate `ground_truth_file` or `predictions` to `ground_truth`'s `old_images` coordinates. Args: ground_truth_file: Path to COCO ground truth file of crops. Generated with `crops_split`. output_file: Path where the merged annotations will be saved. predictions_file: Path to COCO predictions file over `ground_truth_file`. If not None, the annotations of predictions_file will be merged instead of ground_truth_file's. apply_nms: Whether to apply Non Maximum Supression to the merged predictions of each image. Defaults to True. score_thr: Predictions bellow `score_thr` will be filtered. Only used if `apply_nms`. Defaults to 0.0. iou_thr: None of the filtered predictions will have an iou above `iou_thr` to any other. Only used if `apply_nms`. Defaults to 0.5. \"\"\" ground_truth = json . load ( open ( ground_truth_file )) crop_id_to_filename = { x [ \"id\" ]: x [ \"file_name\" ] for x in ground_truth [ \"images\" ]} stem_to_original_id = { Path ( x [ \"file_name\" ]) . stem : x [ \"id\" ] for x in ground_truth [ \"old_images\" ] } stem_to_original_shape = { Path ( x [ \"file_name\" ]) . stem : ( x [ \"width\" ], x [ \"height\" ]) for x in ground_truth [ \"old_images\" ] } if predictions_file is not None : annotations = json . load ( open ( predictions_file )) else : annotations = ground_truth [ \"annotations\" ] for n , crop in enumerate ( annotations ): if not n % 10 : logger . info ( n ) filename = crop_id_to_filename [ crop [ \"image_id\" ]] parts = Path ( filename ) . stem . split ( \"_\" ) stem = \"_\" . join ( parts [: - 2 ]) original_id = stem_to_original_id [ stem ] crop [ \"image_id\" ] = original_id # Corners are encoded in crop's filename # See crops_split.py crop_row = int ( parts [ - 1 ]) crop_col = int ( parts [ - 2 ]) crop [ \"bbox\" ][ 0 ] += crop_col crop [ \"bbox\" ][ 1 ] += crop_row crop [ \"original_image_shape\" ] = stem_to_original_shape [ stem ] if apply_nms : annotations = nms_predictions ( annotations , score_thr = score_thr , iou_thr = iou_thr ) output_file = str ( Path ( output_file ) . parent / f \" { Path ( output_file ) . stem } _ { score_thr } _ { iou_thr } .json\" ) if predictions_file is not None : new_ground_truth = annotations else : new_ground_truth = { \"info\" : ground_truth . get ( \"info\" , []), \"licenses\" : ground_truth . get ( \"licenses\" , []), \"categories\" : ground_truth . get ( \"categories\" , []), \"images\" : ground_truth . get ( \"old_images\" ), \"annotations\" : annotations , } with open ( output_file , \"w\" ) as f : json . dump ( new_ground_truth , f , indent = 2 ) return output_file","title":"crops_merge()"},{"location":"reference/apps/crops-split/","text":"Crops Split App. API REFERENCE crops_split ( ground_truth_file , image_folder , output_file , output_image_folder , crop_height , crop_width , row_overlap = 0 , col_overlap = 0 , min_area_threshold = 0.0 ) Creates new dataset by splitting images into crops and adapting the annotations. Parameters: Name Type Description Default ground_truth_file str Path to a COCO ground_truth_file. required image_folder str Path where the images of the ground_truth_file are stored. required output_file str Path where the new_ground_truth_file will be saved. required output_image_folder str Path where the crops will be saved. required crop_height int Crop height. required crop_width int Crop width required row_overlap int Row overlap. Defaults to 0. 0 col_overlap int Column overlap. Defaults to 0. 0 min_area_threshold float Minimum area threshold ratio. If the cropped annotation area is smaller than the threshold, the annotation is filtered out. Defaults to 0. 0.0 Source code in pyodi/apps/crops/crops_split.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @logger . catch ( reraise = True ) def crops_split ( ground_truth_file : str , image_folder : str , output_file : str , output_image_folder : str , crop_height : int , crop_width : int , row_overlap : int = 0 , col_overlap : int = 0 , min_area_threshold : float = 0.0 , ) -> None : \"\"\"Creates new dataset by splitting images into crops and adapting the annotations. Args: ground_truth_file: Path to a COCO ground_truth_file. image_folder: Path where the images of the ground_truth_file are stored. output_file: Path where the `new_ground_truth_file` will be saved. output_image_folder: Path where the crops will be saved. crop_height: Crop height. crop_width: Crop width row_overlap: Row overlap. Defaults to 0. col_overlap: Column overlap. Defaults to 0. min_area_threshold: Minimum area threshold ratio. If the cropped annotation area is smaller than the threshold, the annotation is filtered out. Defaults to 0. \"\"\" ground_truth = json . load ( open ( ground_truth_file )) image_id_to_annotations : Dict = defaultdict ( list ) for annotation in ground_truth [ \"annotations\" ]: image_id_to_annotations [ annotation [ \"image_id\" ]] . append ( annotation ) output_image_folder_path = Path ( output_image_folder ) output_image_folder_path . mkdir ( exist_ok = True , parents = True ) new_images : List = [] new_annotations : List = [] for image in ground_truth [ \"images\" ]: file_name = Path ( image [ \"file_name\" ]) logger . info ( file_name ) image_pil = Image . open ( Path ( image_folder ) / file_name . name ) crops_corners = get_crops_corners ( image_pil , crop_height , crop_width , row_overlap , col_overlap ) for crop_corners in crops_corners : logger . info ( crop_corners ) crop = image_pil . crop ( crop_corners ) crop_suffixes = \"_\" . join ( map ( str , crop_corners )) crop_file_name = f \" { file_name . stem } _ { crop_suffixes }{ file_name . suffix } \" crop . save ( output_image_folder_path / crop_file_name ) crop_id = len ( new_images ) new_images . append ( { \"file_name\" : Path ( crop_file_name ) . name , \"height\" : int ( crop_height ), \"width\" : int ( crop_width ), \"id\" : int ( crop_id ), } ) for annotation in image_id_to_annotations [ image [ \"id\" ]]: if not annotation_inside_crop ( annotation , crop_corners ): continue new_annotation = get_annotation_in_crop ( annotation , crop_corners ) if filter_annotation_by_area ( annotation , new_annotation , min_area_threshold ): continue new_annotation [ \"id\" ] = len ( new_annotations ) new_annotation [ \"image_id\" ] = crop_id new_annotations . append ( new_annotation ) new_ground_truth = { \"images\" : new_images , \"old_images\" : ground_truth [ \"images\" ], \"annotations\" : new_annotations , \"categories\" : ground_truth [ \"categories\" ], \"licenses\" : ground_truth . get ( \"licenses\" , []), \"info\" : ground_truth . get ( \"info\" ), } Path ( output_file ) . parent . mkdir ( exist_ok = True , parents = True ) with open ( output_file , \"w\" ) as f : json . dump ( new_ground_truth , f , indent = 2 )","title":"split"},{"location":"reference/apps/crops-split/#pyodi.apps.crops.crops_split--crops-split-app","text":"","title":"Crops Split App."},{"location":"reference/apps/crops-split/#pyodi.apps.crops.crops_split--api-reference","text":"","title":"API REFERENCE"},{"location":"reference/apps/crops-split/#pyodi.apps.crops.crops_split.crops_split","text":"Creates new dataset by splitting images into crops and adapting the annotations. Parameters: Name Type Description Default ground_truth_file str Path to a COCO ground_truth_file. required image_folder str Path where the images of the ground_truth_file are stored. required output_file str Path where the new_ground_truth_file will be saved. required output_image_folder str Path where the crops will be saved. required crop_height int Crop height. required crop_width int Crop width required row_overlap int Row overlap. Defaults to 0. 0 col_overlap int Column overlap. Defaults to 0. 0 min_area_threshold float Minimum area threshold ratio. If the cropped annotation area is smaller than the threshold, the annotation is filtered out. Defaults to 0. 0.0 Source code in pyodi/apps/crops/crops_split.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @logger . catch ( reraise = True ) def crops_split ( ground_truth_file : str , image_folder : str , output_file : str , output_image_folder : str , crop_height : int , crop_width : int , row_overlap : int = 0 , col_overlap : int = 0 , min_area_threshold : float = 0.0 , ) -> None : \"\"\"Creates new dataset by splitting images into crops and adapting the annotations. Args: ground_truth_file: Path to a COCO ground_truth_file. image_folder: Path where the images of the ground_truth_file are stored. output_file: Path where the `new_ground_truth_file` will be saved. output_image_folder: Path where the crops will be saved. crop_height: Crop height. crop_width: Crop width row_overlap: Row overlap. Defaults to 0. col_overlap: Column overlap. Defaults to 0. min_area_threshold: Minimum area threshold ratio. If the cropped annotation area is smaller than the threshold, the annotation is filtered out. Defaults to 0. \"\"\" ground_truth = json . load ( open ( ground_truth_file )) image_id_to_annotations : Dict = defaultdict ( list ) for annotation in ground_truth [ \"annotations\" ]: image_id_to_annotations [ annotation [ \"image_id\" ]] . append ( annotation ) output_image_folder_path = Path ( output_image_folder ) output_image_folder_path . mkdir ( exist_ok = True , parents = True ) new_images : List = [] new_annotations : List = [] for image in ground_truth [ \"images\" ]: file_name = Path ( image [ \"file_name\" ]) logger . info ( file_name ) image_pil = Image . open ( Path ( image_folder ) / file_name . name ) crops_corners = get_crops_corners ( image_pil , crop_height , crop_width , row_overlap , col_overlap ) for crop_corners in crops_corners : logger . info ( crop_corners ) crop = image_pil . crop ( crop_corners ) crop_suffixes = \"_\" . join ( map ( str , crop_corners )) crop_file_name = f \" { file_name . stem } _ { crop_suffixes }{ file_name . suffix } \" crop . save ( output_image_folder_path / crop_file_name ) crop_id = len ( new_images ) new_images . append ( { \"file_name\" : Path ( crop_file_name ) . name , \"height\" : int ( crop_height ), \"width\" : int ( crop_width ), \"id\" : int ( crop_id ), } ) for annotation in image_id_to_annotations [ image [ \"id\" ]]: if not annotation_inside_crop ( annotation , crop_corners ): continue new_annotation = get_annotation_in_crop ( annotation , crop_corners ) if filter_annotation_by_area ( annotation , new_annotation , min_area_threshold ): continue new_annotation [ \"id\" ] = len ( new_annotations ) new_annotation [ \"image_id\" ] = crop_id new_annotations . append ( new_annotation ) new_ground_truth = { \"images\" : new_images , \"old_images\" : ground_truth [ \"images\" ], \"annotations\" : new_annotations , \"categories\" : ground_truth [ \"categories\" ], \"licenses\" : ground_truth . get ( \"licenses\" , []), \"info\" : ground_truth . get ( \"info\" ), } Path ( output_file ) . parent . mkdir ( exist_ok = True , parents = True ) with open ( output_file , \"w\" ) as f : json . dump ( new_ground_truth , f , indent = 2 )","title":"crops_split()"},{"location":"reference/apps/evaluation/","text":"Evaluation App. The pyodi evaluation app can be used to evaluate the predictions of an object detection dataset. Example usage: pyodi evaluation \"data/COCO/COCO_val2017.json\" \"data/COCO/COCO_val2017_predictions.json\" This app shows the Average Precision for different IoU values and different areas, the Average Recall for different IoU values and differents maximum detections. An example of the result of executing this app: Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.256 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.438 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.263 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.422 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.239 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.353 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.375 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.122 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.586 API REFERENCE evaluation ( ground_truth_file , predictions_file , string_to_match = None ) Evaluate the predictions of a dataset. Parameters: Name Type Description Default ground_truth_file str Path to COCO ground truth file. required predictions_file str Path to COCO predictions file. required string_to_match Optional [ str ] If not None, only images whose file_name match this parameter will be evaluated. None Source code in pyodi/apps/evaluation.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @logger . catch ( reraise = True ) def evaluation ( ground_truth_file : str , predictions_file : str , string_to_match : Optional [ str ] = None ) -> None : \"\"\"Evaluate the predictions of a dataset. Args: ground_truth_file: Path to COCO ground truth file. predictions_file: Path to COCO predictions file. string_to_match: If not None, only images whose file_name match this parameter will be evaluated. \"\"\" with open ( ground_truth_file ) as gt : coco_ground_truth = load_coco_ground_truth_from_StringIO ( gt ) with open ( predictions_file ) as pred : coco_predictions = coco_ground_truth . loadRes ( json . load ( pred )) coco_eval = COCOeval ( coco_ground_truth , coco_predictions , \"bbox\" ) if string_to_match is not None : filtered_ids = [ k for k , v in coco_ground_truth . imgs . items () if re . match ( string_to_match , v [ \"file_name\" ]) ] logger . info ( \"Number of filtered_ids: {} \" . format ( len ( filtered_ids ))) else : filtered_ids = [ k for k in coco_ground_truth . imgs . keys ()] coco_eval . image_ids = filtered_ids coco_eval . evaluate () coco_eval . accumulate () coco_eval . summarize ()","title":"evaluation"},{"location":"reference/apps/evaluation/#pyodi.apps.evaluation--evaluation-app","text":"The pyodi evaluation app can be used to evaluate the predictions of an object detection dataset. Example usage: pyodi evaluation \"data/COCO/COCO_val2017.json\" \"data/COCO/COCO_val2017_predictions.json\" This app shows the Average Precision for different IoU values and different areas, the Average Recall for different IoU values and differents maximum detections. An example of the result of executing this app: Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.256 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.438 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.263 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.422 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.239 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.353 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.375 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.122 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.586","title":"Evaluation App."},{"location":"reference/apps/evaluation/#pyodi.apps.evaluation--api-reference","text":"","title":"API REFERENCE"},{"location":"reference/apps/evaluation/#pyodi.apps.evaluation.evaluation","text":"Evaluate the predictions of a dataset. Parameters: Name Type Description Default ground_truth_file str Path to COCO ground truth file. required predictions_file str Path to COCO predictions file. required string_to_match Optional [ str ] If not None, only images whose file_name match this parameter will be evaluated. None Source code in pyodi/apps/evaluation.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @logger . catch ( reraise = True ) def evaluation ( ground_truth_file : str , predictions_file : str , string_to_match : Optional [ str ] = None ) -> None : \"\"\"Evaluate the predictions of a dataset. Args: ground_truth_file: Path to COCO ground truth file. predictions_file: Path to COCO predictions file. string_to_match: If not None, only images whose file_name match this parameter will be evaluated. \"\"\" with open ( ground_truth_file ) as gt : coco_ground_truth = load_coco_ground_truth_from_StringIO ( gt ) with open ( predictions_file ) as pred : coco_predictions = coco_ground_truth . loadRes ( json . load ( pred )) coco_eval = COCOeval ( coco_ground_truth , coco_predictions , \"bbox\" ) if string_to_match is not None : filtered_ids = [ k for k , v in coco_ground_truth . imgs . items () if re . match ( string_to_match , v [ \"file_name\" ]) ] logger . info ( \"Number of filtered_ids: {} \" . format ( len ( filtered_ids ))) else : filtered_ids = [ k for k in coco_ground_truth . imgs . keys ()] coco_eval . image_ids = filtered_ids coco_eval . evaluate () coco_eval . accumulate () coco_eval . summarize ()","title":"evaluation()"},{"location":"reference/apps/ground-truth/","text":"Ground Truth App. The pyodi ground-truth app can be used to explore the images and bounding boxes that compose an object detection dataset. The shape distribution of the images and bounding boxes and their locations are the key aspects to take in account when setting your training configuration. Example usage: pyodi ground-truth \\\\ $TINY_COCO_ANIMAL /annotations/train.json The app is divided in three different sections: Images shape distribution Shows information related with the shape of the images present in the dataset. In this case we can clearly identify two main patterns in this dataset and if we have a look at the histogram, we can see how most of images have 640 pixels width, while as height is more distributed between different values. Bounding Boxes shape distribution We observe bounding box distribution, with the possibility of enabling filters by class or sets of classes. This dataset shows a tendency to rectangular bounding boxes with larger width than height and where most of them embrace areas below the 20% of the total image. Bounding Boxes center locations It is possible to check where centers of bounding boxes are most commonly found with respect to the image. This can help us distinguish ROIs in input images. In this case we observe that the objects usually appear in the center of the image. API REFERENCE ground_truth ( ground_truth_file , show = True , output = None , output_size = ( 1600 , 900 )) Explore the images and bounding boxes of a dataset. Parameters: Name Type Description Default ground_truth_file str Path to COCO ground truth file. required show bool Whether to show results or not. Defaults to True. True output Optional [ str ] Results will be saved under output dir. Defaults to None. None output_size Tuple [ int , int ] Size of the saved images when output is defined. Defaults to (1600, 900). (1600, 900) Source code in pyodi/apps/ground_truth.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def ground_truth ( ground_truth_file : str , show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), ) -> None : \"\"\"Explore the images and bounding boxes of a dataset. Args: ground_truth_file: Path to COCO ground truth file. show: Whether to show results or not. Defaults to True. output: Results will be saved under `output` dir. Defaults to None. output_size: Size of the saved images when output is defined. Defaults to (1600, 900). \"\"\" if output is not None : output = str ( Path ( output ) / Path ( ground_truth_file ) . stem ) Path ( output ) . mkdir ( parents = True , exist_ok = True ) df_annotations = coco_ground_truth_to_df ( ground_truth_file ) df_images = df_annotations . loc [ :, df_annotations . columns . str . startswith ( \"img_\" ) ] . drop_duplicates () plot_scatter_with_histograms ( df_images , x = \"img_width\" , y = \"img_height\" , title = \"Image_Shapes\" , show = show , output = output , output_size = output_size , histogram_xbins = dict ( size = 10 ), histogram_ybins = dict ( size = 10 ), ) df_annotations = add_centroids ( df_annotations ) df_annotations [ \"absolute_height\" ] = ( df_annotations [ \"height\" ] / df_annotations [ \"img_height\" ] ) df_annotations [ \"absolute_width\" ] = ( df_annotations [ \"width\" ] / df_annotations [ \"img_width\" ] ) plot_scatter_with_histograms ( df_annotations , x = \"absolute_width\" , y = \"absolute_height\" , title = \"Bounding_Box_Shapes\" , show = show , output = output , output_size = output_size , xaxis_range = ( - 0.01 , 1.01 ), yaxis_range = ( - 0.01 , 1.01 ), histogram_xbins = dict ( size = 0.05 ), histogram_ybins = dict ( size = 0.05 ), ) plot_heatmap ( get_centroids_heatmap ( df_annotations ), title = \"Bounding_Box_Centers\" , show = show , output = output , output_size = output_size , )","title":"ground-truth"},{"location":"reference/apps/ground-truth/#pyodi.apps.ground_truth--ground-truth-app","text":"The pyodi ground-truth app can be used to explore the images and bounding boxes that compose an object detection dataset. The shape distribution of the images and bounding boxes and their locations are the key aspects to take in account when setting your training configuration. Example usage: pyodi ground-truth \\\\ $TINY_COCO_ANIMAL /annotations/train.json The app is divided in three different sections:","title":"Ground Truth App."},{"location":"reference/apps/ground-truth/#pyodi.apps.ground_truth--images-shape-distribution","text":"Shows information related with the shape of the images present in the dataset. In this case we can clearly identify two main patterns in this dataset and if we have a look at the histogram, we can see how most of images have 640 pixels width, while as height is more distributed between different values.","title":"Images shape distribution"},{"location":"reference/apps/ground-truth/#pyodi.apps.ground_truth--bounding-boxes-shape-distribution","text":"We observe bounding box distribution, with the possibility of enabling filters by class or sets of classes. This dataset shows a tendency to rectangular bounding boxes with larger width than height and where most of them embrace areas below the 20% of the total image.","title":"Bounding Boxes shape distribution"},{"location":"reference/apps/ground-truth/#pyodi.apps.ground_truth--bounding-boxes-center-locations","text":"It is possible to check where centers of bounding boxes are most commonly found with respect to the image. This can help us distinguish ROIs in input images. In this case we observe that the objects usually appear in the center of the image.","title":"Bounding Boxes center locations"},{"location":"reference/apps/ground-truth/#pyodi.apps.ground_truth--api-reference","text":"","title":"API REFERENCE"},{"location":"reference/apps/ground-truth/#pyodi.apps.ground_truth.ground_truth","text":"Explore the images and bounding boxes of a dataset. Parameters: Name Type Description Default ground_truth_file str Path to COCO ground truth file. required show bool Whether to show results or not. Defaults to True. True output Optional [ str ] Results will be saved under output dir. Defaults to None. None output_size Tuple [ int , int ] Size of the saved images when output is defined. Defaults to (1600, 900). (1600, 900) Source code in pyodi/apps/ground_truth.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def ground_truth ( ground_truth_file : str , show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), ) -> None : \"\"\"Explore the images and bounding boxes of a dataset. Args: ground_truth_file: Path to COCO ground truth file. show: Whether to show results or not. Defaults to True. output: Results will be saved under `output` dir. Defaults to None. output_size: Size of the saved images when output is defined. Defaults to (1600, 900). \"\"\" if output is not None : output = str ( Path ( output ) / Path ( ground_truth_file ) . stem ) Path ( output ) . mkdir ( parents = True , exist_ok = True ) df_annotations = coco_ground_truth_to_df ( ground_truth_file ) df_images = df_annotations . loc [ :, df_annotations . columns . str . startswith ( \"img_\" ) ] . drop_duplicates () plot_scatter_with_histograms ( df_images , x = \"img_width\" , y = \"img_height\" , title = \"Image_Shapes\" , show = show , output = output , output_size = output_size , histogram_xbins = dict ( size = 10 ), histogram_ybins = dict ( size = 10 ), ) df_annotations = add_centroids ( df_annotations ) df_annotations [ \"absolute_height\" ] = ( df_annotations [ \"height\" ] / df_annotations [ \"img_height\" ] ) df_annotations [ \"absolute_width\" ] = ( df_annotations [ \"width\" ] / df_annotations [ \"img_width\" ] ) plot_scatter_with_histograms ( df_annotations , x = \"absolute_width\" , y = \"absolute_height\" , title = \"Bounding_Box_Shapes\" , show = show , output = output , output_size = output_size , xaxis_range = ( - 0.01 , 1.01 ), yaxis_range = ( - 0.01 , 1.01 ), histogram_xbins = dict ( size = 0.05 ), histogram_ybins = dict ( size = 0.05 ), ) plot_heatmap ( get_centroids_heatmap ( df_annotations ), title = \"Bounding_Box_Centers\" , show = show , output = output , output_size = output_size , )","title":"ground_truth()"},{"location":"reference/apps/paint-annotations/","text":"Paint Annotations App. The pyodi paint-annotations helps you to easily visualize in a beautiful format your object detection dataset. You can also use this function to visualize model predictions if they are in COCO predictions format. Example usage: pyodi paint-annotations \\\\ $TINY_COCO_ANIMAL /annotations/train.json \\\\ $TINY_COCO_ANIMAL /sample_images/ \\\\ $TINY_COCO_ANIMAL /painted_images/ API REFERENCE paint_annotations ( ground_truth_file , image_folder , output_folder , predictions_file = None , score_thr = 0.0 , color_key = 'category_id' , show_label = True , filter_crowd = True , first_n = None , use_exif_orientation = False ) Paint ground_truth_file or predictions_file annotations on image_folder images. Parameters: Name Type Description Default ground_truth_file str Path to COCO ground truth file. required image_folder str Path to root folder where the images of ground_truth_file are. required output_folder str Path to the folder where painted images will be saved. It will be created if it does not exist. required predictions_file Optional [ str ] Path to COCO predictions file. If not None, the annotations of predictions_file will be painted instead of ground_truth_file's. None score_thr float Detections bellow this threshold will not be painted. Default 0.0. 0.0 color_key str Choose the key in annotations on which the color will depend. Defaults to 'category_id'. 'category_id' show_label bool Choose whether to show label and score threshold on image. Default True. True filter_crowd bool Filter out crowd annotations or not. Default True. True first_n Optional [ int ] Paint only first n annotations and stop after that. If None, all images will be painted. None use_exif_orientation bool If an image has an EXIF Orientation tag, other than 1, return a new image that is transposed accordingly. The new image will have the orientation data removed. False Source code in pyodi/apps/paint_annotations.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 @logger . catch ( reraise = True ) def paint_annotations ( ground_truth_file : str , image_folder : str , output_folder : str , predictions_file : Optional [ str ] = None , score_thr : float = 0.0 , color_key : str = \"category_id\" , show_label : bool = True , filter_crowd : bool = True , first_n : Optional [ int ] = None , use_exif_orientation : bool = False , ) -> None : \"\"\"Paint `ground_truth_file` or `predictions_file` annotations on `image_folder` images. Args: ground_truth_file: Path to COCO ground truth file. image_folder: Path to root folder where the images of `ground_truth_file` are. output_folder: Path to the folder where painted images will be saved. It will be created if it does not exist. predictions_file: Path to COCO predictions file. If not None, the annotations of predictions_file will be painted instead of ground_truth_file's. score_thr: Detections bellow this threshold will not be painted. Default 0.0. color_key: Choose the key in annotations on which the color will depend. Defaults to 'category_id'. show_label: Choose whether to show label and score threshold on image. Default True. filter_crowd: Filter out crowd annotations or not. Default True. first_n: Paint only first n annotations and stop after that. If None, all images will be painted. use_exif_orientation: If an image has an EXIF Orientation tag, other than 1, return a new image that is transposed accordingly. The new image will have the orientation data removed. \"\"\" Path ( output_folder ) . mkdir ( exist_ok = True , parents = True ) ground_truth = json . load ( open ( ground_truth_file )) image_name_to_id = { Path ( x [ \"file_name\" ]) . stem : x [ \"id\" ] for x in ground_truth [ \"images\" ] } category_id_to_label = { cat [ \"id\" ]: cat [ \"name\" ] for cat in ground_truth [ \"categories\" ] } image_id_to_annotations : Dict = defaultdict ( list ) if predictions_file is not None : with open ( predictions_file ) as pred : annotations = json . load ( pred ) else : annotations = ground_truth [ \"annotations\" ] n_colors = len ( set ( ann [ color_key ] for ann in annotations )) colormap = cm . rainbow ( np . linspace ( 0 , 1 , n_colors )) for annotation in annotations : if not ( filter_crowd and annotation . get ( \"iscrowd\" , False )): image_id_to_annotations [ annotation [ \"image_id\" ]] . append ( annotation ) image_data = ground_truth [ \"images\" ] first_n = first_n or len ( image_data ) for image in image_data [: first_n ]: image_filename = image [ \"file_name\" ] image_id = image [ \"id\" ] image_path = Path ( image_folder ) / image_filename logger . info ( f \"Loading { image_filename } \" ) if Path ( image_filename ) . stem not in image_name_to_id : logger . warning ( f \" { image_filename } not in ground_truth_file\" ) if image_path . is_file (): image_pil = Image . open ( image_path ) if use_exif_orientation : image_pil = ImageOps . exif_transpose ( image_pil ) width , height = image_pil . size fig = plt . figure ( frameon = False , figsize = ( width / 96 , height / 96 )) ax = plt . Axes ( fig , [ 0.0 , 0.0 , 1.0 , 1.0 ]) ax . set_axis_off () fig . add_axes ( ax ) ax . imshow ( image_pil , aspect = \"auto\" ) polygons = [] colors = [] for annotation in image_id_to_annotations [ image_id ]: score = annotation . get ( \"score\" , 1 ) if score < score_thr : continue bbox_left , bbox_top , bbox_width , bbox_height = annotation [ \"bbox\" ] cat_id = annotation [ \"category_id\" ] label = category_id_to_label [ cat_id ] color_id = annotation [ color_key ] color = colormap [ color_id % len ( colormap )] poly = [ [ bbox_left , bbox_top ], [ bbox_left , bbox_top + bbox_height ], [ bbox_left + bbox_width , bbox_top + bbox_height ], [ bbox_left + bbox_width , bbox_top ], ] polygons . append ( Polygon ( poly )) colors . append ( color ) if show_label : label_text = f \" { label } \" if predictions_file is not None : label_text += f \": { score : .2f } \" ax . text ( bbox_left , bbox_top , label_text , va = \"top\" , ha = \"left\" , bbox = dict ( facecolor = \"white\" , edgecolor = color , alpha = 0.5 , pad = 0 ), ) p = PatchCollection ( polygons , facecolor = colors , linewidths = 0 , alpha = 0.3 ) ax . add_collection ( p ) p = PatchCollection ( polygons , facecolor = \"none\" , edgecolors = colors , linewidths = 1 ) ax . add_collection ( p ) filename = Path ( image_filename ) . stem file_extension = Path ( image_filename ) . suffix output_file = Path ( output_folder ) / f \" { filename } _result { file_extension } \" logger . info ( f \"Saving { output_file } \" ) plt . savefig ( output_file ) plt . close ()","title":"paint-annotations"},{"location":"reference/apps/paint-annotations/#pyodi.apps.paint_annotations--paint-annotations-app","text":"The pyodi paint-annotations helps you to easily visualize in a beautiful format your object detection dataset. You can also use this function to visualize model predictions if they are in COCO predictions format. Example usage: pyodi paint-annotations \\\\ $TINY_COCO_ANIMAL /annotations/train.json \\\\ $TINY_COCO_ANIMAL /sample_images/ \\\\ $TINY_COCO_ANIMAL /painted_images/","title":"Paint Annotations App."},{"location":"reference/apps/paint-annotations/#pyodi.apps.paint_annotations--api-reference","text":"","title":"API REFERENCE"},{"location":"reference/apps/paint-annotations/#pyodi.apps.paint_annotations.paint_annotations","text":"Paint ground_truth_file or predictions_file annotations on image_folder images. Parameters: Name Type Description Default ground_truth_file str Path to COCO ground truth file. required image_folder str Path to root folder where the images of ground_truth_file are. required output_folder str Path to the folder where painted images will be saved. It will be created if it does not exist. required predictions_file Optional [ str ] Path to COCO predictions file. If not None, the annotations of predictions_file will be painted instead of ground_truth_file's. None score_thr float Detections bellow this threshold will not be painted. Default 0.0. 0.0 color_key str Choose the key in annotations on which the color will depend. Defaults to 'category_id'. 'category_id' show_label bool Choose whether to show label and score threshold on image. Default True. True filter_crowd bool Filter out crowd annotations or not. Default True. True first_n Optional [ int ] Paint only first n annotations and stop after that. If None, all images will be painted. None use_exif_orientation bool If an image has an EXIF Orientation tag, other than 1, return a new image that is transposed accordingly. The new image will have the orientation data removed. False Source code in pyodi/apps/paint_annotations.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 @logger . catch ( reraise = True ) def paint_annotations ( ground_truth_file : str , image_folder : str , output_folder : str , predictions_file : Optional [ str ] = None , score_thr : float = 0.0 , color_key : str = \"category_id\" , show_label : bool = True , filter_crowd : bool = True , first_n : Optional [ int ] = None , use_exif_orientation : bool = False , ) -> None : \"\"\"Paint `ground_truth_file` or `predictions_file` annotations on `image_folder` images. Args: ground_truth_file: Path to COCO ground truth file. image_folder: Path to root folder where the images of `ground_truth_file` are. output_folder: Path to the folder where painted images will be saved. It will be created if it does not exist. predictions_file: Path to COCO predictions file. If not None, the annotations of predictions_file will be painted instead of ground_truth_file's. score_thr: Detections bellow this threshold will not be painted. Default 0.0. color_key: Choose the key in annotations on which the color will depend. Defaults to 'category_id'. show_label: Choose whether to show label and score threshold on image. Default True. filter_crowd: Filter out crowd annotations or not. Default True. first_n: Paint only first n annotations and stop after that. If None, all images will be painted. use_exif_orientation: If an image has an EXIF Orientation tag, other than 1, return a new image that is transposed accordingly. The new image will have the orientation data removed. \"\"\" Path ( output_folder ) . mkdir ( exist_ok = True , parents = True ) ground_truth = json . load ( open ( ground_truth_file )) image_name_to_id = { Path ( x [ \"file_name\" ]) . stem : x [ \"id\" ] for x in ground_truth [ \"images\" ] } category_id_to_label = { cat [ \"id\" ]: cat [ \"name\" ] for cat in ground_truth [ \"categories\" ] } image_id_to_annotations : Dict = defaultdict ( list ) if predictions_file is not None : with open ( predictions_file ) as pred : annotations = json . load ( pred ) else : annotations = ground_truth [ \"annotations\" ] n_colors = len ( set ( ann [ color_key ] for ann in annotations )) colormap = cm . rainbow ( np . linspace ( 0 , 1 , n_colors )) for annotation in annotations : if not ( filter_crowd and annotation . get ( \"iscrowd\" , False )): image_id_to_annotations [ annotation [ \"image_id\" ]] . append ( annotation ) image_data = ground_truth [ \"images\" ] first_n = first_n or len ( image_data ) for image in image_data [: first_n ]: image_filename = image [ \"file_name\" ] image_id = image [ \"id\" ] image_path = Path ( image_folder ) / image_filename logger . info ( f \"Loading { image_filename } \" ) if Path ( image_filename ) . stem not in image_name_to_id : logger . warning ( f \" { image_filename } not in ground_truth_file\" ) if image_path . is_file (): image_pil = Image . open ( image_path ) if use_exif_orientation : image_pil = ImageOps . exif_transpose ( image_pil ) width , height = image_pil . size fig = plt . figure ( frameon = False , figsize = ( width / 96 , height / 96 )) ax = plt . Axes ( fig , [ 0.0 , 0.0 , 1.0 , 1.0 ]) ax . set_axis_off () fig . add_axes ( ax ) ax . imshow ( image_pil , aspect = \"auto\" ) polygons = [] colors = [] for annotation in image_id_to_annotations [ image_id ]: score = annotation . get ( \"score\" , 1 ) if score < score_thr : continue bbox_left , bbox_top , bbox_width , bbox_height = annotation [ \"bbox\" ] cat_id = annotation [ \"category_id\" ] label = category_id_to_label [ cat_id ] color_id = annotation [ color_key ] color = colormap [ color_id % len ( colormap )] poly = [ [ bbox_left , bbox_top ], [ bbox_left , bbox_top + bbox_height ], [ bbox_left + bbox_width , bbox_top + bbox_height ], [ bbox_left + bbox_width , bbox_top ], ] polygons . append ( Polygon ( poly )) colors . append ( color ) if show_label : label_text = f \" { label } \" if predictions_file is not None : label_text += f \": { score : .2f } \" ax . text ( bbox_left , bbox_top , label_text , va = \"top\" , ha = \"left\" , bbox = dict ( facecolor = \"white\" , edgecolor = color , alpha = 0.5 , pad = 0 ), ) p = PatchCollection ( polygons , facecolor = colors , linewidths = 0 , alpha = 0.3 ) ax . add_collection ( p ) p = PatchCollection ( polygons , facecolor = \"none\" , edgecolors = colors , linewidths = 1 ) ax . add_collection ( p ) filename = Path ( image_filename ) . stem file_extension = Path ( image_filename ) . suffix output_file = Path ( output_folder ) / f \" { filename } _result { file_extension } \" logger . info ( f \"Saving { output_file } \" ) plt . savefig ( output_file ) plt . close ()","title":"paint_annotations()"},{"location":"reference/apps/train-config-evaluation/","text":"Train Config Evaluation App. The pyodi train-config evaluation app can be used to evaluate a given mmdetection Anchor Generator Configuration to train your model using a specific training pipeline. Procedure Training performance of object detection model depends on how well generated anchors match with ground truth bounding boxes. This simple application provides intuitions about this, by recreating train preprocessing conditions such as image resizing or padding, and computing different metrics based on the largest Intersection over Union (IoU) between ground truth boxes and the provided anchors. Each bounding box is assigned with the anchor that shares a largest IoU with it. We call overlap, to the maximum IoU each ground truth box has with the generated anchor set. Example usage: pyodi train-config evaluation \\\\ $TINY_COCO_ANIMAL /annotations/train.json \\\\ $TINY_COCO_ANIMAL /resources/anchor_config.py \\\\ --input-size [ 1280 ,720 ] The app provides four different plots: Cumulative Overlap It shows a cumulative distribution function for the overlap distribution. This view helps to distinguish which percentage of bounding boxes have a very low overlap with generated anchors and viceversa. It can be very useful to determine positive and negative thresholds for your training, these are the values that determine is a ground truth bounding box will is going to be taken into account in the loss function or discarded and considered as background. Bounding Box Distribution It shows a scatter plot of bounding box width vs height. The color of each point represent the overlap value assigned to that bounding box. Thanks to this plot we can easily observe pattern such low overlap values for large bounding boxes. We could have this into account and generate larger anchors to improve this matching. Scale and Mean Overlap This plot contains a simple histogram with bins of similar scales and its mean overlap value. It help us to visualize how overlap decays when scale increases, as we said before. Log Ratio and Mean Overlap Similarly to previous plot, it shows an histogram of bounding box log ratios and its mean overlap values. It is useful to visualize this relation and see how certain box ratios might be having problems to match with generated anchors. In this example, boxes with negative log ratios, where width is much larger than height, overlaps are very small. See how this matches with patterns observed in bounding box distribution plot, where all boxes placed near to x axis, have low overlaps. API REFERENCE load_anchor_config_file ( anchor_config_file ) Loads the anchor_config_file . Parameters: Name Type Description Default anchor_config_file str File with the anchor configuration. required Returns: Type Description Dict [ str , Any ] Dictionary with the training configuration. Source code in pyodi/apps/train_config/train_config_evaluation.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def load_anchor_config_file ( anchor_config_file : str ) -> Dict [ str , Any ]: \"\"\"Loads the `anchor_config_file`. Args: anchor_config_file: File with the anchor configuration. Returns: Dictionary with the training configuration. \"\"\" logger . info ( \"Loading Train Config File\" ) with TemporaryDirectory () as temp_config_dir : copyfile ( anchor_config_file , osp . join ( temp_config_dir , \"_tempconfig.py\" )) sys . path . insert ( 0 , temp_config_dir ) mod = import_module ( \"_tempconfig\" ) sys . path . pop ( 0 ) train_config = { name : value for name , value in mod . __dict__ . items () if not name . startswith ( \"__\" ) } # delete imported module del sys . modules [ \"_tempconfig\" ] return train_config train_config_evaluation ( ground_truth_file , anchor_config , input_size = ( 1280 , 720 ), show = True , output = None , output_size = ( 1600 , 900 ), keep_ratio = False ) Evaluates the fitness between ground_truth_file and anchor_config_file . Parameters: Name Type Description Default ground_truth_file Union [ str , pd . DataFrame ] Path to COCO ground truth file or coco df_annotations DataFrame to be used from pyodi train-config generation required anchor_config str Path to MMDetection-like anchor_generator section. It can also be a dictionary with the required data. required input_size Tuple [ int , int ] Model image input size. Defaults to (1333, 800). (1280, 720) show bool Show results or not. Defaults to True. True output Optional [ str ] Output directory where results going to be saved. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) keep_ratio bool Whether to keep the aspect ratio or not. Defaults to False. False Examples: # faster_rcnn_r50_fpn.py: anchor_generator = dict ( type = 'AnchorGenerator' , scales = [ 8 ], ratios = [ 0.5 , 1.0 , 2.0 ], strides = [ 4 , 8 , 16 , 32 , 64 ] ) Source code in pyodi/apps/train_config/train_config_evaluation.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 @logger . catch ( reraise = True ) def train_config_evaluation ( ground_truth_file : Union [ str , pd . DataFrame ], anchor_config : str , input_size : Tuple [ int , int ] = ( 1280 , 720 ), show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), keep_ratio : bool = False , ) -> None : \"\"\"Evaluates the fitness between `ground_truth_file` and `anchor_config_file`. Args: ground_truth_file: Path to COCO ground truth file or coco df_annotations DataFrame to be used from [`pyodi train-config generation`][pyodi.apps.train_config.train_config_generation.train_config_generation] anchor_config: Path to MMDetection-like `anchor_generator` section. It can also be a dictionary with the required data. input_size: Model image input size. Defaults to (1333, 800). show: Show results or not. Defaults to True. output: Output directory where results going to be saved. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). keep_ratio: Whether to keep the aspect ratio or not. Defaults to False. Examples: ```python # faster_rcnn_r50_fpn.py: anchor_generator=dict( type='AnchorGenerator', scales=[8], ratios=[0.5, 1.0, 2.0], strides=[4, 8, 16, 32, 64] ) ``` \"\"\" if output is not None : Path ( output ) . mkdir ( parents = True , exist_ok = True ) if isinstance ( ground_truth_file , str ): df_annotations = coco_ground_truth_to_df ( ground_truth_file ) df_annotations = filter_zero_area_bboxes ( df_annotations ) df_annotations = scale_bbox_dimensions ( df_annotations , input_size = input_size , keep_ratio = keep_ratio ) df_annotations = get_scale_and_ratio ( df_annotations , prefix = \"scaled\" ) else : df_annotations = ground_truth_file df_annotations [ \"log_scaled_ratio\" ] = np . log ( df_annotations [ \"scaled_ratio\" ]) if isinstance ( anchor_config , str ): anchor_config_data = load_anchor_config_file ( anchor_config ) elif isinstance ( anchor_config , dict ): anchor_config_data = anchor_config else : raise ValueError ( \"anchor_config must be string or dictionary.\" ) anchor_config_data [ \"anchor_generator\" ] . pop ( \"type\" , None ) anchor_generator = AnchorGenerator ( ** anchor_config_data [ \"anchor_generator\" ]) if isinstance ( anchor_config , str ): logger . info ( anchor_generator . to_string ()) width , height = input_size featmap_sizes = [ ( width // stride , height // stride ) for stride in anchor_generator . strides ] anchors_per_level = anchor_generator . grid_anchors ( featmap_sizes = featmap_sizes ) bboxes = get_bbox_array ( df_annotations , prefix = \"scaled\" , output_bbox_format = \"corners\" ) overlaps = np . zeros ( bboxes . shape [ 0 ]) max_overlap_level = np . zeros ( bboxes . shape [ 0 ]) logger . info ( \"Computing overlaps between anchors and ground truth ...\" ) for i , anchor_level in enumerate ( anchors_per_level ): level_overlaps = get_max_overlap ( bboxes . astype ( np . float32 ), anchor_level . astype ( np . float32 ) ) max_overlap_level [ level_overlaps > overlaps ] = i overlaps = np . maximum ( overlaps , level_overlaps ) df_annotations [ \"overlaps\" ] = overlaps df_annotations [ \"max_overlap_level\" ] = max_overlap_level logger . info ( \"Plotting results ...\" ) plot_overlap_result ( df_annotations , show = show , output = output , output_size = output_size )","title":"evaluation"},{"location":"reference/apps/train-config-evaluation/#pyodi.apps.train_config.train_config_evaluation--train-config-evaluation-app","text":"The pyodi train-config evaluation app can be used to evaluate a given mmdetection Anchor Generator Configuration to train your model using a specific training pipeline.","title":"Train Config Evaluation App."},{"location":"reference/apps/train-config-evaluation/#pyodi.apps.train_config.train_config_evaluation--procedure","text":"Training performance of object detection model depends on how well generated anchors match with ground truth bounding boxes. This simple application provides intuitions about this, by recreating train preprocessing conditions such as image resizing or padding, and computing different metrics based on the largest Intersection over Union (IoU) between ground truth boxes and the provided anchors. Each bounding box is assigned with the anchor that shares a largest IoU with it. We call overlap, to the maximum IoU each ground truth box has with the generated anchor set. Example usage: pyodi train-config evaluation \\\\ $TINY_COCO_ANIMAL /annotations/train.json \\\\ $TINY_COCO_ANIMAL /resources/anchor_config.py \\\\ --input-size [ 1280 ,720 ] The app provides four different plots:","title":"Procedure"},{"location":"reference/apps/train-config-evaluation/#pyodi.apps.train_config.train_config_evaluation--cumulative-overlap","text":"It shows a cumulative distribution function for the overlap distribution. This view helps to distinguish which percentage of bounding boxes have a very low overlap with generated anchors and viceversa. It can be very useful to determine positive and negative thresholds for your training, these are the values that determine is a ground truth bounding box will is going to be taken into account in the loss function or discarded and considered as background.","title":"Cumulative Overlap"},{"location":"reference/apps/train-config-evaluation/#pyodi.apps.train_config.train_config_evaluation--bounding-box-distribution","text":"It shows a scatter plot of bounding box width vs height. The color of each point represent the overlap value assigned to that bounding box. Thanks to this plot we can easily observe pattern such low overlap values for large bounding boxes. We could have this into account and generate larger anchors to improve this matching.","title":"Bounding Box Distribution"},{"location":"reference/apps/train-config-evaluation/#pyodi.apps.train_config.train_config_evaluation--scale-and-mean-overlap","text":"This plot contains a simple histogram with bins of similar scales and its mean overlap value. It help us to visualize how overlap decays when scale increases, as we said before.","title":"Scale and Mean Overlap"},{"location":"reference/apps/train-config-evaluation/#pyodi.apps.train_config.train_config_evaluation--log-ratio-and-mean-overlap","text":"Similarly to previous plot, it shows an histogram of bounding box log ratios and its mean overlap values. It is useful to visualize this relation and see how certain box ratios might be having problems to match with generated anchors. In this example, boxes with negative log ratios, where width is much larger than height, overlaps are very small. See how this matches with patterns observed in bounding box distribution plot, where all boxes placed near to x axis, have low overlaps.","title":"Log Ratio and Mean Overlap"},{"location":"reference/apps/train-config-evaluation/#pyodi.apps.train_config.train_config_evaluation--api-reference","text":"","title":"API REFERENCE"},{"location":"reference/apps/train-config-evaluation/#pyodi.apps.train_config.train_config_evaluation.load_anchor_config_file","text":"Loads the anchor_config_file . Parameters: Name Type Description Default anchor_config_file str File with the anchor configuration. required Returns: Type Description Dict [ str , Any ] Dictionary with the training configuration. Source code in pyodi/apps/train_config/train_config_evaluation.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def load_anchor_config_file ( anchor_config_file : str ) -> Dict [ str , Any ]: \"\"\"Loads the `anchor_config_file`. Args: anchor_config_file: File with the anchor configuration. Returns: Dictionary with the training configuration. \"\"\" logger . info ( \"Loading Train Config File\" ) with TemporaryDirectory () as temp_config_dir : copyfile ( anchor_config_file , osp . join ( temp_config_dir , \"_tempconfig.py\" )) sys . path . insert ( 0 , temp_config_dir ) mod = import_module ( \"_tempconfig\" ) sys . path . pop ( 0 ) train_config = { name : value for name , value in mod . __dict__ . items () if not name . startswith ( \"__\" ) } # delete imported module del sys . modules [ \"_tempconfig\" ] return train_config","title":"load_anchor_config_file()"},{"location":"reference/apps/train-config-evaluation/#pyodi.apps.train_config.train_config_evaluation.train_config_evaluation","text":"Evaluates the fitness between ground_truth_file and anchor_config_file . Parameters: Name Type Description Default ground_truth_file Union [ str , pd . DataFrame ] Path to COCO ground truth file or coco df_annotations DataFrame to be used from pyodi train-config generation required anchor_config str Path to MMDetection-like anchor_generator section. It can also be a dictionary with the required data. required input_size Tuple [ int , int ] Model image input size. Defaults to (1333, 800). (1280, 720) show bool Show results or not. Defaults to True. True output Optional [ str ] Output directory where results going to be saved. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) keep_ratio bool Whether to keep the aspect ratio or not. Defaults to False. False Examples: # faster_rcnn_r50_fpn.py: anchor_generator = dict ( type = 'AnchorGenerator' , scales = [ 8 ], ratios = [ 0.5 , 1.0 , 2.0 ], strides = [ 4 , 8 , 16 , 32 , 64 ] ) Source code in pyodi/apps/train_config/train_config_evaluation.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 @logger . catch ( reraise = True ) def train_config_evaluation ( ground_truth_file : Union [ str , pd . DataFrame ], anchor_config : str , input_size : Tuple [ int , int ] = ( 1280 , 720 ), show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), keep_ratio : bool = False , ) -> None : \"\"\"Evaluates the fitness between `ground_truth_file` and `anchor_config_file`. Args: ground_truth_file: Path to COCO ground truth file or coco df_annotations DataFrame to be used from [`pyodi train-config generation`][pyodi.apps.train_config.train_config_generation.train_config_generation] anchor_config: Path to MMDetection-like `anchor_generator` section. It can also be a dictionary with the required data. input_size: Model image input size. Defaults to (1333, 800). show: Show results or not. Defaults to True. output: Output directory where results going to be saved. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). keep_ratio: Whether to keep the aspect ratio or not. Defaults to False. Examples: ```python # faster_rcnn_r50_fpn.py: anchor_generator=dict( type='AnchorGenerator', scales=[8], ratios=[0.5, 1.0, 2.0], strides=[4, 8, 16, 32, 64] ) ``` \"\"\" if output is not None : Path ( output ) . mkdir ( parents = True , exist_ok = True ) if isinstance ( ground_truth_file , str ): df_annotations = coco_ground_truth_to_df ( ground_truth_file ) df_annotations = filter_zero_area_bboxes ( df_annotations ) df_annotations = scale_bbox_dimensions ( df_annotations , input_size = input_size , keep_ratio = keep_ratio ) df_annotations = get_scale_and_ratio ( df_annotations , prefix = \"scaled\" ) else : df_annotations = ground_truth_file df_annotations [ \"log_scaled_ratio\" ] = np . log ( df_annotations [ \"scaled_ratio\" ]) if isinstance ( anchor_config , str ): anchor_config_data = load_anchor_config_file ( anchor_config ) elif isinstance ( anchor_config , dict ): anchor_config_data = anchor_config else : raise ValueError ( \"anchor_config must be string or dictionary.\" ) anchor_config_data [ \"anchor_generator\" ] . pop ( \"type\" , None ) anchor_generator = AnchorGenerator ( ** anchor_config_data [ \"anchor_generator\" ]) if isinstance ( anchor_config , str ): logger . info ( anchor_generator . to_string ()) width , height = input_size featmap_sizes = [ ( width // stride , height // stride ) for stride in anchor_generator . strides ] anchors_per_level = anchor_generator . grid_anchors ( featmap_sizes = featmap_sizes ) bboxes = get_bbox_array ( df_annotations , prefix = \"scaled\" , output_bbox_format = \"corners\" ) overlaps = np . zeros ( bboxes . shape [ 0 ]) max_overlap_level = np . zeros ( bboxes . shape [ 0 ]) logger . info ( \"Computing overlaps between anchors and ground truth ...\" ) for i , anchor_level in enumerate ( anchors_per_level ): level_overlaps = get_max_overlap ( bboxes . astype ( np . float32 ), anchor_level . astype ( np . float32 ) ) max_overlap_level [ level_overlaps > overlaps ] = i overlaps = np . maximum ( overlaps , level_overlaps ) df_annotations [ \"overlaps\" ] = overlaps df_annotations [ \"max_overlap_level\" ] = max_overlap_level logger . info ( \"Plotting results ...\" ) plot_overlap_result ( df_annotations , show = show , output = output , output_size = output_size )","title":"train_config_evaluation()"},{"location":"reference/apps/train-config-generation/","text":"Train Config Generation App. The pyodi train-config generation app can be used to automatically generate a mmdetection anchor configuration to train your model. The design of anchors is critical for the performance of one-stage detectors. Usually, published models such Faster R-CNN or RetinaNet include default anchors which has been designed to work with general object detection purpose as COCO dataset. Nevertheless, you might be envolved in different problems which data contains only a few different classes that share similar properties, as the object sizes or shapes, this would be the case for a drone detection dataset such Drone vs Bird . You can exploit this knowledge by designing anchors that specially fit the distribution of your data, optimizing the probability of matching ground truth bounding boxes with generated anchors, which can result in an increase in the performance of your model. At the same time, you can reduce the number of anchors you use to boost inference and training time. Procedure The input size parameter determines the model input size and automatically reshapes images and annotations sizes to it. Ground truth boxes are assigned to the anchor base size that has highest Intersection over Union (IoU) score with them. This step, allow us to locate each ground truth bounding box in a feature level of the FPN pyramid. Once this is done, the ratio between the scales of ground truth boxes and the scales of their associated anchors is computed. A log transform is applied to it and they are clustered using kmeans algorithm, where the number of obtained clusters depends on n_scales input parameter. After this step, a similar procedure is followed to obtain the reference scale ratios of the dataset, computing log scales ratios of each box and clustering them with number of clusters equal to n_ratios . Example usage: pyodi train-config generation \\\\ $TINY_COCO_ANIMAL /annotations/train.json \\\\ --input-size [ 1280 ,720 ] \\\\ --n-ratios 3 --n-scales 3 The app shows two different plots: Log Relative Scale vs Log Ratio In this graphic you can distinguish how your bounding boxes scales and ratios are distributed. The x axis represent the log scale of the ratio between the bounding box scales and the scale of their matched anchor base size. The y axis contains the bounding box log ratios. Centroids are the result of combinating the obtained scales and ratios obtained with kmeans. We can see how clusters appear in those areas where box distribution is more dense. We could increase the value of n_ratios from three to four, having into account that the number of anchors is goint to increase, which will influence training computational cost. pyodi train-config generation annotations/train.json --input-size [ 1280 ,720 ] --n-ratios 4 --n-scales 3 In plot below we can observe the result for n_ratios equal to four. Bounding Box Distribution This plot is very useful to observe how the generated anchors fit you bounding box distribution. The number of anchors depends on: The length of base_sizes which determines the number of FPN pyramid levels. A total of n_ratios x n_scales anchors is generated per level We can now increase the number of n_scales and observe the effect on the bounding box distribution plot. Proposed anchors are also attached in a Json file that follows mmdetection anchors format: anchor_generator = dict ( type = 'AnchorGenerator' , scales = [ 1.12 , 3.13 , 8.0 ], ratios = [ 0.33 , 0.67 , 1.4 ], strides = [ 4 , 8 , 16 , 32 , 64 ], base_sizes = [ 4 , 8 , 16 , 32 , 64 ], ) By default, pyodi train-config evaluation is used after the generation of anchors in order to compare which generated anchor config suits better your data. You can disable this evaluation by setting to False the evaluate argument, but it is strongly advised to use the anchor evaluation module. API REFERENCE train_config_generation ( ground_truth_file , input_size = ( 1280 , 720 ), n_ratios = 3 , n_scales = 3 , strides = None , base_sizes = None , show = True , output = None , output_size = ( 1600 , 900 ), keep_ratio = False , evaluate = True ) Computes optimal anchors for a given COCO dataset based on iou clustering. Parameters: Name Type Description Default ground_truth_file str Path to COCO ground truth file. required input_size Tuple [ int , int ] Model image input size. Defaults to (1280, 720). (1280, 720) n_ratios int Number of ratios. Defaults to 3. 3 n_scales int Number of scales. Defaults to 3. 3 strides Optional [ List [ int ]] List of strides. Defatults to [4, 8, 16, 32, 64]. None base_sizes Optional [ List [ int ]] The basic sizes of anchors in multiple levels. If None is given, strides will be used as base_sizes. None show bool Show results or not. Defaults to True. True output Optional [ str ] Output directory where results going to be saved. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) keep_ratio bool Whether to keep the aspect ratio or not. Defaults to False. False evaluate bool Whether to evaluate or not the anchors. Check pyodi train-config evaluation for more information. True Returns: Type Description None Anchor generator instance. Source code in pyodi/apps/train_config/train_config_generation.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 @logger . catch ( reraise = True ) def train_config_generation ( ground_truth_file : str , input_size : Tuple [ int , int ] = ( 1280 , 720 ), n_ratios : int = 3 , n_scales : int = 3 , strides : Optional [ List [ int ]] = None , base_sizes : Optional [ List [ int ]] = None , show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), keep_ratio : bool = False , evaluate : bool = True , ) -> None : \"\"\"Computes optimal anchors for a given COCO dataset based on iou clustering. Args: ground_truth_file: Path to COCO ground truth file. input_size: Model image input size. Defaults to (1280, 720). n_ratios: Number of ratios. Defaults to 3. n_scales: Number of scales. Defaults to 3. strides: List of strides. Defatults to [4, 8, 16, 32, 64]. base_sizes: The basic sizes of anchors in multiple levels. If None is given, strides will be used as base_sizes. show: Show results or not. Defaults to True. output: Output directory where results going to be saved. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). keep_ratio: Whether to keep the aspect ratio or not. Defaults to False. evaluate: Whether to evaluate or not the anchors. Check [`pyodi train-config evaluation`][pyodi.apps.train_config.train_config_evaluation.train_config_evaluation] for more information. Returns: Anchor generator instance. \"\"\" if output is not None : Path ( output ) . mkdir ( parents = True , exist_ok = True ) df_annotations = coco_ground_truth_to_df ( ground_truth_file ) df_annotations = filter_zero_area_bboxes ( df_annotations ) df_annotations = scale_bbox_dimensions ( df_annotations , input_size = input_size , keep_ratio = keep_ratio ) df_annotations = get_scale_and_ratio ( df_annotations , prefix = \"scaled\" ) if strides is None : strides = [ 4 , 8 , 16 , 32 , 64 ] if base_sizes is None : base_sizes = strides # Assign fpn level df_annotations [ \"fpn_level\" ] = find_pyramid_level ( get_bbox_array ( df_annotations , prefix = \"scaled\" )[:, 2 :], base_sizes ) df_annotations [ \"fpn_level_scale\" ] = df_annotations [ \"fpn_level\" ] . replace ( { i : scale for i , scale in enumerate ( base_sizes )} ) df_annotations [ \"level_scale\" ] = ( df_annotations [ \"scaled_scale\" ] / df_annotations [ \"fpn_level_scale\" ] ) # Normalize to log scale df_annotations [ \"log_ratio\" ] = np . log ( df_annotations [ \"scaled_ratio\" ]) df_annotations [ \"log_level_scale\" ] = np . log ( df_annotations [ \"level_scale\" ]) # Cluster bboxes by scale and ratio independently clustering_results = [ kmeans_euclidean ( df_annotations [ value ] . to_numpy (), n_clusters = n_clusters ) for i , ( value , n_clusters ) in enumerate ( zip ([ \"log_level_scale\" , \"log_ratio\" ], [ n_scales , n_ratios ]) ) ] # Bring back from log scale scales = np . e ** clustering_results [ 0 ][ \"centroids\" ] ratios = np . e ** clustering_results [ 1 ][ \"centroids\" ] anchor_generator = AnchorGenerator ( strides = strides , ratios = ratios , scales = scales , base_sizes = base_sizes , ) logger . info ( f \"Anchor configuration: \\n { anchor_generator . to_string () } \" ) plot_clustering_results ( df_annotations , anchor_generator , show = show , output = output , output_size = output_size , title = \"COCO_anchor_generation\" , ) if evaluate : anchor_config = dict ( anchor_generator = anchor_generator . to_dict ()) train_config_evaluation ( ground_truth_file = df_annotations , anchor_config = anchor_config , # type: ignore input_size = input_size , show = show , output = output , output_size = output_size , ) if output : output_file = Path ( output ) / \"anchor_config.py\" with open ( output_file , \"w\" ) as f : f . write ( anchor_generator . to_string ())","title":"generation"},{"location":"reference/apps/train-config-generation/#pyodi.apps.train_config.train_config_generation--train-config-generation-app","text":"The pyodi train-config generation app can be used to automatically generate a mmdetection anchor configuration to train your model. The design of anchors is critical for the performance of one-stage detectors. Usually, published models such Faster R-CNN or RetinaNet include default anchors which has been designed to work with general object detection purpose as COCO dataset. Nevertheless, you might be envolved in different problems which data contains only a few different classes that share similar properties, as the object sizes or shapes, this would be the case for a drone detection dataset such Drone vs Bird . You can exploit this knowledge by designing anchors that specially fit the distribution of your data, optimizing the probability of matching ground truth bounding boxes with generated anchors, which can result in an increase in the performance of your model. At the same time, you can reduce the number of anchors you use to boost inference and training time.","title":"Train Config Generation App."},{"location":"reference/apps/train-config-generation/#pyodi.apps.train_config.train_config_generation--procedure","text":"The input size parameter determines the model input size and automatically reshapes images and annotations sizes to it. Ground truth boxes are assigned to the anchor base size that has highest Intersection over Union (IoU) score with them. This step, allow us to locate each ground truth bounding box in a feature level of the FPN pyramid. Once this is done, the ratio between the scales of ground truth boxes and the scales of their associated anchors is computed. A log transform is applied to it and they are clustered using kmeans algorithm, where the number of obtained clusters depends on n_scales input parameter. After this step, a similar procedure is followed to obtain the reference scale ratios of the dataset, computing log scales ratios of each box and clustering them with number of clusters equal to n_ratios . Example usage: pyodi train-config generation \\\\ $TINY_COCO_ANIMAL /annotations/train.json \\\\ --input-size [ 1280 ,720 ] \\\\ --n-ratios 3 --n-scales 3 The app shows two different plots:","title":"Procedure"},{"location":"reference/apps/train-config-generation/#pyodi.apps.train_config.train_config_generation--log-relative-scale-vs-log-ratio","text":"In this graphic you can distinguish how your bounding boxes scales and ratios are distributed. The x axis represent the log scale of the ratio between the bounding box scales and the scale of their matched anchor base size. The y axis contains the bounding box log ratios. Centroids are the result of combinating the obtained scales and ratios obtained with kmeans. We can see how clusters appear in those areas where box distribution is more dense. We could increase the value of n_ratios from three to four, having into account that the number of anchors is goint to increase, which will influence training computational cost. pyodi train-config generation annotations/train.json --input-size [ 1280 ,720 ] --n-ratios 4 --n-scales 3 In plot below we can observe the result for n_ratios equal to four.","title":"Log Relative Scale vs Log Ratio"},{"location":"reference/apps/train-config-generation/#pyodi.apps.train_config.train_config_generation--bounding-box-distribution","text":"This plot is very useful to observe how the generated anchors fit you bounding box distribution. The number of anchors depends on: The length of base_sizes which determines the number of FPN pyramid levels. A total of n_ratios x n_scales anchors is generated per level We can now increase the number of n_scales and observe the effect on the bounding box distribution plot. Proposed anchors are also attached in a Json file that follows mmdetection anchors format: anchor_generator = dict ( type = 'AnchorGenerator' , scales = [ 1.12 , 3.13 , 8.0 ], ratios = [ 0.33 , 0.67 , 1.4 ], strides = [ 4 , 8 , 16 , 32 , 64 ], base_sizes = [ 4 , 8 , 16 , 32 , 64 ], ) By default, pyodi train-config evaluation is used after the generation of anchors in order to compare which generated anchor config suits better your data. You can disable this evaluation by setting to False the evaluate argument, but it is strongly advised to use the anchor evaluation module.","title":"Bounding Box Distribution"},{"location":"reference/apps/train-config-generation/#pyodi.apps.train_config.train_config_generation--api-reference","text":"","title":"API REFERENCE"},{"location":"reference/apps/train-config-generation/#pyodi.apps.train_config.train_config_generation.train_config_generation","text":"Computes optimal anchors for a given COCO dataset based on iou clustering. Parameters: Name Type Description Default ground_truth_file str Path to COCO ground truth file. required input_size Tuple [ int , int ] Model image input size. Defaults to (1280, 720). (1280, 720) n_ratios int Number of ratios. Defaults to 3. 3 n_scales int Number of scales. Defaults to 3. 3 strides Optional [ List [ int ]] List of strides. Defatults to [4, 8, 16, 32, 64]. None base_sizes Optional [ List [ int ]] The basic sizes of anchors in multiple levels. If None is given, strides will be used as base_sizes. None show bool Show results or not. Defaults to True. True output Optional [ str ] Output directory where results going to be saved. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) keep_ratio bool Whether to keep the aspect ratio or not. Defaults to False. False evaluate bool Whether to evaluate or not the anchors. Check pyodi train-config evaluation for more information. True Returns: Type Description None Anchor generator instance. Source code in pyodi/apps/train_config/train_config_generation.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 @logger . catch ( reraise = True ) def train_config_generation ( ground_truth_file : str , input_size : Tuple [ int , int ] = ( 1280 , 720 ), n_ratios : int = 3 , n_scales : int = 3 , strides : Optional [ List [ int ]] = None , base_sizes : Optional [ List [ int ]] = None , show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), keep_ratio : bool = False , evaluate : bool = True , ) -> None : \"\"\"Computes optimal anchors for a given COCO dataset based on iou clustering. Args: ground_truth_file: Path to COCO ground truth file. input_size: Model image input size. Defaults to (1280, 720). n_ratios: Number of ratios. Defaults to 3. n_scales: Number of scales. Defaults to 3. strides: List of strides. Defatults to [4, 8, 16, 32, 64]. base_sizes: The basic sizes of anchors in multiple levels. If None is given, strides will be used as base_sizes. show: Show results or not. Defaults to True. output: Output directory where results going to be saved. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). keep_ratio: Whether to keep the aspect ratio or not. Defaults to False. evaluate: Whether to evaluate or not the anchors. Check [`pyodi train-config evaluation`][pyodi.apps.train_config.train_config_evaluation.train_config_evaluation] for more information. Returns: Anchor generator instance. \"\"\" if output is not None : Path ( output ) . mkdir ( parents = True , exist_ok = True ) df_annotations = coco_ground_truth_to_df ( ground_truth_file ) df_annotations = filter_zero_area_bboxes ( df_annotations ) df_annotations = scale_bbox_dimensions ( df_annotations , input_size = input_size , keep_ratio = keep_ratio ) df_annotations = get_scale_and_ratio ( df_annotations , prefix = \"scaled\" ) if strides is None : strides = [ 4 , 8 , 16 , 32 , 64 ] if base_sizes is None : base_sizes = strides # Assign fpn level df_annotations [ \"fpn_level\" ] = find_pyramid_level ( get_bbox_array ( df_annotations , prefix = \"scaled\" )[:, 2 :], base_sizes ) df_annotations [ \"fpn_level_scale\" ] = df_annotations [ \"fpn_level\" ] . replace ( { i : scale for i , scale in enumerate ( base_sizes )} ) df_annotations [ \"level_scale\" ] = ( df_annotations [ \"scaled_scale\" ] / df_annotations [ \"fpn_level_scale\" ] ) # Normalize to log scale df_annotations [ \"log_ratio\" ] = np . log ( df_annotations [ \"scaled_ratio\" ]) df_annotations [ \"log_level_scale\" ] = np . log ( df_annotations [ \"level_scale\" ]) # Cluster bboxes by scale and ratio independently clustering_results = [ kmeans_euclidean ( df_annotations [ value ] . to_numpy (), n_clusters = n_clusters ) for i , ( value , n_clusters ) in enumerate ( zip ([ \"log_level_scale\" , \"log_ratio\" ], [ n_scales , n_ratios ]) ) ] # Bring back from log scale scales = np . e ** clustering_results [ 0 ][ \"centroids\" ] ratios = np . e ** clustering_results [ 1 ][ \"centroids\" ] anchor_generator = AnchorGenerator ( strides = strides , ratios = ratios , scales = scales , base_sizes = base_sizes , ) logger . info ( f \"Anchor configuration: \\n { anchor_generator . to_string () } \" ) plot_clustering_results ( df_annotations , anchor_generator , show = show , output = output , output_size = output_size , title = \"COCO_anchor_generation\" , ) if evaluate : anchor_config = dict ( anchor_generator = anchor_generator . to_dict ()) train_config_evaluation ( ground_truth_file = df_annotations , anchor_config = anchor_config , # type: ignore input_size = input_size , show = show , output = output , output_size = output_size , ) if output : output_file = Path ( output ) / \"anchor_config.py\" with open ( output_file , \"w\" ) as f : f . write ( anchor_generator . to_string ())","title":"train_config_generation()"},{"location":"reference/core/anchor_generator/","text":"AnchorGenerator Bases: object Standard anchor generator for 2D anchor-based detectors. Parameters: Name Type Description Default strides list [ int ] Strides of anchors in multiple feture levels. required ratios list [ float ] The list of ratios between the height and width of anchors in a single level. required scales list [ int ] | None Anchor scales for anchors in a single level. It cannot be set at the same time if octave_base_scale and scales_per_octave are set. None base_sizes list [ int ] | None The basic sizes of anchors in multiple levels. If None is given, strides will be used as base_sizes. None scale_major bool Whether to multiply scales first when generating base anchors. If true, the anchors in the same row will have the same scales. By default it is True in V2.0 True octave_base_scale int The base scale of octave. None scales_per_octave int Number of scales for each octave. octave_base_scale and scales_per_octave are usually used in retinanet and the scales should be None when they are set. None centers list [ tuple [ float , float ]] | None The centers of the anchor relative to the feature grid center in multiple feature levels. By default it is set to be None and not used. If a list of tuple of float is given, they will be used to shift the centers of anchors. None center_offset float The offset of center in propotion to anchors' width and height. By default it is 0 in V2.0. 0.0 Examples: >>> from mmdet.core import AnchorGenerator >>> self = AnchorGenerator ([ 16 ], [ 1. ], [ 1. ], [ 9 ]) >>> all_anchors = self . grid_anchors ([( 2 , 2 )], device = 'cpu' ) >>> print ( all_anchors ) [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]])] >>> self = AnchorGenerator ([ 16 , 32 ], [ 1. ], [ 1. ], [ 9 , 18 ]) >>> all_anchors = self . grid_anchors ([( 2 , 2 ), ( 1 , 1 )], device = 'cpu' ) >>> print ( all_anchors ) [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]]), tensor([[-9., -9., 9., 9.]])] Source code in pyodi/core/anchor_generator.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 class AnchorGenerator ( object ): \"\"\"Standard anchor generator for 2D anchor-based detectors. Args: strides (list[int]): Strides of anchors in multiple feture levels. ratios (list[float]): The list of ratios between the height and width of anchors in a single level. scales (list[int] | None): Anchor scales for anchors in a single level. It cannot be set at the same time if `octave_base_scale` and `scales_per_octave` are set. base_sizes (list[int] | None): The basic sizes of anchors in multiple levels. If None is given, strides will be used as base_sizes. scale_major (bool): Whether to multiply scales first when generating base anchors. If true, the anchors in the same row will have the same scales. By default it is True in V2.0 octave_base_scale (int): The base scale of octave. scales_per_octave (int): Number of scales for each octave. `octave_base_scale` and `scales_per_octave` are usually used in retinanet and the `scales` should be None when they are set. centers (list[tuple[float, float]] | None): The centers of the anchor relative to the feature grid center in multiple feature levels. By default it is set to be None and not used. If a list of tuple of float is given, they will be used to shift the centers of anchors. center_offset (float): The offset of center in propotion to anchors' width and height. By default it is 0 in V2.0. Examples: >>> from mmdet.core import AnchorGenerator >>> self = AnchorGenerator([16], [1.], [1.], [9]) >>> all_anchors = self.grid_anchors([(2, 2)], device='cpu') >>> print(all_anchors) [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]])] >>> self = AnchorGenerator([16, 32], [1.], [1.], [9, 18]) >>> all_anchors = self.grid_anchors([(2, 2), (1, 1)], device='cpu') >>> print(all_anchors) [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]]), \\ tensor([[-9., -9., 9., 9.]])] \"\"\" def __init__ ( self , strides : List [ int ], ratios : List [ float ], scales : Optional [ List [ float ]] = None , base_sizes : Optional [ List [ int ]] = None , scale_major : bool = True , octave_base_scale : Optional [ int ] = None , scales_per_octave : Optional [ int ] = None , centers : Optional [ List [ Tuple [ float , float ]]] = None , center_offset : float = 0.0 , ) -> None : # check center and center_offset if center_offset != 0 : assert centers is None , ( \"center cannot be set when center_offset\" \"!=0, {} is given.\" . format ( centers ) ) if not ( 0 <= center_offset <= 1 ): raise ValueError ( \"center_offset should be in range [0, 1], {} is\" \" given.\" . format ( center_offset ) ) if centers is not None : assert len ( centers ) == len ( strides ), ( \"The number of strides should be the same as centers, got \" \" {} and {} \" . format ( strides , centers ) ) # calculate base sizes of anchors self . strides = strides self . base_sizes = list ( strides ) if base_sizes is None else base_sizes assert len ( self . base_sizes ) == len ( self . strides ), ( \"The number of strides should be the same as base sizes, got \" \" {} and {} \" . format ( self . strides , self . base_sizes ) ) # calculate scales of anchors assert ( octave_base_scale is not None and scales_per_octave is not None ) ^ ( scales is not None ), ( \"scales and octave_base_scale with scales_per_octave cannot\" \" be set at the same time\" ) if scales is not None : self . scales = np . array ( scales ) elif octave_base_scale is not None and scales_per_octave is not None : octave_scales = np . array ( [ 2 ** ( i / scales_per_octave ) for i in range ( scales_per_octave )] ) scales = octave_scales * octave_base_scale self . scales = np . array ( scales ) else : raise ValueError ( \"Either scales or octave_base_scale with \" \"scales_per_octave should be set\" ) self . octave_base_scale = octave_base_scale self . scales_per_octave = scales_per_octave self . ratios = np . array ( ratios ) self . scale_major = scale_major self . centers = centers self . center_offset = center_offset self . base_anchors = self . gen_base_anchors () @property def num_levels ( self ) -> int : \"\"\"Returns the number of levels. Returns: Number of levels. \"\"\" return len ( self . strides ) def gen_base_anchors ( self ) -> List [ ndarray ]: \"\"\"Computes the anchors. Returns: List of arrays with the anchors. \"\"\" multi_level_base_anchors = [] for i , base_size in enumerate ( self . base_sizes ): center = None if self . centers is not None : center = self . centers [ i ] multi_level_base_anchors . append ( self . gen_single_level_base_anchors ( base_size , scales = self . scales , ratios = self . ratios , center = center ) ) return multi_level_base_anchors def gen_single_level_base_anchors ( self , base_size : int , scales : ndarray , ratios : ndarray , center : Optional [ Tuple [ float , float ]] = None , ) -> ndarray : \"\"\"Computes the anchors of a single level. Args: base_size: Basic size of the anchors in a single level. scales: Anchor scales for anchors in a single level ratios: Ratios between height and width of anchors in a single level. center: Center of the anchor relative to the feature grid center in single level. Returns: Array with the anchors. \"\"\" w = base_size h = base_size if center is None : x_center = self . center_offset * w y_center = self . center_offset * h else : x_center , y_center = center h_ratios = np . sqrt ( ratios ) w_ratios = 1 / h_ratios if self . scale_major : ws = ( w * w_ratios [:, None ] * scales [ None , :]) . flatten () hs = ( h * h_ratios [:, None ] * scales [ None , :]) . flatten () else : ws = ( w * scales [:, None ] * w_ratios [ None , :]) . flatten () hs = ( h * scales [:, None ] * h_ratios [ None , :]) . flatten () # use float anchor and the anchor's center is aligned with the # pixel center base_anchors = [ x_center - 0.5 * ws , y_center - 0.5 * hs , x_center + 0.5 * ws , y_center + 0.5 * hs , ] base_anchors = np . stack ( base_anchors , axis =- 1 ) return base_anchors def _meshgrid ( self , x : ndarray , y : ndarray , row_major : bool = True ) -> Tuple [ ndarray , ndarray ]: xx = np . tile ( x , len ( y )) # yy = y.view(-1, 1).repeat(1, len(x)).view(-1) yy = np . tile ( np . reshape ( y , [ - 1 , 1 ]), ( 1 , len ( x ))) . flatten () if row_major : return xx , yy else : return yy , xx def grid_anchors ( self , featmap_sizes : List [ Tuple [ int , int ]]) -> List [ ndarray ]: \"\"\"Generate grid anchors in multiple feature levels. Args: featmap_sizes: List of feature map sizes in multiple feature levels. Returns: Anchors in multiple feature levels. The sizes of each tensor should be [N, 4], where N = width * height * num_base_anchors, width and height are the sizes of the corresponding feature level, num_base_anchors is the number of anchors for that level. \"\"\" assert self . num_levels == len ( featmap_sizes ) multi_level_anchors = [] for i in range ( self . num_levels ): anchors = self . single_level_grid_anchors ( self . base_anchors [ i ], featmap_sizes [ i ], self . strides [ i ], ) multi_level_anchors . append ( anchors ) return multi_level_anchors def single_level_grid_anchors ( self , base_anchors : ndarray , featmap_size : Tuple [ int , int ], stride : int = 16 ) -> ndarray : \"\"\"Generate grid anchors in a single feature level. Args: base_anchors: Anchors in a single level. featmap_size: Feature map size in a single level. stride: Number of stride. Defaults to 16. Returns: Grid of anchors in a single feature level. \"\"\" feat_h , feat_w = featmap_size shift_x = np . arange ( 0 , feat_w ) * stride shift_y = np . arange ( 0 , feat_h ) * stride shift_xx , shift_yy = self . _meshgrid ( shift_x , shift_y ) shifts = np . stack ([ shift_xx , shift_yy , shift_xx , shift_yy ], axis =- 1 ) shifts = shifts . astype ( base_anchors . dtype ) # first feat_w elements correspond to the first row of shifts # add A anchors (1, A, 4) to K shifts (K, 1, 4) to get # shifted anchors (K, A, 4), reshape to (K*A, 4) all_anchors = base_anchors [ None , :, :] + shifts [:, None , :] all_anchors = np . reshape ( all_anchors , [ - 1 , 4 ]) # first A rows correspond to A anchors of (0, 0) in feature map, # then (0, 1), (0, 2), ... return all_anchors def __repr__ ( self ) -> str : indent_str = \" \" repr_str = self . __class__ . __name__ + \"( \\n \" repr_str += \" {} strides= {} , \\n \" . format ( indent_str , self . strides ) repr_str += \" {} ratios= {} , \\n \" . format ( indent_str , self . ratios ) repr_str += \" {} scales= {} , \\n \" . format ( indent_str , self . scales ) repr_str += \" {} base_sizes= {} , \\n \" . format ( indent_str , self . base_sizes ) repr_str += \" {} scale_major= {} , \\n \" . format ( indent_str , self . scale_major ) repr_str += \" {} octave_base_scale= {} , \\n \" . format ( indent_str , self . octave_base_scale ) repr_str += \" {} scales_per_octave= {} , \\n \" . format ( indent_str , self . scales_per_octave ) repr_str += \" {} num_levels= {} , \\n \" . format ( indent_str , self . num_levels ) repr_str += \" {} centers= {} , \\n \" . format ( indent_str , self . centers ) repr_str += \" {} center_offset= {} )\" . format ( indent_str , self . center_offset ) return repr_str def to_string ( self ) -> str : \"\"\"Transforms configuration into string. Returns: String with config. \"\"\" anchor_config = self . to_dict () string = \"anchor_generator=dict( \\n \" for k , v in anchor_config . items (): string += f \" { ' ' * 4 }{ k } = { v } , \\n \" string += \")\" return string def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Transforms configuration into dictionary. Returns: Dictionary with config. \"\"\" anchor_config = dict ( type = \"'AnchorGenerator'\" , scales = sorted ( list ( self . scales . ravel ())), ratios = sorted ( list ( self . ratios . ravel ())), strides = list ( self . strides ), base_sizes = list ( self . base_sizes ), ) return anchor_config num_levels : int property Returns the number of levels. Returns: Type Description int Number of levels. gen_base_anchors () Computes the anchors. Returns: Type Description List [ ndarray ] List of arrays with the anchors. Source code in pyodi/core/anchor_generator.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def gen_base_anchors ( self ) -> List [ ndarray ]: \"\"\"Computes the anchors. Returns: List of arrays with the anchors. \"\"\" multi_level_base_anchors = [] for i , base_size in enumerate ( self . base_sizes ): center = None if self . centers is not None : center = self . centers [ i ] multi_level_base_anchors . append ( self . gen_single_level_base_anchors ( base_size , scales = self . scales , ratios = self . ratios , center = center ) ) return multi_level_base_anchors gen_single_level_base_anchors ( base_size , scales , ratios , center = None ) Computes the anchors of a single level. Parameters: Name Type Description Default base_size int Basic size of the anchors in a single level. required scales ndarray Anchor scales for anchors in a single level required ratios ndarray Ratios between height and width of anchors in a single level. required center Optional [ Tuple [ float , float ]] Center of the anchor relative to the feature grid center in single level. None Returns: Type Description ndarray Array with the anchors. Source code in pyodi/core/anchor_generator.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def gen_single_level_base_anchors ( self , base_size : int , scales : ndarray , ratios : ndarray , center : Optional [ Tuple [ float , float ]] = None , ) -> ndarray : \"\"\"Computes the anchors of a single level. Args: base_size: Basic size of the anchors in a single level. scales: Anchor scales for anchors in a single level ratios: Ratios between height and width of anchors in a single level. center: Center of the anchor relative to the feature grid center in single level. Returns: Array with the anchors. \"\"\" w = base_size h = base_size if center is None : x_center = self . center_offset * w y_center = self . center_offset * h else : x_center , y_center = center h_ratios = np . sqrt ( ratios ) w_ratios = 1 / h_ratios if self . scale_major : ws = ( w * w_ratios [:, None ] * scales [ None , :]) . flatten () hs = ( h * h_ratios [:, None ] * scales [ None , :]) . flatten () else : ws = ( w * scales [:, None ] * w_ratios [ None , :]) . flatten () hs = ( h * scales [:, None ] * h_ratios [ None , :]) . flatten () # use float anchor and the anchor's center is aligned with the # pixel center base_anchors = [ x_center - 0.5 * ws , y_center - 0.5 * hs , x_center + 0.5 * ws , y_center + 0.5 * hs , ] base_anchors = np . stack ( base_anchors , axis =- 1 ) return base_anchors grid_anchors ( featmap_sizes ) Generate grid anchors in multiple feature levels. Parameters: Name Type Description Default featmap_sizes List [ Tuple [ int , int ]] List of feature map sizes in multiple feature levels. required Returns: Type Description List [ ndarray ] Anchors in multiple feature levels. The sizes of each tensor should be List [ ndarray ] [N, 4], where N = width * height * num_base_anchors, width and height are List [ ndarray ] the sizes of the corresponding feature level, num_base_anchors is the List [ ndarray ] number of anchors for that level. Source code in pyodi/core/anchor_generator.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def grid_anchors ( self , featmap_sizes : List [ Tuple [ int , int ]]) -> List [ ndarray ]: \"\"\"Generate grid anchors in multiple feature levels. Args: featmap_sizes: List of feature map sizes in multiple feature levels. Returns: Anchors in multiple feature levels. The sizes of each tensor should be [N, 4], where N = width * height * num_base_anchors, width and height are the sizes of the corresponding feature level, num_base_anchors is the number of anchors for that level. \"\"\" assert self . num_levels == len ( featmap_sizes ) multi_level_anchors = [] for i in range ( self . num_levels ): anchors = self . single_level_grid_anchors ( self . base_anchors [ i ], featmap_sizes [ i ], self . strides [ i ], ) multi_level_anchors . append ( anchors ) return multi_level_anchors single_level_grid_anchors ( base_anchors , featmap_size , stride = 16 ) Generate grid anchors in a single feature level. Parameters: Name Type Description Default base_anchors ndarray Anchors in a single level. required featmap_size Tuple [ int , int ] Feature map size in a single level. required stride int Number of stride. Defaults to 16. 16 Returns: Type Description ndarray Grid of anchors in a single feature level. Source code in pyodi/core/anchor_generator.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def single_level_grid_anchors ( self , base_anchors : ndarray , featmap_size : Tuple [ int , int ], stride : int = 16 ) -> ndarray : \"\"\"Generate grid anchors in a single feature level. Args: base_anchors: Anchors in a single level. featmap_size: Feature map size in a single level. stride: Number of stride. Defaults to 16. Returns: Grid of anchors in a single feature level. \"\"\" feat_h , feat_w = featmap_size shift_x = np . arange ( 0 , feat_w ) * stride shift_y = np . arange ( 0 , feat_h ) * stride shift_xx , shift_yy = self . _meshgrid ( shift_x , shift_y ) shifts = np . stack ([ shift_xx , shift_yy , shift_xx , shift_yy ], axis =- 1 ) shifts = shifts . astype ( base_anchors . dtype ) # first feat_w elements correspond to the first row of shifts # add A anchors (1, A, 4) to K shifts (K, 1, 4) to get # shifted anchors (K, A, 4), reshape to (K*A, 4) all_anchors = base_anchors [ None , :, :] + shifts [:, None , :] all_anchors = np . reshape ( all_anchors , [ - 1 , 4 ]) # first A rows correspond to A anchors of (0, 0) in feature map, # then (0, 1), (0, 2), ... return all_anchors to_dict () Transforms configuration into dictionary. Returns: Type Description Dict [ str , Any ] Dictionary with config. Source code in pyodi/core/anchor_generator.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Transforms configuration into dictionary. Returns: Dictionary with config. \"\"\" anchor_config = dict ( type = \"'AnchorGenerator'\" , scales = sorted ( list ( self . scales . ravel ())), ratios = sorted ( list ( self . ratios . ravel ())), strides = list ( self . strides ), base_sizes = list ( self . base_sizes ), ) return anchor_config to_string () Transforms configuration into string. Returns: Type Description str String with config. Source code in pyodi/core/anchor_generator.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def to_string ( self ) -> str : \"\"\"Transforms configuration into string. Returns: String with config. \"\"\" anchor_config = self . to_dict () string = \"anchor_generator=dict( \\n \" for k , v in anchor_config . items (): string += f \" { ' ' * 4 }{ k } = { v } , \\n \" string += \")\" return string","title":"anchor_generator"},{"location":"reference/core/anchor_generator/#pyodi.core.anchor_generator.AnchorGenerator","text":"Bases: object Standard anchor generator for 2D anchor-based detectors. Parameters: Name Type Description Default strides list [ int ] Strides of anchors in multiple feture levels. required ratios list [ float ] The list of ratios between the height and width of anchors in a single level. required scales list [ int ] | None Anchor scales for anchors in a single level. It cannot be set at the same time if octave_base_scale and scales_per_octave are set. None base_sizes list [ int ] | None The basic sizes of anchors in multiple levels. If None is given, strides will be used as base_sizes. None scale_major bool Whether to multiply scales first when generating base anchors. If true, the anchors in the same row will have the same scales. By default it is True in V2.0 True octave_base_scale int The base scale of octave. None scales_per_octave int Number of scales for each octave. octave_base_scale and scales_per_octave are usually used in retinanet and the scales should be None when they are set. None centers list [ tuple [ float , float ]] | None The centers of the anchor relative to the feature grid center in multiple feature levels. By default it is set to be None and not used. If a list of tuple of float is given, they will be used to shift the centers of anchors. None center_offset float The offset of center in propotion to anchors' width and height. By default it is 0 in V2.0. 0.0 Examples: >>> from mmdet.core import AnchorGenerator >>> self = AnchorGenerator ([ 16 ], [ 1. ], [ 1. ], [ 9 ]) >>> all_anchors = self . grid_anchors ([( 2 , 2 )], device = 'cpu' ) >>> print ( all_anchors ) [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]])] >>> self = AnchorGenerator ([ 16 , 32 ], [ 1. ], [ 1. ], [ 9 , 18 ]) >>> all_anchors = self . grid_anchors ([( 2 , 2 ), ( 1 , 1 )], device = 'cpu' ) >>> print ( all_anchors ) [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]]), tensor([[-9., -9., 9., 9.]])] Source code in pyodi/core/anchor_generator.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 class AnchorGenerator ( object ): \"\"\"Standard anchor generator for 2D anchor-based detectors. Args: strides (list[int]): Strides of anchors in multiple feture levels. ratios (list[float]): The list of ratios between the height and width of anchors in a single level. scales (list[int] | None): Anchor scales for anchors in a single level. It cannot be set at the same time if `octave_base_scale` and `scales_per_octave` are set. base_sizes (list[int] | None): The basic sizes of anchors in multiple levels. If None is given, strides will be used as base_sizes. scale_major (bool): Whether to multiply scales first when generating base anchors. If true, the anchors in the same row will have the same scales. By default it is True in V2.0 octave_base_scale (int): The base scale of octave. scales_per_octave (int): Number of scales for each octave. `octave_base_scale` and `scales_per_octave` are usually used in retinanet and the `scales` should be None when they are set. centers (list[tuple[float, float]] | None): The centers of the anchor relative to the feature grid center in multiple feature levels. By default it is set to be None and not used. If a list of tuple of float is given, they will be used to shift the centers of anchors. center_offset (float): The offset of center in propotion to anchors' width and height. By default it is 0 in V2.0. Examples: >>> from mmdet.core import AnchorGenerator >>> self = AnchorGenerator([16], [1.], [1.], [9]) >>> all_anchors = self.grid_anchors([(2, 2)], device='cpu') >>> print(all_anchors) [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]])] >>> self = AnchorGenerator([16, 32], [1.], [1.], [9, 18]) >>> all_anchors = self.grid_anchors([(2, 2), (1, 1)], device='cpu') >>> print(all_anchors) [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]]), \\ tensor([[-9., -9., 9., 9.]])] \"\"\" def __init__ ( self , strides : List [ int ], ratios : List [ float ], scales : Optional [ List [ float ]] = None , base_sizes : Optional [ List [ int ]] = None , scale_major : bool = True , octave_base_scale : Optional [ int ] = None , scales_per_octave : Optional [ int ] = None , centers : Optional [ List [ Tuple [ float , float ]]] = None , center_offset : float = 0.0 , ) -> None : # check center and center_offset if center_offset != 0 : assert centers is None , ( \"center cannot be set when center_offset\" \"!=0, {} is given.\" . format ( centers ) ) if not ( 0 <= center_offset <= 1 ): raise ValueError ( \"center_offset should be in range [0, 1], {} is\" \" given.\" . format ( center_offset ) ) if centers is not None : assert len ( centers ) == len ( strides ), ( \"The number of strides should be the same as centers, got \" \" {} and {} \" . format ( strides , centers ) ) # calculate base sizes of anchors self . strides = strides self . base_sizes = list ( strides ) if base_sizes is None else base_sizes assert len ( self . base_sizes ) == len ( self . strides ), ( \"The number of strides should be the same as base sizes, got \" \" {} and {} \" . format ( self . strides , self . base_sizes ) ) # calculate scales of anchors assert ( octave_base_scale is not None and scales_per_octave is not None ) ^ ( scales is not None ), ( \"scales and octave_base_scale with scales_per_octave cannot\" \" be set at the same time\" ) if scales is not None : self . scales = np . array ( scales ) elif octave_base_scale is not None and scales_per_octave is not None : octave_scales = np . array ( [ 2 ** ( i / scales_per_octave ) for i in range ( scales_per_octave )] ) scales = octave_scales * octave_base_scale self . scales = np . array ( scales ) else : raise ValueError ( \"Either scales or octave_base_scale with \" \"scales_per_octave should be set\" ) self . octave_base_scale = octave_base_scale self . scales_per_octave = scales_per_octave self . ratios = np . array ( ratios ) self . scale_major = scale_major self . centers = centers self . center_offset = center_offset self . base_anchors = self . gen_base_anchors () @property def num_levels ( self ) -> int : \"\"\"Returns the number of levels. Returns: Number of levels. \"\"\" return len ( self . strides ) def gen_base_anchors ( self ) -> List [ ndarray ]: \"\"\"Computes the anchors. Returns: List of arrays with the anchors. \"\"\" multi_level_base_anchors = [] for i , base_size in enumerate ( self . base_sizes ): center = None if self . centers is not None : center = self . centers [ i ] multi_level_base_anchors . append ( self . gen_single_level_base_anchors ( base_size , scales = self . scales , ratios = self . ratios , center = center ) ) return multi_level_base_anchors def gen_single_level_base_anchors ( self , base_size : int , scales : ndarray , ratios : ndarray , center : Optional [ Tuple [ float , float ]] = None , ) -> ndarray : \"\"\"Computes the anchors of a single level. Args: base_size: Basic size of the anchors in a single level. scales: Anchor scales for anchors in a single level ratios: Ratios between height and width of anchors in a single level. center: Center of the anchor relative to the feature grid center in single level. Returns: Array with the anchors. \"\"\" w = base_size h = base_size if center is None : x_center = self . center_offset * w y_center = self . center_offset * h else : x_center , y_center = center h_ratios = np . sqrt ( ratios ) w_ratios = 1 / h_ratios if self . scale_major : ws = ( w * w_ratios [:, None ] * scales [ None , :]) . flatten () hs = ( h * h_ratios [:, None ] * scales [ None , :]) . flatten () else : ws = ( w * scales [:, None ] * w_ratios [ None , :]) . flatten () hs = ( h * scales [:, None ] * h_ratios [ None , :]) . flatten () # use float anchor and the anchor's center is aligned with the # pixel center base_anchors = [ x_center - 0.5 * ws , y_center - 0.5 * hs , x_center + 0.5 * ws , y_center + 0.5 * hs , ] base_anchors = np . stack ( base_anchors , axis =- 1 ) return base_anchors def _meshgrid ( self , x : ndarray , y : ndarray , row_major : bool = True ) -> Tuple [ ndarray , ndarray ]: xx = np . tile ( x , len ( y )) # yy = y.view(-1, 1).repeat(1, len(x)).view(-1) yy = np . tile ( np . reshape ( y , [ - 1 , 1 ]), ( 1 , len ( x ))) . flatten () if row_major : return xx , yy else : return yy , xx def grid_anchors ( self , featmap_sizes : List [ Tuple [ int , int ]]) -> List [ ndarray ]: \"\"\"Generate grid anchors in multiple feature levels. Args: featmap_sizes: List of feature map sizes in multiple feature levels. Returns: Anchors in multiple feature levels. The sizes of each tensor should be [N, 4], where N = width * height * num_base_anchors, width and height are the sizes of the corresponding feature level, num_base_anchors is the number of anchors for that level. \"\"\" assert self . num_levels == len ( featmap_sizes ) multi_level_anchors = [] for i in range ( self . num_levels ): anchors = self . single_level_grid_anchors ( self . base_anchors [ i ], featmap_sizes [ i ], self . strides [ i ], ) multi_level_anchors . append ( anchors ) return multi_level_anchors def single_level_grid_anchors ( self , base_anchors : ndarray , featmap_size : Tuple [ int , int ], stride : int = 16 ) -> ndarray : \"\"\"Generate grid anchors in a single feature level. Args: base_anchors: Anchors in a single level. featmap_size: Feature map size in a single level. stride: Number of stride. Defaults to 16. Returns: Grid of anchors in a single feature level. \"\"\" feat_h , feat_w = featmap_size shift_x = np . arange ( 0 , feat_w ) * stride shift_y = np . arange ( 0 , feat_h ) * stride shift_xx , shift_yy = self . _meshgrid ( shift_x , shift_y ) shifts = np . stack ([ shift_xx , shift_yy , shift_xx , shift_yy ], axis =- 1 ) shifts = shifts . astype ( base_anchors . dtype ) # first feat_w elements correspond to the first row of shifts # add A anchors (1, A, 4) to K shifts (K, 1, 4) to get # shifted anchors (K, A, 4), reshape to (K*A, 4) all_anchors = base_anchors [ None , :, :] + shifts [:, None , :] all_anchors = np . reshape ( all_anchors , [ - 1 , 4 ]) # first A rows correspond to A anchors of (0, 0) in feature map, # then (0, 1), (0, 2), ... return all_anchors def __repr__ ( self ) -> str : indent_str = \" \" repr_str = self . __class__ . __name__ + \"( \\n \" repr_str += \" {} strides= {} , \\n \" . format ( indent_str , self . strides ) repr_str += \" {} ratios= {} , \\n \" . format ( indent_str , self . ratios ) repr_str += \" {} scales= {} , \\n \" . format ( indent_str , self . scales ) repr_str += \" {} base_sizes= {} , \\n \" . format ( indent_str , self . base_sizes ) repr_str += \" {} scale_major= {} , \\n \" . format ( indent_str , self . scale_major ) repr_str += \" {} octave_base_scale= {} , \\n \" . format ( indent_str , self . octave_base_scale ) repr_str += \" {} scales_per_octave= {} , \\n \" . format ( indent_str , self . scales_per_octave ) repr_str += \" {} num_levels= {} , \\n \" . format ( indent_str , self . num_levels ) repr_str += \" {} centers= {} , \\n \" . format ( indent_str , self . centers ) repr_str += \" {} center_offset= {} )\" . format ( indent_str , self . center_offset ) return repr_str def to_string ( self ) -> str : \"\"\"Transforms configuration into string. Returns: String with config. \"\"\" anchor_config = self . to_dict () string = \"anchor_generator=dict( \\n \" for k , v in anchor_config . items (): string += f \" { ' ' * 4 }{ k } = { v } , \\n \" string += \")\" return string def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Transforms configuration into dictionary. Returns: Dictionary with config. \"\"\" anchor_config = dict ( type = \"'AnchorGenerator'\" , scales = sorted ( list ( self . scales . ravel ())), ratios = sorted ( list ( self . ratios . ravel ())), strides = list ( self . strides ), base_sizes = list ( self . base_sizes ), ) return anchor_config","title":"AnchorGenerator"},{"location":"reference/core/anchor_generator/#pyodi.core.anchor_generator.AnchorGenerator.num_levels","text":"Returns the number of levels. Returns: Type Description int Number of levels.","title":"num_levels"},{"location":"reference/core/anchor_generator/#pyodi.core.anchor_generator.AnchorGenerator.gen_base_anchors","text":"Computes the anchors. Returns: Type Description List [ ndarray ] List of arrays with the anchors. Source code in pyodi/core/anchor_generator.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def gen_base_anchors ( self ) -> List [ ndarray ]: \"\"\"Computes the anchors. Returns: List of arrays with the anchors. \"\"\" multi_level_base_anchors = [] for i , base_size in enumerate ( self . base_sizes ): center = None if self . centers is not None : center = self . centers [ i ] multi_level_base_anchors . append ( self . gen_single_level_base_anchors ( base_size , scales = self . scales , ratios = self . ratios , center = center ) ) return multi_level_base_anchors","title":"gen_base_anchors()"},{"location":"reference/core/anchor_generator/#pyodi.core.anchor_generator.AnchorGenerator.gen_single_level_base_anchors","text":"Computes the anchors of a single level. Parameters: Name Type Description Default base_size int Basic size of the anchors in a single level. required scales ndarray Anchor scales for anchors in a single level required ratios ndarray Ratios between height and width of anchors in a single level. required center Optional [ Tuple [ float , float ]] Center of the anchor relative to the feature grid center in single level. None Returns: Type Description ndarray Array with the anchors. Source code in pyodi/core/anchor_generator.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def gen_single_level_base_anchors ( self , base_size : int , scales : ndarray , ratios : ndarray , center : Optional [ Tuple [ float , float ]] = None , ) -> ndarray : \"\"\"Computes the anchors of a single level. Args: base_size: Basic size of the anchors in a single level. scales: Anchor scales for anchors in a single level ratios: Ratios between height and width of anchors in a single level. center: Center of the anchor relative to the feature grid center in single level. Returns: Array with the anchors. \"\"\" w = base_size h = base_size if center is None : x_center = self . center_offset * w y_center = self . center_offset * h else : x_center , y_center = center h_ratios = np . sqrt ( ratios ) w_ratios = 1 / h_ratios if self . scale_major : ws = ( w * w_ratios [:, None ] * scales [ None , :]) . flatten () hs = ( h * h_ratios [:, None ] * scales [ None , :]) . flatten () else : ws = ( w * scales [:, None ] * w_ratios [ None , :]) . flatten () hs = ( h * scales [:, None ] * h_ratios [ None , :]) . flatten () # use float anchor and the anchor's center is aligned with the # pixel center base_anchors = [ x_center - 0.5 * ws , y_center - 0.5 * hs , x_center + 0.5 * ws , y_center + 0.5 * hs , ] base_anchors = np . stack ( base_anchors , axis =- 1 ) return base_anchors","title":"gen_single_level_base_anchors()"},{"location":"reference/core/anchor_generator/#pyodi.core.anchor_generator.AnchorGenerator.grid_anchors","text":"Generate grid anchors in multiple feature levels. Parameters: Name Type Description Default featmap_sizes List [ Tuple [ int , int ]] List of feature map sizes in multiple feature levels. required Returns: Type Description List [ ndarray ] Anchors in multiple feature levels. The sizes of each tensor should be List [ ndarray ] [N, 4], where N = width * height * num_base_anchors, width and height are List [ ndarray ] the sizes of the corresponding feature level, num_base_anchors is the List [ ndarray ] number of anchors for that level. Source code in pyodi/core/anchor_generator.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def grid_anchors ( self , featmap_sizes : List [ Tuple [ int , int ]]) -> List [ ndarray ]: \"\"\"Generate grid anchors in multiple feature levels. Args: featmap_sizes: List of feature map sizes in multiple feature levels. Returns: Anchors in multiple feature levels. The sizes of each tensor should be [N, 4], where N = width * height * num_base_anchors, width and height are the sizes of the corresponding feature level, num_base_anchors is the number of anchors for that level. \"\"\" assert self . num_levels == len ( featmap_sizes ) multi_level_anchors = [] for i in range ( self . num_levels ): anchors = self . single_level_grid_anchors ( self . base_anchors [ i ], featmap_sizes [ i ], self . strides [ i ], ) multi_level_anchors . append ( anchors ) return multi_level_anchors","title":"grid_anchors()"},{"location":"reference/core/anchor_generator/#pyodi.core.anchor_generator.AnchorGenerator.single_level_grid_anchors","text":"Generate grid anchors in a single feature level. Parameters: Name Type Description Default base_anchors ndarray Anchors in a single level. required featmap_size Tuple [ int , int ] Feature map size in a single level. required stride int Number of stride. Defaults to 16. 16 Returns: Type Description ndarray Grid of anchors in a single feature level. Source code in pyodi/core/anchor_generator.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def single_level_grid_anchors ( self , base_anchors : ndarray , featmap_size : Tuple [ int , int ], stride : int = 16 ) -> ndarray : \"\"\"Generate grid anchors in a single feature level. Args: base_anchors: Anchors in a single level. featmap_size: Feature map size in a single level. stride: Number of stride. Defaults to 16. Returns: Grid of anchors in a single feature level. \"\"\" feat_h , feat_w = featmap_size shift_x = np . arange ( 0 , feat_w ) * stride shift_y = np . arange ( 0 , feat_h ) * stride shift_xx , shift_yy = self . _meshgrid ( shift_x , shift_y ) shifts = np . stack ([ shift_xx , shift_yy , shift_xx , shift_yy ], axis =- 1 ) shifts = shifts . astype ( base_anchors . dtype ) # first feat_w elements correspond to the first row of shifts # add A anchors (1, A, 4) to K shifts (K, 1, 4) to get # shifted anchors (K, A, 4), reshape to (K*A, 4) all_anchors = base_anchors [ None , :, :] + shifts [:, None , :] all_anchors = np . reshape ( all_anchors , [ - 1 , 4 ]) # first A rows correspond to A anchors of (0, 0) in feature map, # then (0, 1), (0, 2), ... return all_anchors","title":"single_level_grid_anchors()"},{"location":"reference/core/anchor_generator/#pyodi.core.anchor_generator.AnchorGenerator.to_dict","text":"Transforms configuration into dictionary. Returns: Type Description Dict [ str , Any ] Dictionary with config. Source code in pyodi/core/anchor_generator.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Transforms configuration into dictionary. Returns: Dictionary with config. \"\"\" anchor_config = dict ( type = \"'AnchorGenerator'\" , scales = sorted ( list ( self . scales . ravel ())), ratios = sorted ( list ( self . ratios . ravel ())), strides = list ( self . strides ), base_sizes = list ( self . base_sizes ), ) return anchor_config","title":"to_dict()"},{"location":"reference/core/anchor_generator/#pyodi.core.anchor_generator.AnchorGenerator.to_string","text":"Transforms configuration into string. Returns: Type Description str String with config. Source code in pyodi/core/anchor_generator.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def to_string ( self ) -> str : \"\"\"Transforms configuration into string. Returns: String with config. \"\"\" anchor_config = self . to_dict () string = \"anchor_generator=dict( \\n \" for k , v in anchor_config . items (): string += f \" { ' ' * 4 }{ k } = { v } , \\n \" string += \")\" return string","title":"to_string()"},{"location":"reference/core/boxes/","text":"add_centroids ( df , prefix = None , input_bbox_format = 'coco' ) Computes bbox centroids. Parameters: Name Type Description Default df pd . DataFrame pd.DataFrame with COCO annotations. required prefix str Prefix to apply to column names, use for scaled data. Defaults to None. None input_bbox_format str Input bounding box format. Can be \"coco\" or \"corners\". \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Defaults to \"coco\". 'coco' Returns: Type Description pd . DataFrame pd.DataFrame with new columns [prefix_]row_centroid/col_centroid Source code in pyodi/core/boxes.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def add_centroids ( df : pd . DataFrame , prefix : str = None , input_bbox_format : str = \"coco\" ) -> pd . DataFrame : \"\"\"Computes bbox centroids. Args: df: pd.DataFrame with COCO annotations. prefix: Prefix to apply to column names, use for scaled data. Defaults to None. input_bbox_format: Input bounding box format. Can be \"coco\" or \"corners\". \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Defaults to \"coco\". Returns: pd.DataFrame with new columns [prefix_]row_centroid/col_centroid \"\"\" columns = [ \"col_centroid\" , \"row_centroid\" ] bboxes = get_bbox_array ( df , prefix = prefix , input_bbox_format = input_bbox_format ) if prefix : columns = [ f \" { prefix } _ { col } \" for col in columns ] df [ columns [ 0 ]] = bboxes [:, 0 ] + bboxes [:, 2 ] // 2 df [ columns [ 1 ]] = bboxes [:, 1 ] + bboxes [:, 3 ] // 2 return df check_bbox_formats ( args ) Check if bounding boxes are in a valid format. Source code in pyodi/core/boxes.py 8 9 10 11 12 13 14 def check_bbox_formats ( * args : Any ) -> None : \"\"\"Check if bounding boxes are in a valid format.\"\"\" for arg in args : if not ( arg in [ \"coco\" , \"corners\" ]): raise ValueError ( f \"Invalid format { arg } , only coco and corners format are allowed\" ) coco_to_corners ( bboxes ) Transforms bboxes array from coco format to corners. Parameters: Name Type Description Default bboxes np . ndarray Array with dimension N x 4 with bbox coordinates in corner format required Returns: Type Description np . ndarray Array with dimension N x 4 with bbox coordinates in coco format np . ndarray [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Source code in pyodi/core/boxes.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def coco_to_corners ( bboxes : np . ndarray ) -> np . ndarray : \"\"\"Transforms bboxes array from coco format to corners. Args: bboxes: Array with dimension N x 4 with bbox coordinates in corner format [col_left, row_top, width, height]. Returns: Array with dimension N x 4 with bbox coordinates in coco format [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] \"\"\" bboxes = bboxes . copy () bboxes [ ... , 2 :] = bboxes [ ... , : 2 ] + bboxes [ ... , 2 :] if ( bboxes < 0 ) . any (): logger . warning ( \"Clipping bboxes to min corner 0, found negative value\" ) bboxes = np . clip ( bboxes , 0 , None ) return bboxes corners_to_coco ( bboxes ) Transforms bboxes array from corners format to coco. Parameters: Name Type Description Default bboxes np . ndarray Array with dimension N x 4 with bbox coordinates in corner format required Returns: Type Description np . ndarray Array with dimension N x 4 with bbox coordinates in coco format np . ndarray [col_left, row_top, width, height]. Source code in pyodi/core/boxes.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def corners_to_coco ( bboxes : np . ndarray ) -> np . ndarray : \"\"\"Transforms bboxes array from corners format to coco. Args: bboxes: Array with dimension N x 4 with bbox coordinates in corner format [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Returns: Array with dimension N x 4 with bbox coordinates in coco format [col_left, row_top, width, height]. \"\"\" bboxes = bboxes . copy () bboxes [ ... , 2 :] = bboxes [ ... , 2 :] - bboxes [ ... , : 2 ] return bboxes denormalize ( bboxes , image_width , image_height ) Transforms bboxes array from (0, 1) range to pixels. Bboxes can be in both formats \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Parameters: Name Type Description Default bboxes np . ndarray Bounding boxes. required image_width int Image width in pixels. required image_height int Image height in pixels. required Returns: Type Description np . ndarray Bounding boxes with coordinates in pixels. Source code in pyodi/core/boxes.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def denormalize ( bboxes : np . ndarray , image_width : int , image_height : int ) -> np . ndarray : \"\"\"Transforms bboxes array from (0, 1) range to pixels. Bboxes can be in both formats: \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Args: bboxes: Bounding boxes. image_width: Image width in pixels. image_height: Image height in pixels. Returns: Bounding boxes with coordinates in pixels. \"\"\" norms = np . array ([ image_width , image_height , image_width , image_height ]) bboxes = bboxes * norms return bboxes filter_zero_area_bboxes ( df ) Filters those bboxes with height or width equal to zero. Parameters: Name Type Description Default df pd . DataFrame pd.DataFrame with COCO annotations. required Returns: Type Description pd . DataFrame Filtered pd.DataFrame with COCO annotations. Source code in pyodi/core/boxes.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def filter_zero_area_bboxes ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Filters those bboxes with height or width equal to zero. Args: df: pd.DataFrame with COCO annotations. Returns: Filtered pd.DataFrame with COCO annotations. \"\"\" cols = [ \"width\" , \"height\" ] all_bboxes = len ( df ) df = df [( df [ cols ] > 0 ) . all ( axis = 1 )] . reset_index () filtered_bboxes = len ( df ) n_filtered = all_bboxes - filtered_bboxes if n_filtered : logger . warning ( f \"A total of { n_filtered } bboxes have been filtered from your data \" \"for having area equal to zero.\" ) return df get_bbox_array ( df , prefix = None , input_bbox_format = 'coco' , output_bbox_format = 'coco' ) Returns array with bbox coordinates. Parameters: Name Type Description Default df pd . DataFrame pd.DataFrame with COCO annotations. required prefix Optional [ str ] Prefix to apply to column names, use for scaled data. Defaults to None. None input_bbox_format str Input bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". 'coco' output_bbox_format str Output bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". 'coco' Returns: Type Description np . ndarray Array with dimension N x 4 with bbox coordinates. Examples: coco : >>>[col_left, row_top, width, height] corners : >>>[col_left, row_top, col_right, row_bottom] Source code in pyodi/core/boxes.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def get_bbox_array ( df : pd . DataFrame , prefix : Optional [ str ] = None , input_bbox_format : str = \"coco\" , output_bbox_format : str = \"coco\" , ) -> np . ndarray : \"\"\"Returns array with bbox coordinates. Args: df: pd.DataFrame with COCO annotations. prefix: Prefix to apply to column names, use for scaled data. Defaults to None. input_bbox_format: Input bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". output_bbox_format: Output bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". Returns: Array with dimension N x 4 with bbox coordinates. Examples: `coco`: >>>[col_left, row_top, width, height] `corners`: >>>[col_left, row_top, col_right, row_bottom] \"\"\" check_bbox_formats ( input_bbox_format , output_bbox_format ) columns = get_bbox_column_names ( input_bbox_format , prefix = prefix ) bboxes = df [ columns ] . to_numpy () if input_bbox_format != output_bbox_format : convert = globals ()[ f \" { input_bbox_format } _to_ { output_bbox_format } \" ] bboxes = convert ( bboxes ) return bboxes get_bbox_column_names ( bbox_format , prefix = None ) Returns predefined column names for each format. When bbox_format is 'coco' column names are [\"col_left\", \"row_top\", \"width\", \"height\"], when 'corners' [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"]. Parameters: Name Type Description Default bbox_format str Bounding box format. Can be \"coco\" or \"corners\". required prefix Optional [ str ] Prefix to apply to column names, use for scaled data. Defaults to None. None Returns: Type Description List [ str ] Column names for specified bbox format Source code in pyodi/core/boxes.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def get_bbox_column_names ( bbox_format : str , prefix : Optional [ str ] = None ) -> List [ str ]: \"\"\"Returns predefined column names for each format. When bbox_format is 'coco' column names are [\"col_left\", \"row_top\", \"width\", \"height\"], when 'corners' [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"]. Args: bbox_format: Bounding box format. Can be \"coco\" or \"corners\". prefix: Prefix to apply to column names, use for scaled data. Defaults to None. Returns: Column names for specified bbox format \"\"\" if bbox_format == \"coco\" : columns = [ \"col_left\" , \"row_top\" , \"width\" , \"height\" ] elif bbox_format == \"corners\" : columns = [ \"col_left\" , \"row_top\" , \"col_right\" , \"row_bottom\" ] else : raise ValueError ( f \"Invalid bbox format, { bbox_format } does not exist\" ) if prefix : columns = [ f \" { prefix } _ { col } \" for col in columns ] return columns get_df_from_bboxes ( bboxes , input_bbox_format = 'coco' , output_bbox_format = 'corners' ) Creates pd.DataFrame of annotations in Coco format from array of bboxes. Parameters: Name Type Description Default bboxes np . ndarray Array of bboxes of shape [n, 4]. required input_bbox_format str Input bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". 'coco' output_bbox_format str Output bounding box format. Can be \"coco\" or \"corners\". Defaults to \"corners\". 'corners' Returns: Type Description pd . DataFrame pd.DataFrame with Coco annotations. Source code in pyodi/core/boxes.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def get_df_from_bboxes ( bboxes : np . ndarray , input_bbox_format : str = \"coco\" , output_bbox_format : str = \"corners\" , ) -> pd . DataFrame : \"\"\"Creates pd.DataFrame of annotations in Coco format from array of bboxes. Args: bboxes: Array of bboxes of shape [n, 4]. input_bbox_format: Input bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". output_bbox_format: Output bounding box format. Can be \"coco\" or \"corners\". Defaults to \"corners\". Returns: pd.DataFrame with Coco annotations. \"\"\" check_bbox_formats ( input_bbox_format , output_bbox_format ) if input_bbox_format != output_bbox_format : convert = globals ()[ f \" { input_bbox_format } _to_ { output_bbox_format } \" ] bboxes = convert ( bboxes ) return pd . DataFrame ( bboxes , columns = get_bbox_column_names ( output_bbox_format )) get_scale_and_ratio ( df , prefix = None ) Returns df with area and ratio per bbox measurements. Parameters: Name Type Description Default df pd . DataFrame pd.DataFrame with COCO annotations. required prefix str Prefix to apply to column names, use for scaled data. None Returns: Type Description pd . DataFrame pd.DataFrame with new columns [prefix_]area/ratio Source code in pyodi/core/boxes.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_scale_and_ratio ( df : pd . DataFrame , prefix : str = None ) -> pd . DataFrame : \"\"\"Returns df with area and ratio per bbox measurements. Args: df: pd.DataFrame with COCO annotations. prefix: Prefix to apply to column names, use for scaled data. Returns: pd.DataFrame with new columns [prefix_]area/ratio \"\"\" columns = [ \"width\" , \"height\" , \"scale\" , \"ratio\" ] if prefix : columns = [ f \" { prefix } _ { col } \" for col in columns ] df [ columns [ 2 ]] = np . sqrt ( df [ columns [ 0 ]] * df [ columns [ 1 ]]) df [ columns [ 3 ]] = df [ columns [ 1 ]] / df [ columns [ 0 ]] return df normalize ( bboxes , image_width , image_height ) Transforms bboxes array from pixels to (0, 1) range. Bboxes can be in both formats \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Parameters: Name Type Description Default bboxes np . ndarray Bounding boxes. required image_width int Image width in pixels. required image_height int Image height in pixels. required Returns: Type Description np . ndarray Bounding boxes with coordinates in (0, 1) range. Source code in pyodi/core/boxes.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def normalize ( bboxes : np . ndarray , image_width : int , image_height : int ) -> np . ndarray : \"\"\"Transforms bboxes array from pixels to (0, 1) range. Bboxes can be in both formats: \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Args: bboxes: Bounding boxes. image_width: Image width in pixels. image_height: Image height in pixels. Returns: Bounding boxes with coordinates in (0, 1) range. \"\"\" norms = np . array ([ image_width , image_height , image_width , image_height ]) bboxes = bboxes * 1 / norms return bboxes scale_bbox_dimensions ( df , input_size = ( 1280 , 720 ), keep_ratio = False ) Resizes bboxes dimensions to model input size. Parameters: Name Type Description Default df pd . DataFrame pd.DataFrame with COCO annotations. required input_size Tuple [ int , int ] Model input size. Defaults to (1280, 720). (1280, 720) keep_ratio bool Whether to keep the aspect ratio or not. Defaults to False. False Returns: Type Description pd . DataFrame pd.DataFrame with COCO annotations and scaled image sizes. Source code in pyodi/core/boxes.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def scale_bbox_dimensions ( df : pd . DataFrame , input_size : Tuple [ int , int ] = ( 1280 , 720 ), keep_ratio : bool = False , ) -> pd . DataFrame : \"\"\"Resizes bboxes dimensions to model input size. Args: df: pd.DataFrame with COCO annotations. input_size: Model input size. Defaults to (1280, 720). keep_ratio: Whether to keep the aspect ratio or not. Defaults to False. Returns: pd.DataFrame with COCO annotations and scaled image sizes. \"\"\" if keep_ratio : scale_factor = pd . concat ( [ max ( input_size ) / df [[ \"img_height\" , \"img_width\" ]] . max ( 1 ), min ( input_size ) / df [[ \"img_height\" , \"img_width\" ]] . min ( 1 ), ], axis = 1 , ) . min ( 1 ) w_scale = np . round ( df [ \"img_width\" ] * scale_factor ) / df [ \"img_width\" ] h_scale = np . round ( df [ \"img_height\" ] * scale_factor ) / df [ \"img_height\" ] else : w_scale = input_size [ 0 ] / df [ \"img_width\" ] h_scale = input_size [ 1 ] / df [ \"img_height\" ] df [ \"scaled_col_left\" ] = np . ceil ( df [ \"col_left\" ] * w_scale ) df [ \"scaled_row_top\" ] = np . ceil ( df [ \"row_top\" ] * h_scale ) df [ \"scaled_width\" ] = np . ceil ( df [ \"width\" ] * w_scale ) df [ \"scaled_height\" ] = np . ceil ( df [ \"height\" ] * h_scale ) return df","title":"boxes"},{"location":"reference/core/boxes/#pyodi.core.boxes.add_centroids","text":"Computes bbox centroids. Parameters: Name Type Description Default df pd . DataFrame pd.DataFrame with COCO annotations. required prefix str Prefix to apply to column names, use for scaled data. Defaults to None. None input_bbox_format str Input bounding box format. Can be \"coco\" or \"corners\". \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Defaults to \"coco\". 'coco' Returns: Type Description pd . DataFrame pd.DataFrame with new columns [prefix_]row_centroid/col_centroid Source code in pyodi/core/boxes.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def add_centroids ( df : pd . DataFrame , prefix : str = None , input_bbox_format : str = \"coco\" ) -> pd . DataFrame : \"\"\"Computes bbox centroids. Args: df: pd.DataFrame with COCO annotations. prefix: Prefix to apply to column names, use for scaled data. Defaults to None. input_bbox_format: Input bounding box format. Can be \"coco\" or \"corners\". \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Defaults to \"coco\". Returns: pd.DataFrame with new columns [prefix_]row_centroid/col_centroid \"\"\" columns = [ \"col_centroid\" , \"row_centroid\" ] bboxes = get_bbox_array ( df , prefix = prefix , input_bbox_format = input_bbox_format ) if prefix : columns = [ f \" { prefix } _ { col } \" for col in columns ] df [ columns [ 0 ]] = bboxes [:, 0 ] + bboxes [:, 2 ] // 2 df [ columns [ 1 ]] = bboxes [:, 1 ] + bboxes [:, 3 ] // 2 return df","title":"add_centroids()"},{"location":"reference/core/boxes/#pyodi.core.boxes.check_bbox_formats","text":"Check if bounding boxes are in a valid format. Source code in pyodi/core/boxes.py 8 9 10 11 12 13 14 def check_bbox_formats ( * args : Any ) -> None : \"\"\"Check if bounding boxes are in a valid format.\"\"\" for arg in args : if not ( arg in [ \"coco\" , \"corners\" ]): raise ValueError ( f \"Invalid format { arg } , only coco and corners format are allowed\" )","title":"check_bbox_formats()"},{"location":"reference/core/boxes/#pyodi.core.boxes.coco_to_corners","text":"Transforms bboxes array from coco format to corners. Parameters: Name Type Description Default bboxes np . ndarray Array with dimension N x 4 with bbox coordinates in corner format required Returns: Type Description np . ndarray Array with dimension N x 4 with bbox coordinates in coco format np . ndarray [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Source code in pyodi/core/boxes.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def coco_to_corners ( bboxes : np . ndarray ) -> np . ndarray : \"\"\"Transforms bboxes array from coco format to corners. Args: bboxes: Array with dimension N x 4 with bbox coordinates in corner format [col_left, row_top, width, height]. Returns: Array with dimension N x 4 with bbox coordinates in coco format [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] \"\"\" bboxes = bboxes . copy () bboxes [ ... , 2 :] = bboxes [ ... , : 2 ] + bboxes [ ... , 2 :] if ( bboxes < 0 ) . any (): logger . warning ( \"Clipping bboxes to min corner 0, found negative value\" ) bboxes = np . clip ( bboxes , 0 , None ) return bboxes","title":"coco_to_corners()"},{"location":"reference/core/boxes/#pyodi.core.boxes.corners_to_coco","text":"Transforms bboxes array from corners format to coco. Parameters: Name Type Description Default bboxes np . ndarray Array with dimension N x 4 with bbox coordinates in corner format required Returns: Type Description np . ndarray Array with dimension N x 4 with bbox coordinates in coco format np . ndarray [col_left, row_top, width, height]. Source code in pyodi/core/boxes.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def corners_to_coco ( bboxes : np . ndarray ) -> np . ndarray : \"\"\"Transforms bboxes array from corners format to coco. Args: bboxes: Array with dimension N x 4 with bbox coordinates in corner format [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Returns: Array with dimension N x 4 with bbox coordinates in coco format [col_left, row_top, width, height]. \"\"\" bboxes = bboxes . copy () bboxes [ ... , 2 :] = bboxes [ ... , 2 :] - bboxes [ ... , : 2 ] return bboxes","title":"corners_to_coco()"},{"location":"reference/core/boxes/#pyodi.core.boxes.denormalize","text":"Transforms bboxes array from (0, 1) range to pixels. Bboxes can be in both formats \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Parameters: Name Type Description Default bboxes np . ndarray Bounding boxes. required image_width int Image width in pixels. required image_height int Image height in pixels. required Returns: Type Description np . ndarray Bounding boxes with coordinates in pixels. Source code in pyodi/core/boxes.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def denormalize ( bboxes : np . ndarray , image_width : int , image_height : int ) -> np . ndarray : \"\"\"Transforms bboxes array from (0, 1) range to pixels. Bboxes can be in both formats: \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Args: bboxes: Bounding boxes. image_width: Image width in pixels. image_height: Image height in pixels. Returns: Bounding boxes with coordinates in pixels. \"\"\" norms = np . array ([ image_width , image_height , image_width , image_height ]) bboxes = bboxes * norms return bboxes","title":"denormalize()"},{"location":"reference/core/boxes/#pyodi.core.boxes.filter_zero_area_bboxes","text":"Filters those bboxes with height or width equal to zero. Parameters: Name Type Description Default df pd . DataFrame pd.DataFrame with COCO annotations. required Returns: Type Description pd . DataFrame Filtered pd.DataFrame with COCO annotations. Source code in pyodi/core/boxes.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def filter_zero_area_bboxes ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Filters those bboxes with height or width equal to zero. Args: df: pd.DataFrame with COCO annotations. Returns: Filtered pd.DataFrame with COCO annotations. \"\"\" cols = [ \"width\" , \"height\" ] all_bboxes = len ( df ) df = df [( df [ cols ] > 0 ) . all ( axis = 1 )] . reset_index () filtered_bboxes = len ( df ) n_filtered = all_bboxes - filtered_bboxes if n_filtered : logger . warning ( f \"A total of { n_filtered } bboxes have been filtered from your data \" \"for having area equal to zero.\" ) return df","title":"filter_zero_area_bboxes()"},{"location":"reference/core/boxes/#pyodi.core.boxes.get_bbox_array","text":"Returns array with bbox coordinates. Parameters: Name Type Description Default df pd . DataFrame pd.DataFrame with COCO annotations. required prefix Optional [ str ] Prefix to apply to column names, use for scaled data. Defaults to None. None input_bbox_format str Input bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". 'coco' output_bbox_format str Output bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". 'coco' Returns: Type Description np . ndarray Array with dimension N x 4 with bbox coordinates. Examples: coco : >>>[col_left, row_top, width, height] corners : >>>[col_left, row_top, col_right, row_bottom] Source code in pyodi/core/boxes.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def get_bbox_array ( df : pd . DataFrame , prefix : Optional [ str ] = None , input_bbox_format : str = \"coco\" , output_bbox_format : str = \"coco\" , ) -> np . ndarray : \"\"\"Returns array with bbox coordinates. Args: df: pd.DataFrame with COCO annotations. prefix: Prefix to apply to column names, use for scaled data. Defaults to None. input_bbox_format: Input bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". output_bbox_format: Output bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". Returns: Array with dimension N x 4 with bbox coordinates. Examples: `coco`: >>>[col_left, row_top, width, height] `corners`: >>>[col_left, row_top, col_right, row_bottom] \"\"\" check_bbox_formats ( input_bbox_format , output_bbox_format ) columns = get_bbox_column_names ( input_bbox_format , prefix = prefix ) bboxes = df [ columns ] . to_numpy () if input_bbox_format != output_bbox_format : convert = globals ()[ f \" { input_bbox_format } _to_ { output_bbox_format } \" ] bboxes = convert ( bboxes ) return bboxes","title":"get_bbox_array()"},{"location":"reference/core/boxes/#pyodi.core.boxes.get_bbox_column_names","text":"Returns predefined column names for each format. When bbox_format is 'coco' column names are [\"col_left\", \"row_top\", \"width\", \"height\"], when 'corners' [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"]. Parameters: Name Type Description Default bbox_format str Bounding box format. Can be \"coco\" or \"corners\". required prefix Optional [ str ] Prefix to apply to column names, use for scaled data. Defaults to None. None Returns: Type Description List [ str ] Column names for specified bbox format Source code in pyodi/core/boxes.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def get_bbox_column_names ( bbox_format : str , prefix : Optional [ str ] = None ) -> List [ str ]: \"\"\"Returns predefined column names for each format. When bbox_format is 'coco' column names are [\"col_left\", \"row_top\", \"width\", \"height\"], when 'corners' [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"]. Args: bbox_format: Bounding box format. Can be \"coco\" or \"corners\". prefix: Prefix to apply to column names, use for scaled data. Defaults to None. Returns: Column names for specified bbox format \"\"\" if bbox_format == \"coco\" : columns = [ \"col_left\" , \"row_top\" , \"width\" , \"height\" ] elif bbox_format == \"corners\" : columns = [ \"col_left\" , \"row_top\" , \"col_right\" , \"row_bottom\" ] else : raise ValueError ( f \"Invalid bbox format, { bbox_format } does not exist\" ) if prefix : columns = [ f \" { prefix } _ { col } \" for col in columns ] return columns","title":"get_bbox_column_names()"},{"location":"reference/core/boxes/#pyodi.core.boxes.get_df_from_bboxes","text":"Creates pd.DataFrame of annotations in Coco format from array of bboxes. Parameters: Name Type Description Default bboxes np . ndarray Array of bboxes of shape [n, 4]. required input_bbox_format str Input bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". 'coco' output_bbox_format str Output bounding box format. Can be \"coco\" or \"corners\". Defaults to \"corners\". 'corners' Returns: Type Description pd . DataFrame pd.DataFrame with Coco annotations. Source code in pyodi/core/boxes.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def get_df_from_bboxes ( bboxes : np . ndarray , input_bbox_format : str = \"coco\" , output_bbox_format : str = \"corners\" , ) -> pd . DataFrame : \"\"\"Creates pd.DataFrame of annotations in Coco format from array of bboxes. Args: bboxes: Array of bboxes of shape [n, 4]. input_bbox_format: Input bounding box format. Can be \"coco\" or \"corners\". Defaults to \"coco\". output_bbox_format: Output bounding box format. Can be \"coco\" or \"corners\". Defaults to \"corners\". Returns: pd.DataFrame with Coco annotations. \"\"\" check_bbox_formats ( input_bbox_format , output_bbox_format ) if input_bbox_format != output_bbox_format : convert = globals ()[ f \" { input_bbox_format } _to_ { output_bbox_format } \" ] bboxes = convert ( bboxes ) return pd . DataFrame ( bboxes , columns = get_bbox_column_names ( output_bbox_format ))","title":"get_df_from_bboxes()"},{"location":"reference/core/boxes/#pyodi.core.boxes.get_scale_and_ratio","text":"Returns df with area and ratio per bbox measurements. Parameters: Name Type Description Default df pd . DataFrame pd.DataFrame with COCO annotations. required prefix str Prefix to apply to column names, use for scaled data. None Returns: Type Description pd . DataFrame pd.DataFrame with new columns [prefix_]area/ratio Source code in pyodi/core/boxes.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_scale_and_ratio ( df : pd . DataFrame , prefix : str = None ) -> pd . DataFrame : \"\"\"Returns df with area and ratio per bbox measurements. Args: df: pd.DataFrame with COCO annotations. prefix: Prefix to apply to column names, use for scaled data. Returns: pd.DataFrame with new columns [prefix_]area/ratio \"\"\" columns = [ \"width\" , \"height\" , \"scale\" , \"ratio\" ] if prefix : columns = [ f \" { prefix } _ { col } \" for col in columns ] df [ columns [ 2 ]] = np . sqrt ( df [ columns [ 0 ]] * df [ columns [ 1 ]]) df [ columns [ 3 ]] = df [ columns [ 1 ]] / df [ columns [ 0 ]] return df","title":"get_scale_and_ratio()"},{"location":"reference/core/boxes/#pyodi.core.boxes.normalize","text":"Transforms bboxes array from pixels to (0, 1) range. Bboxes can be in both formats \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Parameters: Name Type Description Default bboxes np . ndarray Bounding boxes. required image_width int Image width in pixels. required image_height int Image height in pixels. required Returns: Type Description np . ndarray Bounding boxes with coordinates in (0, 1) range. Source code in pyodi/core/boxes.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def normalize ( bboxes : np . ndarray , image_width : int , image_height : int ) -> np . ndarray : \"\"\"Transforms bboxes array from pixels to (0, 1) range. Bboxes can be in both formats: \"coco\" [\"col_left\", \"row_top\", \"width\", \"height\"] \"corners\" [\"col_left\", \"row_top\", \"col_right\", \"row_bottom\"] Args: bboxes: Bounding boxes. image_width: Image width in pixels. image_height: Image height in pixels. Returns: Bounding boxes with coordinates in (0, 1) range. \"\"\" norms = np . array ([ image_width , image_height , image_width , image_height ]) bboxes = bboxes * 1 / norms return bboxes","title":"normalize()"},{"location":"reference/core/boxes/#pyodi.core.boxes.scale_bbox_dimensions","text":"Resizes bboxes dimensions to model input size. Parameters: Name Type Description Default df pd . DataFrame pd.DataFrame with COCO annotations. required input_size Tuple [ int , int ] Model input size. Defaults to (1280, 720). (1280, 720) keep_ratio bool Whether to keep the aspect ratio or not. Defaults to False. False Returns: Type Description pd . DataFrame pd.DataFrame with COCO annotations and scaled image sizes. Source code in pyodi/core/boxes.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def scale_bbox_dimensions ( df : pd . DataFrame , input_size : Tuple [ int , int ] = ( 1280 , 720 ), keep_ratio : bool = False , ) -> pd . DataFrame : \"\"\"Resizes bboxes dimensions to model input size. Args: df: pd.DataFrame with COCO annotations. input_size: Model input size. Defaults to (1280, 720). keep_ratio: Whether to keep the aspect ratio or not. Defaults to False. Returns: pd.DataFrame with COCO annotations and scaled image sizes. \"\"\" if keep_ratio : scale_factor = pd . concat ( [ max ( input_size ) / df [[ \"img_height\" , \"img_width\" ]] . max ( 1 ), min ( input_size ) / df [[ \"img_height\" , \"img_width\" ]] . min ( 1 ), ], axis = 1 , ) . min ( 1 ) w_scale = np . round ( df [ \"img_width\" ] * scale_factor ) / df [ \"img_width\" ] h_scale = np . round ( df [ \"img_height\" ] * scale_factor ) / df [ \"img_height\" ] else : w_scale = input_size [ 0 ] / df [ \"img_width\" ] h_scale = input_size [ 1 ] / df [ \"img_height\" ] df [ \"scaled_col_left\" ] = np . ceil ( df [ \"col_left\" ] * w_scale ) df [ \"scaled_row_top\" ] = np . ceil ( df [ \"row_top\" ] * h_scale ) df [ \"scaled_width\" ] = np . ceil ( df [ \"width\" ] * w_scale ) df [ \"scaled_height\" ] = np . ceil ( df [ \"height\" ] * h_scale ) return df","title":"scale_bbox_dimensions()"},{"location":"reference/core/clustering/","text":"find_pyramid_level ( bboxes , base_sizes ) Matches bboxes with pyramid levels given their stride. Parameters: Name Type Description Default bboxes ndarray Bbox array with dimension [n, 2] in width-height order. required base_sizes List [ int ] The basic sizes of anchors in multiple levels. required Returns: Type Description ndarray Best match per bbox corresponding with index of stride. Source code in pyodi/core/clustering.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def find_pyramid_level ( bboxes : ndarray , base_sizes : List [ int ]) -> ndarray : \"\"\"Matches bboxes with pyramid levels given their stride. Args: bboxes: Bbox array with dimension [n, 2] in width-height order. base_sizes: The basic sizes of anchors in multiple levels. Returns: Best match per bbox corresponding with index of stride. \"\"\" base_sizes = sorted ( base_sizes ) levels = np . tile ( base_sizes , ( 2 , 1 )) . T ious = origin_iou ( bboxes , levels ) return np . argmax ( ious , axis = 1 ) get_max_overlap ( boxes , anchors ) Computes max intersection-over-union between box and anchors. Parameters: Name Type Description Default boxes ndarray Array of bboxes with shape [n, 4]. In corner format. required anchors ndarray Array of bboxes with shape [m, 4]. In corner format. required Returns: Type Description ndarray Max iou between box and anchors with shape [n, 1]. Source code in pyodi/core/clustering.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @njit ( float32 [:]( float32 [:, :], float32 [:, :]), parallel = True ) def get_max_overlap ( boxes : ndarray , anchors : ndarray ) -> ndarray : \"\"\"Computes max intersection-over-union between box and anchors. Args: boxes: Array of bboxes with shape [n, 4]. In corner format. anchors: Array of bboxes with shape [m, 4]. In corner format. Returns: Max iou between box and anchors with shape [n, 1]. \"\"\" rows = boxes . shape [ 0 ] cols = anchors . shape [ 0 ] overlap = np . zeros ( rows , dtype = np . float32 ) box_areas = ( boxes [:, 2 ] - boxes [:, 0 ]) * ( boxes [:, 3 ] - boxes [:, 1 ]) anchors_areas = ( anchors [:, 2 ] - anchors [:, 0 ]) * ( anchors [:, 3 ] - anchors [:, 1 ]) for row in prange ( rows ): for col in range ( cols ): ymin = max ( boxes [ row , 0 ], anchors [ col , 0 ]) xmin = max ( boxes [ row , 1 ], anchors [ col , 1 ]) ymax = min ( boxes [ row , 2 ], anchors [ col , 2 ]) xmax = min ( boxes [ row , 3 ], anchors [ col , 3 ]) intersection = max ( 0 , ymax - ymin ) * max ( 0 , xmax - xmin ) union = box_areas [ row ] + anchors_areas [ col ] - intersection overlap [ row ] = max ( intersection / union , overlap [ row ]) return overlap kmeans_euclidean ( values , n_clusters = 3 , silhouette_metric = False ) Computes k-means clustering with euclidean distance. Parameters: Name Type Description Default values ndarray Data for the k-means algorithm. required n_clusters int Number of clusters. 3 silhouette_metric bool Whether to compute the silhouette metric or not. Defaults to False. False Returns: Type Description Dict [ str , Union [ ndarray , float64 ]] Clustering results. Source code in pyodi/core/clustering.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def kmeans_euclidean ( values : ndarray , n_clusters : int = 3 , silhouette_metric : bool = False , ) -> Dict [ str , Union [ ndarray , float64 ]]: \"\"\"Computes k-means clustering with euclidean distance. Args: values: Data for the k-means algorithm. n_clusters: Number of clusters. silhouette_metric: Whether to compute the silhouette metric or not. Defaults to False. Returns: Clustering results. \"\"\" if len ( values . shape ) == 1 : values = values [:, None ] kmeans = KMeans ( n_clusters = n_clusters ) kmeans . fit ( values ) result = dict ( centroids = kmeans . cluster_centers_ , labels = kmeans . labels_ ) if silhouette_metric : result [ \"silhouette\" ] = silhouette_score ( values , labels = kmeans . labels_ ) return result origin_iou ( bboxes , clusters ) Calculates the Intersection over Union (IoU) between a box and k clusters. Note: COCO format shifted to origin. Parameters: Name Type Description Default bboxes ndarray Bboxes array with dimension [n, 2] in width-height order. required clusters ndarray Bbox array with dimension [n, 2] in width-height order. required Returns: Type Description ndarray BBox array with centroids with dimensions [k, 2]. Source code in pyodi/core/clustering.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def origin_iou ( bboxes : ndarray , clusters : ndarray ) -> ndarray : \"\"\"Calculates the Intersection over Union (IoU) between a box and k clusters. Note: COCO format shifted to origin. Args: bboxes: Bboxes array with dimension [n, 2] in width-height order. clusters: Bbox array with dimension [n, 2] in width-height order. Returns: BBox array with centroids with dimensions [k, 2]. \"\"\" col = np . minimum ( bboxes [:, None , 0 ], clusters [:, 0 ]) row = np . minimum ( bboxes [:, None , 1 ], clusters [:, 1 ]) if np . count_nonzero ( col == 0 ) > 0 or np . count_nonzero ( row == 0 ) > 0 : raise ValueError ( \"Box has no area\" ) intersection = col * row box_area = bboxes [:, 0 ] * bboxes [:, 1 ] cluster_area = clusters [:, 0 ] * clusters [:, 1 ] iou_ = intersection / ( box_area [:, None ] + cluster_area - intersection ) return iou_","title":"clustering"},{"location":"reference/core/clustering/#pyodi.core.clustering.find_pyramid_level","text":"Matches bboxes with pyramid levels given their stride. Parameters: Name Type Description Default bboxes ndarray Bbox array with dimension [n, 2] in width-height order. required base_sizes List [ int ] The basic sizes of anchors in multiple levels. required Returns: Type Description ndarray Best match per bbox corresponding with index of stride. Source code in pyodi/core/clustering.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def find_pyramid_level ( bboxes : ndarray , base_sizes : List [ int ]) -> ndarray : \"\"\"Matches bboxes with pyramid levels given their stride. Args: bboxes: Bbox array with dimension [n, 2] in width-height order. base_sizes: The basic sizes of anchors in multiple levels. Returns: Best match per bbox corresponding with index of stride. \"\"\" base_sizes = sorted ( base_sizes ) levels = np . tile ( base_sizes , ( 2 , 1 )) . T ious = origin_iou ( bboxes , levels ) return np . argmax ( ious , axis = 1 )","title":"find_pyramid_level()"},{"location":"reference/core/clustering/#pyodi.core.clustering.get_max_overlap","text":"Computes max intersection-over-union between box and anchors. Parameters: Name Type Description Default boxes ndarray Array of bboxes with shape [n, 4]. In corner format. required anchors ndarray Array of bboxes with shape [m, 4]. In corner format. required Returns: Type Description ndarray Max iou between box and anchors with shape [n, 1]. Source code in pyodi/core/clustering.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @njit ( float32 [:]( float32 [:, :], float32 [:, :]), parallel = True ) def get_max_overlap ( boxes : ndarray , anchors : ndarray ) -> ndarray : \"\"\"Computes max intersection-over-union between box and anchors. Args: boxes: Array of bboxes with shape [n, 4]. In corner format. anchors: Array of bboxes with shape [m, 4]. In corner format. Returns: Max iou between box and anchors with shape [n, 1]. \"\"\" rows = boxes . shape [ 0 ] cols = anchors . shape [ 0 ] overlap = np . zeros ( rows , dtype = np . float32 ) box_areas = ( boxes [:, 2 ] - boxes [:, 0 ]) * ( boxes [:, 3 ] - boxes [:, 1 ]) anchors_areas = ( anchors [:, 2 ] - anchors [:, 0 ]) * ( anchors [:, 3 ] - anchors [:, 1 ]) for row in prange ( rows ): for col in range ( cols ): ymin = max ( boxes [ row , 0 ], anchors [ col , 0 ]) xmin = max ( boxes [ row , 1 ], anchors [ col , 1 ]) ymax = min ( boxes [ row , 2 ], anchors [ col , 2 ]) xmax = min ( boxes [ row , 3 ], anchors [ col , 3 ]) intersection = max ( 0 , ymax - ymin ) * max ( 0 , xmax - xmin ) union = box_areas [ row ] + anchors_areas [ col ] - intersection overlap [ row ] = max ( intersection / union , overlap [ row ]) return overlap","title":"get_max_overlap()"},{"location":"reference/core/clustering/#pyodi.core.clustering.kmeans_euclidean","text":"Computes k-means clustering with euclidean distance. Parameters: Name Type Description Default values ndarray Data for the k-means algorithm. required n_clusters int Number of clusters. 3 silhouette_metric bool Whether to compute the silhouette metric or not. Defaults to False. False Returns: Type Description Dict [ str , Union [ ndarray , float64 ]] Clustering results. Source code in pyodi/core/clustering.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def kmeans_euclidean ( values : ndarray , n_clusters : int = 3 , silhouette_metric : bool = False , ) -> Dict [ str , Union [ ndarray , float64 ]]: \"\"\"Computes k-means clustering with euclidean distance. Args: values: Data for the k-means algorithm. n_clusters: Number of clusters. silhouette_metric: Whether to compute the silhouette metric or not. Defaults to False. Returns: Clustering results. \"\"\" if len ( values . shape ) == 1 : values = values [:, None ] kmeans = KMeans ( n_clusters = n_clusters ) kmeans . fit ( values ) result = dict ( centroids = kmeans . cluster_centers_ , labels = kmeans . labels_ ) if silhouette_metric : result [ \"silhouette\" ] = silhouette_score ( values , labels = kmeans . labels_ ) return result","title":"kmeans_euclidean()"},{"location":"reference/core/clustering/#pyodi.core.clustering.origin_iou","text":"Calculates the Intersection over Union (IoU) between a box and k clusters. Note: COCO format shifted to origin. Parameters: Name Type Description Default bboxes ndarray Bboxes array with dimension [n, 2] in width-height order. required clusters ndarray Bbox array with dimension [n, 2] in width-height order. required Returns: Type Description ndarray BBox array with centroids with dimensions [k, 2]. Source code in pyodi/core/clustering.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def origin_iou ( bboxes : ndarray , clusters : ndarray ) -> ndarray : \"\"\"Calculates the Intersection over Union (IoU) between a box and k clusters. Note: COCO format shifted to origin. Args: bboxes: Bboxes array with dimension [n, 2] in width-height order. clusters: Bbox array with dimension [n, 2] in width-height order. Returns: BBox array with centroids with dimensions [k, 2]. \"\"\" col = np . minimum ( bboxes [:, None , 0 ], clusters [:, 0 ]) row = np . minimum ( bboxes [:, None , 1 ], clusters [:, 1 ]) if np . count_nonzero ( col == 0 ) > 0 or np . count_nonzero ( row == 0 ) > 0 : raise ValueError ( \"Box has no area\" ) intersection = col * row box_area = bboxes [:, 0 ] * bboxes [:, 1 ] cluster_area = clusters [:, 0 ] * clusters [:, 1 ] iou_ = intersection / ( box_area [:, None ] + cluster_area - intersection ) return iou_","title":"origin_iou()"},{"location":"reference/core/crops/","text":"annotation_inside_crop ( annotation , crop_corners ) Check whether annotation coordinates lie inside crop coordinates. Parameters: Name Type Description Default annotation Dict Single annotation entry in COCO format. required crop_corners List [ int ] Generated from get_crop_corners . required Returns: Name Type Description bool bool True if any annotation coordinate lies inside crop. Source code in pyodi/core/crops.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def annotation_inside_crop ( annotation : Dict , crop_corners : List [ int ]) -> bool : \"\"\"Check whether annotation coordinates lie inside crop coordinates. Args: annotation (Dict): Single annotation entry in COCO format. crop_corners (List[int]): Generated from `get_crop_corners`. Returns: bool: True if any annotation coordinate lies inside crop. \"\"\" left , top , width , height = annotation [ \"bbox\" ] right = left + width bottom = top + height if left > crop_corners [ 2 ]: return False if top > crop_corners [ 3 ]: return False if right < crop_corners [ 0 ]: return False if bottom < crop_corners [ 1 ]: return False return True filter_annotation_by_area ( annotation , new_annotation , min_area_threshold ) Check whether cropped annotation area is smaller than minimum area size. Parameters: Name Type Description Default annotation Dict Single annotation entry in COCO format. required new_annotation Dict Single annotation entry in COCO format. required min_area_threshold float Minimum area threshold ratio. required Returns: Type Description bool True if annotation area is smaller than the minimum area size. Source code in pyodi/core/crops.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def filter_annotation_by_area ( annotation : Dict , new_annotation : Dict , min_area_threshold : float ) -> bool : \"\"\"Check whether cropped annotation area is smaller than minimum area size. Args: annotation: Single annotation entry in COCO format. new_annotation: Single annotation entry in COCO format. min_area_threshold: Minimum area threshold ratio. Returns: True if annotation area is smaller than the minimum area size. \"\"\" area = annotation [ \"area\" ] new_area = new_annotation [ \"area\" ] min_area = area * min_area_threshold if new_area > min_area : return False return True get_annotation_in_crop ( annotation , crop_corners ) Translate annotation coordinates to crop coordinates. Parameters: Name Type Description Default annotation Dict Single annotation entry in COCO format. required crop_corners List [ int ] Generated from get_crop_corners . required Returns: Name Type Description Dict Dict Annotation entry with coordinates translated to crop coordinates. Source code in pyodi/core/crops.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def get_annotation_in_crop ( annotation : Dict , crop_corners : List [ int ]) -> Dict : \"\"\"Translate annotation coordinates to crop coordinates. Args: annotation (Dict): Single annotation entry in COCO format. crop_corners (List[int]): Generated from `get_crop_corners`. Returns: Dict: Annotation entry with coordinates translated to crop coordinates. \"\"\" left , top , width , height = annotation [ \"bbox\" ] right = left + width bottom = top + height new_left = max ( left - crop_corners [ 0 ], 0 ) new_top = max ( top - crop_corners [ 1 ], 0 ) new_right = min ( right - crop_corners [ 0 ], crop_corners [ 2 ] - crop_corners [ 0 ]) new_bottom = min ( bottom - crop_corners [ 1 ], crop_corners [ 3 ] - crop_corners [ 1 ]) new_width = new_right - new_left new_height = new_bottom - new_top new_bbox = [ new_left , new_top , new_width , new_height ] new_area = new_width * new_height new_segmentation = [ new_left , new_top , new_left , new_top + new_height , new_left + new_width , new_top + new_height , new_left + new_width , new_top , ] return { \"bbox\" : new_bbox , \"area\" : new_area , \"segmentation\" : new_segmentation , \"iscrowd\" : annotation [ \"iscrowd\" ], \"score\" : annotation . get ( \"score\" , 1 ), \"category_id\" : annotation [ \"category_id\" ], } get_crops_corners ( image_pil , crop_height , crop_width , row_overlap = 0 , col_overlap = 0 ) Divides image_pil in crops. The crops corners will be generated using the crop_height , crop_width , row_overlap and col_overlap arguments. Parameters: Name Type Description Default image_pil PIL . Image Instance of PIL.Image required row_overlap int Default 0. 0 col_overlap int Default 0. 0 Returns: Type Description List [ List [ int ]] List[List[int]]: List of 4 corner coordinates for each crop of the N crops. [ [crop_0_left, crop_0_top, crop_0_right, crop_0_bottom], ... [crop_N_left, crop_N_top, crop_N_right, crop_N_bottom] ] Source code in pyodi/core/crops.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_crops_corners ( image_pil : Image , crop_height : int , crop_width : int , row_overlap : int = 0 , col_overlap : int = 0 , ) -> List [ List [ int ]]: \"\"\"Divides `image_pil` in crops. The crops corners will be generated using the `crop_height`, `crop_width`, `row_overlap` and `col_overlap` arguments. Args: image_pil (PIL.Image): Instance of PIL.Image crop_height (int) crop_width (int) row_overlap (int, optional): Default 0. col_overlap (int, optional): Default 0. Returns: List[List[int]]: List of 4 corner coordinates for each crop of the N crops. [ [crop_0_left, crop_0_top, crop_0_right, crop_0_bottom], ... [crop_N_left, crop_N_top, crop_N_right, crop_N_bottom] ] \"\"\" crops_corners = [] row_max = row_min = 0 width , height = image_pil . size while row_max - row_overlap < height : col_min = col_max = 0 row_max = row_min + crop_height while col_max - col_overlap < width : col_max = col_min + crop_width if row_max > height or col_max > width : rmax = min ( height , row_max ) cmax = min ( width , col_max ) crops_corners . append ( [ cmax - crop_width , rmax - crop_height , cmax , rmax ] ) else : crops_corners . append ([ col_min , row_min , col_max , row_max ]) col_min = col_max - col_overlap row_min = row_max - row_overlap return crops_corners","title":"crops"},{"location":"reference/core/crops/#pyodi.core.crops.annotation_inside_crop","text":"Check whether annotation coordinates lie inside crop coordinates. Parameters: Name Type Description Default annotation Dict Single annotation entry in COCO format. required crop_corners List [ int ] Generated from get_crop_corners . required Returns: Name Type Description bool bool True if any annotation coordinate lies inside crop. Source code in pyodi/core/crops.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def annotation_inside_crop ( annotation : Dict , crop_corners : List [ int ]) -> bool : \"\"\"Check whether annotation coordinates lie inside crop coordinates. Args: annotation (Dict): Single annotation entry in COCO format. crop_corners (List[int]): Generated from `get_crop_corners`. Returns: bool: True if any annotation coordinate lies inside crop. \"\"\" left , top , width , height = annotation [ \"bbox\" ] right = left + width bottom = top + height if left > crop_corners [ 2 ]: return False if top > crop_corners [ 3 ]: return False if right < crop_corners [ 0 ]: return False if bottom < crop_corners [ 1 ]: return False return True","title":"annotation_inside_crop()"},{"location":"reference/core/crops/#pyodi.core.crops.filter_annotation_by_area","text":"Check whether cropped annotation area is smaller than minimum area size. Parameters: Name Type Description Default annotation Dict Single annotation entry in COCO format. required new_annotation Dict Single annotation entry in COCO format. required min_area_threshold float Minimum area threshold ratio. required Returns: Type Description bool True if annotation area is smaller than the minimum area size. Source code in pyodi/core/crops.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def filter_annotation_by_area ( annotation : Dict , new_annotation : Dict , min_area_threshold : float ) -> bool : \"\"\"Check whether cropped annotation area is smaller than minimum area size. Args: annotation: Single annotation entry in COCO format. new_annotation: Single annotation entry in COCO format. min_area_threshold: Minimum area threshold ratio. Returns: True if annotation area is smaller than the minimum area size. \"\"\" area = annotation [ \"area\" ] new_area = new_annotation [ \"area\" ] min_area = area * min_area_threshold if new_area > min_area : return False return True","title":"filter_annotation_by_area()"},{"location":"reference/core/crops/#pyodi.core.crops.get_annotation_in_crop","text":"Translate annotation coordinates to crop coordinates. Parameters: Name Type Description Default annotation Dict Single annotation entry in COCO format. required crop_corners List [ int ] Generated from get_crop_corners . required Returns: Name Type Description Dict Dict Annotation entry with coordinates translated to crop coordinates. Source code in pyodi/core/crops.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def get_annotation_in_crop ( annotation : Dict , crop_corners : List [ int ]) -> Dict : \"\"\"Translate annotation coordinates to crop coordinates. Args: annotation (Dict): Single annotation entry in COCO format. crop_corners (List[int]): Generated from `get_crop_corners`. Returns: Dict: Annotation entry with coordinates translated to crop coordinates. \"\"\" left , top , width , height = annotation [ \"bbox\" ] right = left + width bottom = top + height new_left = max ( left - crop_corners [ 0 ], 0 ) new_top = max ( top - crop_corners [ 1 ], 0 ) new_right = min ( right - crop_corners [ 0 ], crop_corners [ 2 ] - crop_corners [ 0 ]) new_bottom = min ( bottom - crop_corners [ 1 ], crop_corners [ 3 ] - crop_corners [ 1 ]) new_width = new_right - new_left new_height = new_bottom - new_top new_bbox = [ new_left , new_top , new_width , new_height ] new_area = new_width * new_height new_segmentation = [ new_left , new_top , new_left , new_top + new_height , new_left + new_width , new_top + new_height , new_left + new_width , new_top , ] return { \"bbox\" : new_bbox , \"area\" : new_area , \"segmentation\" : new_segmentation , \"iscrowd\" : annotation [ \"iscrowd\" ], \"score\" : annotation . get ( \"score\" , 1 ), \"category_id\" : annotation [ \"category_id\" ], }","title":"get_annotation_in_crop()"},{"location":"reference/core/crops/#pyodi.core.crops.get_crops_corners","text":"Divides image_pil in crops. The crops corners will be generated using the crop_height , crop_width , row_overlap and col_overlap arguments. Parameters: Name Type Description Default image_pil PIL . Image Instance of PIL.Image required row_overlap int Default 0. 0 col_overlap int Default 0. 0 Returns: Type Description List [ List [ int ]] List[List[int]]: List of 4 corner coordinates for each crop of the N crops. [ [crop_0_left, crop_0_top, crop_0_right, crop_0_bottom], ... [crop_N_left, crop_N_top, crop_N_right, crop_N_bottom] ] Source code in pyodi/core/crops.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_crops_corners ( image_pil : Image , crop_height : int , crop_width : int , row_overlap : int = 0 , col_overlap : int = 0 , ) -> List [ List [ int ]]: \"\"\"Divides `image_pil` in crops. The crops corners will be generated using the `crop_height`, `crop_width`, `row_overlap` and `col_overlap` arguments. Args: image_pil (PIL.Image): Instance of PIL.Image crop_height (int) crop_width (int) row_overlap (int, optional): Default 0. col_overlap (int, optional): Default 0. Returns: List[List[int]]: List of 4 corner coordinates for each crop of the N crops. [ [crop_0_left, crop_0_top, crop_0_right, crop_0_bottom], ... [crop_N_left, crop_N_top, crop_N_right, crop_N_bottom] ] \"\"\" crops_corners = [] row_max = row_min = 0 width , height = image_pil . size while row_max - row_overlap < height : col_min = col_max = 0 row_max = row_min + crop_height while col_max - col_overlap < width : col_max = col_min + crop_width if row_max > height or col_max > width : rmax = min ( height , row_max ) cmax = min ( width , col_max ) crops_corners . append ( [ cmax - crop_width , rmax - crop_height , cmax , rmax ] ) else : crops_corners . append ([ col_min , row_min , col_max , row_max ]) col_min = col_max - col_overlap row_min = row_max - row_overlap return crops_corners","title":"get_crops_corners()"},{"location":"reference/core/nms/","text":"nms ( dets , scores , iou_thr ) Non Maximum supression algorithm from https://github.com/ZFTurbo/Weighted-Boxes-Fusion/blob/master/ensemble_boxes/ensemble_boxes_nms.py. Parameters: Name Type Description Default dets np . ndarray Array of predictions in corner format. required scores np . ndarray Array of scores for each prediction. required iou_thr float None of the filtered predictions will have an iou above iou_thr to any other. required Returns: Type Description np . ndarray List of filtered predictions in COCO format. Source code in pyodi/core/nms.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @jit ( nopython = True ) def nms ( dets : np . ndarray , scores : np . ndarray , iou_thr : float ) -> np . ndarray : \"\"\"Non Maximum supression algorithm from https://github.com/ZFTurbo/Weighted-Boxes-Fusion/blob/master/ensemble_boxes/ensemble_boxes_nms.py. Args: dets: Array of predictions in corner format. scores: Array of scores for each prediction. iou_thr: None of the filtered predictions will have an iou above `iou_thr` to any other. Returns: List of filtered predictions in COCO format. \"\"\" x1 = dets [:, 0 ] y1 = dets [:, 1 ] x2 = dets [:, 2 ] y2 = dets [:, 3 ] areas = ( x2 - x1 ) * ( y2 - y1 ) order = scores . argsort ()[:: - 1 ] keep = [] while order . size > 0 : i = order [ 0 ] keep . append ( i ) xx1 = np . maximum ( x1 [ i ], x1 [ order [ 1 :]]) yy1 = np . maximum ( y1 [ i ], y1 [ order [ 1 :]]) xx2 = np . minimum ( x2 [ i ], x2 [ order [ 1 :]]) yy2 = np . minimum ( y2 [ i ], y2 [ order [ 1 :]]) w = np . maximum ( 0.0 , xx2 - xx1 ) h = np . maximum ( 0.0 , yy2 - yy1 ) inter = w * h ovr = inter / ( areas [ i ] + areas [ order [ 1 :]] - inter ) inds = np . where ( ovr <= iou_thr )[ 0 ] order = order [ inds + 1 ] return keep nms_predictions ( predictions , score_thr = 0.0 , iou_thr = 0.5 ) Apply Non Maximum supression to all the images in a COCO predictions list. Parameters: Name Type Description Default predictions List [ Dict [ Any , Any ]] List of predictions in COCO format. required score_thr float Predictions below score_thr will be filtered. Defaults to 0.0. 0.0 iou_thr float None of the filtered predictions will have an iou above iou_thr to any other. Defaults to 0.5. 0.5 Returns: Type Description List [ Dict [ Any , Any ]] List of filtered predictions in COCO format. Source code in pyodi/core/nms.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def nms_predictions ( predictions : List [ Dict [ Any , Any ]], score_thr : float = 0.0 , iou_thr : float = 0.5 , ) -> List [ Dict [ Any , Any ]]: \"\"\"Apply Non Maximum supression to all the images in a COCO `predictions` list. Args: predictions: List of predictions in COCO format. score_thr: Predictions below `score_thr` will be filtered. Defaults to 0.0. iou_thr: None of the filtered predictions will have an iou above `iou_thr` to any other. Defaults to 0.5. Returns: List of filtered predictions in COCO format. \"\"\" new_predictions = [] image_id_to_all_boxes : Dict [ str , List [ List [ float ]]] = defaultdict ( list ) image_id_to_shape : Dict [ str , Tuple [ int , int ]] = dict () for prediction in predictions : image_id_to_all_boxes [ prediction [ \"image_id\" ]] . append ( [ * prediction [ \"bbox\" ], prediction [ \"score\" ], prediction [ \"category_id\" ]] ) if prediction [ \"image_id\" ] not in image_id_to_shape : image_id_to_shape [ prediction [ \"image_id\" ]] = prediction [ \"original_image_shape\" ] for image_id , all_boxes in image_id_to_all_boxes . items (): categories = np . array ([ box [ - 1 ] for box in all_boxes ]) scores = np . array ([ box [ - 2 ] for box in all_boxes ]) boxes = np . vstack ([ box [: - 2 ] for box in all_boxes ]) image_width , image_height = image_id_to_shape [ image_id ] boxes = normalize ( coco_to_corners ( boxes ), image_width , image_height ) logger . info ( f \"Before nms: { boxes . shape } \" ) keep = nms ( boxes , scores , iou_thr = iou_thr ) # Filter out predictions boxes , categories , scores = boxes [ keep ], categories [ keep ], scores [ keep ] logger . info ( f \"After nms: { boxes . shape } \" ) logger . info ( f \"Before score threshold: { boxes . shape } \" ) above_thr = scores > score_thr boxes = boxes [ above_thr ] scores = scores [ above_thr ] categories = categories [ above_thr ] logger . info ( f \"After score threshold: { boxes . shape } \" ) boxes = denormalize ( corners_to_coco ( boxes ), image_width , image_height ) for box , score , category in zip ( boxes , scores , categories ): new_predictions . append ( { \"image_id\" : image_id , \"bbox\" : box . tolist (), \"score\" : float ( score ), \"category_id\" : int ( category ), } ) return new_predictions","title":"nms"},{"location":"reference/core/nms/#pyodi.core.nms.nms","text":"Non Maximum supression algorithm from https://github.com/ZFTurbo/Weighted-Boxes-Fusion/blob/master/ensemble_boxes/ensemble_boxes_nms.py. Parameters: Name Type Description Default dets np . ndarray Array of predictions in corner format. required scores np . ndarray Array of scores for each prediction. required iou_thr float None of the filtered predictions will have an iou above iou_thr to any other. required Returns: Type Description np . ndarray List of filtered predictions in COCO format. Source code in pyodi/core/nms.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @jit ( nopython = True ) def nms ( dets : np . ndarray , scores : np . ndarray , iou_thr : float ) -> np . ndarray : \"\"\"Non Maximum supression algorithm from https://github.com/ZFTurbo/Weighted-Boxes-Fusion/blob/master/ensemble_boxes/ensemble_boxes_nms.py. Args: dets: Array of predictions in corner format. scores: Array of scores for each prediction. iou_thr: None of the filtered predictions will have an iou above `iou_thr` to any other. Returns: List of filtered predictions in COCO format. \"\"\" x1 = dets [:, 0 ] y1 = dets [:, 1 ] x2 = dets [:, 2 ] y2 = dets [:, 3 ] areas = ( x2 - x1 ) * ( y2 - y1 ) order = scores . argsort ()[:: - 1 ] keep = [] while order . size > 0 : i = order [ 0 ] keep . append ( i ) xx1 = np . maximum ( x1 [ i ], x1 [ order [ 1 :]]) yy1 = np . maximum ( y1 [ i ], y1 [ order [ 1 :]]) xx2 = np . minimum ( x2 [ i ], x2 [ order [ 1 :]]) yy2 = np . minimum ( y2 [ i ], y2 [ order [ 1 :]]) w = np . maximum ( 0.0 , xx2 - xx1 ) h = np . maximum ( 0.0 , yy2 - yy1 ) inter = w * h ovr = inter / ( areas [ i ] + areas [ order [ 1 :]] - inter ) inds = np . where ( ovr <= iou_thr )[ 0 ] order = order [ inds + 1 ] return keep","title":"nms()"},{"location":"reference/core/nms/#pyodi.core.nms.nms_predictions","text":"Apply Non Maximum supression to all the images in a COCO predictions list. Parameters: Name Type Description Default predictions List [ Dict [ Any , Any ]] List of predictions in COCO format. required score_thr float Predictions below score_thr will be filtered. Defaults to 0.0. 0.0 iou_thr float None of the filtered predictions will have an iou above iou_thr to any other. Defaults to 0.5. 0.5 Returns: Type Description List [ Dict [ Any , Any ]] List of filtered predictions in COCO format. Source code in pyodi/core/nms.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def nms_predictions ( predictions : List [ Dict [ Any , Any ]], score_thr : float = 0.0 , iou_thr : float = 0.5 , ) -> List [ Dict [ Any , Any ]]: \"\"\"Apply Non Maximum supression to all the images in a COCO `predictions` list. Args: predictions: List of predictions in COCO format. score_thr: Predictions below `score_thr` will be filtered. Defaults to 0.0. iou_thr: None of the filtered predictions will have an iou above `iou_thr` to any other. Defaults to 0.5. Returns: List of filtered predictions in COCO format. \"\"\" new_predictions = [] image_id_to_all_boxes : Dict [ str , List [ List [ float ]]] = defaultdict ( list ) image_id_to_shape : Dict [ str , Tuple [ int , int ]] = dict () for prediction in predictions : image_id_to_all_boxes [ prediction [ \"image_id\" ]] . append ( [ * prediction [ \"bbox\" ], prediction [ \"score\" ], prediction [ \"category_id\" ]] ) if prediction [ \"image_id\" ] not in image_id_to_shape : image_id_to_shape [ prediction [ \"image_id\" ]] = prediction [ \"original_image_shape\" ] for image_id , all_boxes in image_id_to_all_boxes . items (): categories = np . array ([ box [ - 1 ] for box in all_boxes ]) scores = np . array ([ box [ - 2 ] for box in all_boxes ]) boxes = np . vstack ([ box [: - 2 ] for box in all_boxes ]) image_width , image_height = image_id_to_shape [ image_id ] boxes = normalize ( coco_to_corners ( boxes ), image_width , image_height ) logger . info ( f \"Before nms: { boxes . shape } \" ) keep = nms ( boxes , scores , iou_thr = iou_thr ) # Filter out predictions boxes , categories , scores = boxes [ keep ], categories [ keep ], scores [ keep ] logger . info ( f \"After nms: { boxes . shape } \" ) logger . info ( f \"Before score threshold: { boxes . shape } \" ) above_thr = scores > score_thr boxes = boxes [ above_thr ] scores = scores [ above_thr ] categories = categories [ above_thr ] logger . info ( f \"After score threshold: { boxes . shape } \" ) boxes = denormalize ( corners_to_coco ( boxes ), image_width , image_height ) for box , score , category in zip ( boxes , scores , categories ): new_predictions . append ( { \"image_id\" : image_id , \"bbox\" : box . tolist (), \"score\" : float ( score ), \"category_id\" : int ( category ), } ) return new_predictions","title":"nms_predictions()"},{"location":"reference/core/utils/","text":"coco_ground_truth_to_df ( ground_truth_file , max_images = 200000 ) Load and transforms COCO ground truth data to pd.DataFrame object. Parameters: Name Type Description Default ground_truth_file str Path of ground truth file. required max_images int Maximum number of images to process. 200000 Returns: Type Description pd . DataFrame pd.DataFrame with df_annotations keys and image sizes. Source code in pyodi/core/utils.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def coco_ground_truth_to_df ( ground_truth_file : str , max_images : int = 200000 ) -> pd . DataFrame : \"\"\"Load and transforms COCO ground truth data to pd.DataFrame object. Args: ground_truth_file: Path of ground truth file. max_images: Maximum number of images to process. Returns: pd.DataFrame with df_annotations keys and image sizes. \"\"\" logger . info ( \"Loading Ground Truth File\" ) with open ( ground_truth_file ) as gt : coco_ground_truth = json . load ( gt ) if len ( coco_ground_truth [ \"images\" ]) > max_images : logger . warning ( f \"Number of images { len ( coco_ground_truth [ 'images' ]) } exceeds maximum: \" f \" { max_images } . \\n All the exceeding images will be ignored.\" ) logger . info ( \"Converting COCO Ground Truth to pd.DataFrame\" ) df_images = pd . DataFrame ( coco_ground_truth [ \"images\" ][: max_images ])[ [ \"id\" , \"file_name\" , \"width\" , \"height\" ] ] df_images = df_images . add_prefix ( \"img_\" ) df_annotations = pd . DataFrame ( coco_ground_truth [ \"annotations\" ]) # Replace label with category name categories = { x [ \"id\" ]: x [ \"name\" ] for x in coco_ground_truth [ \"categories\" ]} df_annotations [ \"category\" ] = df_annotations [ \"category_id\" ] . replace ( categories ) # Add bbox columns bbox_columns = [ \"col_left\" , \"row_top\" , \"width\" , \"height\" ] df_annotations [ bbox_columns ] = pd . DataFrame ( df_annotations . bbox . tolist (), index = df_annotations . index ) # Filter columns by name column_names = [ \"image_id\" , \"area\" , \"id\" , \"category\" ] + bbox_columns if \"iscrowd\" in df_annotations . columns : column_names . append ( \"iscrowd\" ) # Join with images df_annotations = df_annotations [ column_names ] . join ( df_images . set_index ( \"img_id\" ), how = \"inner\" , on = \"image_id\" ) return df_annotations load_coco_ground_truth_from_StringIO ( string_io ) Returns COCO object from StringIO. Parameters: Name Type Description Default string_io TextIO IO stream in text mode. required Returns: Type Description COCO COCO object. Source code in pyodi/core/utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def load_coco_ground_truth_from_StringIO ( string_io : TextIO ) -> COCO : \"\"\"Returns COCO object from StringIO. Args: string_io: IO stream in text mode. Returns: COCO object. \"\"\" coco_ground_truth = COCO () coco_ground_truth . dataset = json . load ( string_io ) coco_ground_truth . createIndex () return coco_ground_truth","title":"utils"},{"location":"reference/core/utils/#pyodi.core.utils.coco_ground_truth_to_df","text":"Load and transforms COCO ground truth data to pd.DataFrame object. Parameters: Name Type Description Default ground_truth_file str Path of ground truth file. required max_images int Maximum number of images to process. 200000 Returns: Type Description pd . DataFrame pd.DataFrame with df_annotations keys and image sizes. Source code in pyodi/core/utils.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def coco_ground_truth_to_df ( ground_truth_file : str , max_images : int = 200000 ) -> pd . DataFrame : \"\"\"Load and transforms COCO ground truth data to pd.DataFrame object. Args: ground_truth_file: Path of ground truth file. max_images: Maximum number of images to process. Returns: pd.DataFrame with df_annotations keys and image sizes. \"\"\" logger . info ( \"Loading Ground Truth File\" ) with open ( ground_truth_file ) as gt : coco_ground_truth = json . load ( gt ) if len ( coco_ground_truth [ \"images\" ]) > max_images : logger . warning ( f \"Number of images { len ( coco_ground_truth [ 'images' ]) } exceeds maximum: \" f \" { max_images } . \\n All the exceeding images will be ignored.\" ) logger . info ( \"Converting COCO Ground Truth to pd.DataFrame\" ) df_images = pd . DataFrame ( coco_ground_truth [ \"images\" ][: max_images ])[ [ \"id\" , \"file_name\" , \"width\" , \"height\" ] ] df_images = df_images . add_prefix ( \"img_\" ) df_annotations = pd . DataFrame ( coco_ground_truth [ \"annotations\" ]) # Replace label with category name categories = { x [ \"id\" ]: x [ \"name\" ] for x in coco_ground_truth [ \"categories\" ]} df_annotations [ \"category\" ] = df_annotations [ \"category_id\" ] . replace ( categories ) # Add bbox columns bbox_columns = [ \"col_left\" , \"row_top\" , \"width\" , \"height\" ] df_annotations [ bbox_columns ] = pd . DataFrame ( df_annotations . bbox . tolist (), index = df_annotations . index ) # Filter columns by name column_names = [ \"image_id\" , \"area\" , \"id\" , \"category\" ] + bbox_columns if \"iscrowd\" in df_annotations . columns : column_names . append ( \"iscrowd\" ) # Join with images df_annotations = df_annotations [ column_names ] . join ( df_images . set_index ( \"img_id\" ), how = \"inner\" , on = \"image_id\" ) return df_annotations","title":"coco_ground_truth_to_df()"},{"location":"reference/core/utils/#pyodi.core.utils.load_coco_ground_truth_from_StringIO","text":"Returns COCO object from StringIO. Parameters: Name Type Description Default string_io TextIO IO stream in text mode. required Returns: Type Description COCO COCO object. Source code in pyodi/core/utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def load_coco_ground_truth_from_StringIO ( string_io : TextIO ) -> COCO : \"\"\"Returns COCO object from StringIO. Args: string_io: IO stream in text mode. Returns: COCO object. \"\"\" coco_ground_truth = COCO () coco_ground_truth . dataset = json . load ( string_io ) coco_ground_truth . createIndex () return coco_ground_truth","title":"load_coco_ground_truth_from_StringIO()"},{"location":"reference/plots/boxes/","text":"get_centroids_heatmap ( df , n_rows = 9 , n_cols = 9 ) Returns centroids heatmap. Parameters: Name Type Description Default df DataFrame DataFrame with annotations. required n_rows int Number of rows. 9 n_cols int Number of columns. 9 Returns: Type Description np . ndarray Centroids heatmap. With shape ( n_rows , n_cols ). Source code in pyodi/plots/boxes.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def get_centroids_heatmap ( df : DataFrame , n_rows : int = 9 , n_cols : int = 9 ) -> np . ndarray : \"\"\"Returns centroids heatmap. Args: df: DataFrame with annotations. n_rows: Number of rows. n_cols: Number of columns. Returns: Centroids heatmap. With shape (`n_rows`, `n_cols`). \"\"\" rows = df [ \"row_centroid\" ] / df [ \"img_height\" ] cols = df [ \"col_centroid\" ] / df [ \"img_width\" ] heatmap = np . zeros (( n_rows , n_cols )) for row , col in zip ( rows , cols ): heatmap [ int ( row * n_rows ), int ( col * n_cols )] += 1 return heatmap plot_heatmap ( heatmap , title = '' , show = True , output = None , output_size = ( 1600 , 900 )) Plots heatmap figure. Parameters: Name Type Description Default heatmap np . ndarray Heatmap (2D array) data to plot. required title str Title of the figure. Defaults to \"\". '' show bool Whether to show results or not. Defaults to True. True output Optional [ str ] Results will be saved under output dir. Defaults to None. None output_size Tuple [ int , int ] Size of the saved images when output is defined. Defaults to (1600, 900). (1600, 900) Returns: Type Description go . Figure Heatmap figure. Source code in pyodi/plots/boxes.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def plot_heatmap ( heatmap : np . ndarray , title : str = \"\" , show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), ) -> go . Figure : \"\"\"Plots heatmap figure. Args: heatmap: Heatmap (2D array) data to plot. title: Title of the figure. Defaults to \"\". show: Whether to show results or not. Defaults to True. output: Results will be saved under `output` dir. Defaults to None. output_size: Size of the saved images when output is defined. Defaults to (1600, 900). Returns: Heatmap figure. \"\"\" fig = go . Figure ( data = go . Heatmap ( z = heatmap )) fig . update_layout ( title_text = title , title_font_size = 20 ) fig . update_xaxes ( showticklabels = False ) fig . update_yaxes ( showticklabels = False ) if show : fig . show () if output : save_figure ( fig , title , output , output_size ) return fig","title":"boxes"},{"location":"reference/plots/boxes/#pyodi.plots.boxes.get_centroids_heatmap","text":"Returns centroids heatmap. Parameters: Name Type Description Default df DataFrame DataFrame with annotations. required n_rows int Number of rows. 9 n_cols int Number of columns. 9 Returns: Type Description np . ndarray Centroids heatmap. With shape ( n_rows , n_cols ). Source code in pyodi/plots/boxes.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def get_centroids_heatmap ( df : DataFrame , n_rows : int = 9 , n_cols : int = 9 ) -> np . ndarray : \"\"\"Returns centroids heatmap. Args: df: DataFrame with annotations. n_rows: Number of rows. n_cols: Number of columns. Returns: Centroids heatmap. With shape (`n_rows`, `n_cols`). \"\"\" rows = df [ \"row_centroid\" ] / df [ \"img_height\" ] cols = df [ \"col_centroid\" ] / df [ \"img_width\" ] heatmap = np . zeros (( n_rows , n_cols )) for row , col in zip ( rows , cols ): heatmap [ int ( row * n_rows ), int ( col * n_cols )] += 1 return heatmap","title":"get_centroids_heatmap()"},{"location":"reference/plots/boxes/#pyodi.plots.boxes.plot_heatmap","text":"Plots heatmap figure. Parameters: Name Type Description Default heatmap np . ndarray Heatmap (2D array) data to plot. required title str Title of the figure. Defaults to \"\". '' show bool Whether to show results or not. Defaults to True. True output Optional [ str ] Results will be saved under output dir. Defaults to None. None output_size Tuple [ int , int ] Size of the saved images when output is defined. Defaults to (1600, 900). (1600, 900) Returns: Type Description go . Figure Heatmap figure. Source code in pyodi/plots/boxes.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def plot_heatmap ( heatmap : np . ndarray , title : str = \"\" , show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), ) -> go . Figure : \"\"\"Plots heatmap figure. Args: heatmap: Heatmap (2D array) data to plot. title: Title of the figure. Defaults to \"\". show: Whether to show results or not. Defaults to True. output: Results will be saved under `output` dir. Defaults to None. output_size: Size of the saved images when output is defined. Defaults to (1600, 900). Returns: Heatmap figure. \"\"\" fig = go . Figure ( data = go . Heatmap ( z = heatmap )) fig . update_layout ( title_text = title , title_font_size = 20 ) fig . update_xaxes ( showticklabels = False ) fig . update_yaxes ( showticklabels = False ) if show : fig . show () if output : save_figure ( fig , title , output , output_size ) return fig","title":"plot_heatmap()"},{"location":"reference/plots/clustering/","text":"plot_clustering_results ( df_annotations , anchor_generator , show = True , output = None , output_size = ( 1600 , 900 ), centroid_color = None , title = None ) Plots cluster results in two different views, width vs height and area vs ratio. Parameters: Name Type Description Default df_annotations DataFrame COCO annotations generated DataFrame. required anchor_generator AnchorGenerator Anchor generator instance. required show Optional [ bool ] Whether to show the figure or not. Defaults to True. True output Optional [ str ] Output path folder. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) centroid_color Optional [ Tuple ] Plotly rgb color format for painting centroids. Defaults to None. None title Optional [ str ] Plot title and filename if output is not None. Defaults to None. None Source code in pyodi/plots/clustering.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def plot_clustering_results ( df_annotations : DataFrame , anchor_generator : AnchorGenerator , show : Optional [ bool ] = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), centroid_color : Optional [ Tuple ] = None , title : Optional [ str ] = None , ) -> None : \"\"\"Plots cluster results in two different views, width vs height and area vs ratio. Args: df_annotations: COCO annotations generated DataFrame. anchor_generator: Anchor generator instance. show: Whether to show the figure or not. Defaults to True. output: Output path folder. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). centroid_color: Plotly rgb color format for painting centroids. Defaults to None. title: Plot title and filename if output is not None. Defaults to None. \"\"\" if centroid_color is None : centroid_color = COLORS [ len ( df_annotations . category . unique ()) % len ( COLORS )] fig = make_subplots ( rows = 1 , cols = 2 , subplot_titles = [ \"Relative Log Scale vs Log Ratio\" , \"Scaled Width vs Scaled Height\" , ], ) plot_scatter_with_histograms ( df_annotations , x = \"log_level_scale\" , y = \"log_ratio\" , legendgroup = \"classes\" , show = False , colors = COLORS , histogram = False , fig = fig , ) cluster_grid = np . array ( np . meshgrid ( np . log ( anchor_generator . scales ), np . log ( anchor_generator . ratios )) ) . T . reshape ( - 1 , 2 ) fig . append_trace ( go . Scattergl ( x = cluster_grid [:, 0 ], y = cluster_grid [:, 1 ], mode = \"markers\" , legendgroup = \"centroids\" , name = \"centroids\" , marker = dict ( color = centroid_color , size = 10 , line = dict ( width = 2 , color = \"DarkSlateGrey\" ), ), ), row = 1 , col = 1 , ) plot_scatter_with_histograms ( df_annotations , x = \"scaled_width\" , y = \"scaled_height\" , show = False , colors = COLORS , legendgroup = \"classes\" , histogram = False , showlegend = False , fig = fig , col = 2 , ) for anchor_level in anchor_generator . base_anchors : anchor_level = get_df_from_bboxes ( anchor_level , input_bbox_format = \"corners\" , output_bbox_format = \"coco\" ) fig . append_trace ( go . Scattergl ( x = anchor_level [ \"width\" ], y = anchor_level [ \"height\" ], mode = \"markers\" , legendgroup = \"centroids\" , name = \"centroids\" , showlegend = False , marker = dict ( color = centroid_color , size = 10 , line = dict ( width = 2 , color = \"DarkSlateGrey\" ), ), ), row = 1 , col = 2 , ) fig [ \"layout\" ] . update ( title = title , xaxis2 = dict ( title = \"Scaled Width\" ), xaxis = dict ( title = \"Log Relative Scale\" ), yaxis2 = dict ( title = \"Scaled Height\" ), yaxis = dict ( title = \"Log Ratio\" ), ) if show : fig . show () if output : save_figure ( fig , \"clusters\" , output , output_size )","title":"clustering"},{"location":"reference/plots/clustering/#pyodi.plots.clustering.plot_clustering_results","text":"Plots cluster results in two different views, width vs height and area vs ratio. Parameters: Name Type Description Default df_annotations DataFrame COCO annotations generated DataFrame. required anchor_generator AnchorGenerator Anchor generator instance. required show Optional [ bool ] Whether to show the figure or not. Defaults to True. True output Optional [ str ] Output path folder. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) centroid_color Optional [ Tuple ] Plotly rgb color format for painting centroids. Defaults to None. None title Optional [ str ] Plot title and filename if output is not None. Defaults to None. None Source code in pyodi/plots/clustering.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def plot_clustering_results ( df_annotations : DataFrame , anchor_generator : AnchorGenerator , show : Optional [ bool ] = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), centroid_color : Optional [ Tuple ] = None , title : Optional [ str ] = None , ) -> None : \"\"\"Plots cluster results in two different views, width vs height and area vs ratio. Args: df_annotations: COCO annotations generated DataFrame. anchor_generator: Anchor generator instance. show: Whether to show the figure or not. Defaults to True. output: Output path folder. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). centroid_color: Plotly rgb color format for painting centroids. Defaults to None. title: Plot title and filename if output is not None. Defaults to None. \"\"\" if centroid_color is None : centroid_color = COLORS [ len ( df_annotations . category . unique ()) % len ( COLORS )] fig = make_subplots ( rows = 1 , cols = 2 , subplot_titles = [ \"Relative Log Scale vs Log Ratio\" , \"Scaled Width vs Scaled Height\" , ], ) plot_scatter_with_histograms ( df_annotations , x = \"log_level_scale\" , y = \"log_ratio\" , legendgroup = \"classes\" , show = False , colors = COLORS , histogram = False , fig = fig , ) cluster_grid = np . array ( np . meshgrid ( np . log ( anchor_generator . scales ), np . log ( anchor_generator . ratios )) ) . T . reshape ( - 1 , 2 ) fig . append_trace ( go . Scattergl ( x = cluster_grid [:, 0 ], y = cluster_grid [:, 1 ], mode = \"markers\" , legendgroup = \"centroids\" , name = \"centroids\" , marker = dict ( color = centroid_color , size = 10 , line = dict ( width = 2 , color = \"DarkSlateGrey\" ), ), ), row = 1 , col = 1 , ) plot_scatter_with_histograms ( df_annotations , x = \"scaled_width\" , y = \"scaled_height\" , show = False , colors = COLORS , legendgroup = \"classes\" , histogram = False , showlegend = False , fig = fig , col = 2 , ) for anchor_level in anchor_generator . base_anchors : anchor_level = get_df_from_bboxes ( anchor_level , input_bbox_format = \"corners\" , output_bbox_format = \"coco\" ) fig . append_trace ( go . Scattergl ( x = anchor_level [ \"width\" ], y = anchor_level [ \"height\" ], mode = \"markers\" , legendgroup = \"centroids\" , name = \"centroids\" , showlegend = False , marker = dict ( color = centroid_color , size = 10 , line = dict ( width = 2 , color = \"DarkSlateGrey\" ), ), ), row = 1 , col = 2 , ) fig [ \"layout\" ] . update ( title = title , xaxis2 = dict ( title = \"Scaled Width\" ), xaxis = dict ( title = \"Log Relative Scale\" ), yaxis2 = dict ( title = \"Scaled Height\" ), yaxis = dict ( title = \"Log Ratio\" ), ) if show : fig . show () if output : save_figure ( fig , \"clusters\" , output , output_size )","title":"plot_clustering_results()"},{"location":"reference/plots/common/","text":"plot_histogram ( df , column , title = None , xrange = None , yrange = None , xbins = None , histnorm = 'percent' , show = False , output = None , output_size = ( 1600 , 900 )) Plot histogram figure. Parameters: Name Type Description Default df DataFrame Data to plot. required column str DataFrame column to plot. required title Optional [ str ] Title of figure. Defaults to None. None xrange Optional [ Tuple [ int , int ]] Range in axis X. Defaults to None. None yrange Optional [ Tuple [ int , int ]] Range in axis Y. Defaults to None. None xbins Optional [ Dict [ str , Any ]] Width of X bins. Defaults to None. None histnorm Optional [ str ] Histnorm. Defaults to \"percent\". 'percent' show bool Whether to show the figure or not. Defaults to False. False output Optional [ str ] Output path folder. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) Returns: Type Description go . Figure Histogram figure. Source code in pyodi/plots/common.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def plot_histogram ( df : DataFrame , column : str , title : Optional [ str ] = None , xrange : Optional [ Tuple [ int , int ]] = None , yrange : Optional [ Tuple [ int , int ]] = None , xbins : Optional [ Dict [ str , Any ]] = None , histnorm : Optional [ str ] = \"percent\" , show : bool = False , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), ) -> go . Figure : \"\"\"Plot histogram figure. Args: df: Data to plot. column: DataFrame column to plot. title: Title of figure. Defaults to None. xrange: Range in axis X. Defaults to None. yrange: Range in axis Y. Defaults to None. xbins: Width of X bins. Defaults to None. histnorm: Histnorm. Defaults to \"percent\". show: Whether to show the figure or not. Defaults to False. output: Output path folder. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). Returns: Histogram figure. \"\"\" logger . info ( f \"Plotting { column } Histogram\" ) fig = go . Figure ( data = [ go . Histogram ( x = df [ column ], histnorm = histnorm , hovertext = df [ \"file_name\" ], xbins = xbins ) ] ) if xrange is not None : fig . update_xaxes ( range = xrange ) if yrange is not None : fig . update_yaxes ( range = yrange ) if title is None : title = f \" { column } histogram\" fig . update_layout ( title_text = title , title_font_size = 20 ) if show : fig . show () if output : save_figure ( fig , title , output , output_size ) return fig plot_scatter_with_histograms ( df , x = 'width' , y = 'height' , title = None , show = True , output = None , output_size = ( 1600 , 900 ), histogram = True , label = 'category' , colors = None , legendgroup = None , fig = None , row = 1 , col = 1 , xaxis_range = None , yaxis_range = None , histogram_xbins = None , histogram_ybins = None , kwargs ) Allows to compare the relation between two variables of your COCO dataset. Parameters: Name Type Description Default df DataFrame COCO annotations generated DataFrame. required x str Name of column that will be represented in x axis. Defaults to \"width\". 'width' y str Name of column that will be represented in y axis. Defaults to \"height\". 'height' title Optional [ str ] Plot name. Defaults to None. None show bool Whether to show the figure or not. Defaults to True. True output Optional [ str ] Output path folder. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) histogram bool Whether to draw a marginal histogram distribution of each axis or not. Defaults to True. True label str Name of the column with class information in df_annotations. Defaults to 'category'. 'category' colors Optional [ List ] List of rgb colors to use. If None default plotly colors are used. Defaults to None. None legendgroup Optional [ str ] When present legend is grouped by different categories (see https://plotly.com/python/legend/). None fig Optional [ go . Figure ] When figure is provided, trace is automatically added on it. Defaults to None. None row int Subplot row to use when fig is provided. Defaults to 1. 1 col int Subplot col to use when fig is provided. Defaults to 1. 1 xaxis_range Optional [ Tuple [ float , float ]] range of values for the histogram's horizontal axis None yaxis_range Optional [ Tuple [ float , float ]] range of values for the histogram's vertical axis None histogram_xbins Optional [ Dict [ str , Any ]] number of bins for the histogram's horizontal axis None histogram_ybins Optional [ Dict [ str , Any ]] number of bins for the histogram's vertical axis None Returns: Type Description go . Figure Plotly figure. Source code in pyodi/plots/common.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def plot_scatter_with_histograms ( df : DataFrame , x : str = \"width\" , y : str = \"height\" , title : Optional [ str ] = None , show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), histogram : bool = True , label : str = \"category\" , colors : Optional [ List ] = None , legendgroup : Optional [ str ] = None , fig : Optional [ go . Figure ] = None , row : int = 1 , col : int = 1 , xaxis_range : Optional [ Tuple [ float , float ]] = None , yaxis_range : Optional [ Tuple [ float , float ]] = None , histogram_xbins : Optional [ Dict [ str , Any ]] = None , histogram_ybins : Optional [ Dict [ str , Any ]] = None , ** kwargs : Any , ) -> go . Figure : \"\"\"Allows to compare the relation between two variables of your COCO dataset. Args: df: COCO annotations generated DataFrame. x: Name of column that will be represented in x axis. Defaults to \"width\". y: Name of column that will be represented in y axis. Defaults to \"height\". title: Plot name. Defaults to None. show: Whether to show the figure or not. Defaults to True. output: Output path folder. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). histogram: Whether to draw a marginal histogram distribution of each axis or not. Defaults to True. label: Name of the column with class information in df_annotations. Defaults to 'category'. colors: List of rgb colors to use. If None default plotly colors are used. Defaults to None. legendgroup: When present legend is grouped by different categories (see https://plotly.com/python/legend/). fig: When figure is provided, trace is automatically added on it. Defaults to None. row: Subplot row to use when fig is provided. Defaults to 1. col: Subplot col to use when fig is provided. Defaults to 1. xaxis_range: range of values for the histogram's horizontal axis yaxis_range: range of values for the histogram's vertical axis histogram_xbins: number of bins for the histogram's horizontal axis histogram_ybins: number of bins for the histogram's vertical axis Returns: Plotly figure. \"\"\" logger . info ( \"Plotting Scatter with Histograms\" ) if not fig : fig = make_subplots ( rows = 1 , cols = 1 ) classes = [( 0 , None )] if label in df : classes = list ( enumerate ( sorted ( df [ label ] . unique ()))) for i , c in classes : if c : filtered_df = df [ df [ label ] == c ] else : filtered_df = df scatter = go . Scattergl ( x = filtered_df [ x ], y = filtered_df [ y ], mode = \"markers\" , name = str ( c or \"Images Shape\" ), text = filtered_df [ \"img_file_name\" ], marker = dict ( color = colors [ i % len ( colors )] if colors else None ), legendgroup = f \"legendgroup_ { i } \" if legendgroup else None , ** kwargs , ) fig . add_trace ( scatter , row = row , col = col ) if histogram : fig . add_histogram ( x = df [ x ], name = f \" { x } distribution\" , yaxis = \"y2\" , marker = dict ( color = \"rgb(246, 207, 113)\" ), histnorm = \"percent\" , xbins = histogram_xbins , ) fig . add_histogram ( y = df [ y ], name = f \" { y } distribution\" , xaxis = \"x2\" , marker = dict ( color = \"rgb(102, 197, 204)\" ), histnorm = \"percent\" , ybins = histogram_ybins , ) fig . layout = dict ( xaxis = dict ( domain = [ 0 , 0.84 ], showgrid = False , zeroline = False , range = xaxis_range ), yaxis = dict ( domain = [ 0 , 0.83 ], showgrid = False , zeroline = False , range = yaxis_range ), xaxis2 = dict ( domain = [ 0.85 , 1 ], showgrid = False , zeroline = False , range = ( 0 , 100 ) ), yaxis2 = dict ( domain = [ 0.85 , 1 ], showgrid = False , zeroline = False , range = ( 0 , 100 ) ), ) if title is None : title = f \" { x } vs { y } \" fig . update_layout ( title_text = title , xaxis_title = f \" { x } \" , yaxis_title = f \" { y } \" , title_font_size = 20 ) if show : fig . show () if output : save_figure ( fig , title , output , output_size ) return fig save_figure ( figure , output_name , output_dir , output_size ) Saves figure into png image file. Parameters: Name Type Description Default figure go . Figure Figure to save. required output_name str Output filename. required output_dir str Output directory. required output_size Tuple [ int , int ] Size of saved image. required Source code in pyodi/plots/common.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def save_figure ( figure : go . Figure , output_name : str , output_dir : str , output_size : Tuple [ int , int ] ) -> None : \"\"\"Saves figure into png image file. Args: figure: Figure to save. output_name: Output filename. output_dir: Output directory. output_size: Size of saved image. \"\"\" output = str ( Path ( output_dir ) / ( output_name . replace ( \" \" , \"_\" ) + \".png\" )) figure . update_layout ( width = output_size [ 0 ], height = output_size [ 1 ]) figure . write_image ( output , engine = \"kaleido\" )","title":"common"},{"location":"reference/plots/common/#pyodi.plots.common.plot_histogram","text":"Plot histogram figure. Parameters: Name Type Description Default df DataFrame Data to plot. required column str DataFrame column to plot. required title Optional [ str ] Title of figure. Defaults to None. None xrange Optional [ Tuple [ int , int ]] Range in axis X. Defaults to None. None yrange Optional [ Tuple [ int , int ]] Range in axis Y. Defaults to None. None xbins Optional [ Dict [ str , Any ]] Width of X bins. Defaults to None. None histnorm Optional [ str ] Histnorm. Defaults to \"percent\". 'percent' show bool Whether to show the figure or not. Defaults to False. False output Optional [ str ] Output path folder. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) Returns: Type Description go . Figure Histogram figure. Source code in pyodi/plots/common.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def plot_histogram ( df : DataFrame , column : str , title : Optional [ str ] = None , xrange : Optional [ Tuple [ int , int ]] = None , yrange : Optional [ Tuple [ int , int ]] = None , xbins : Optional [ Dict [ str , Any ]] = None , histnorm : Optional [ str ] = \"percent\" , show : bool = False , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), ) -> go . Figure : \"\"\"Plot histogram figure. Args: df: Data to plot. column: DataFrame column to plot. title: Title of figure. Defaults to None. xrange: Range in axis X. Defaults to None. yrange: Range in axis Y. Defaults to None. xbins: Width of X bins. Defaults to None. histnorm: Histnorm. Defaults to \"percent\". show: Whether to show the figure or not. Defaults to False. output: Output path folder. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). Returns: Histogram figure. \"\"\" logger . info ( f \"Plotting { column } Histogram\" ) fig = go . Figure ( data = [ go . Histogram ( x = df [ column ], histnorm = histnorm , hovertext = df [ \"file_name\" ], xbins = xbins ) ] ) if xrange is not None : fig . update_xaxes ( range = xrange ) if yrange is not None : fig . update_yaxes ( range = yrange ) if title is None : title = f \" { column } histogram\" fig . update_layout ( title_text = title , title_font_size = 20 ) if show : fig . show () if output : save_figure ( fig , title , output , output_size ) return fig","title":"plot_histogram()"},{"location":"reference/plots/common/#pyodi.plots.common.plot_scatter_with_histograms","text":"Allows to compare the relation between two variables of your COCO dataset. Parameters: Name Type Description Default df DataFrame COCO annotations generated DataFrame. required x str Name of column that will be represented in x axis. Defaults to \"width\". 'width' y str Name of column that will be represented in y axis. Defaults to \"height\". 'height' title Optional [ str ] Plot name. Defaults to None. None show bool Whether to show the figure or not. Defaults to True. True output Optional [ str ] Output path folder. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) histogram bool Whether to draw a marginal histogram distribution of each axis or not. Defaults to True. True label str Name of the column with class information in df_annotations. Defaults to 'category'. 'category' colors Optional [ List ] List of rgb colors to use. If None default plotly colors are used. Defaults to None. None legendgroup Optional [ str ] When present legend is grouped by different categories (see https://plotly.com/python/legend/). None fig Optional [ go . Figure ] When figure is provided, trace is automatically added on it. Defaults to None. None row int Subplot row to use when fig is provided. Defaults to 1. 1 col int Subplot col to use when fig is provided. Defaults to 1. 1 xaxis_range Optional [ Tuple [ float , float ]] range of values for the histogram's horizontal axis None yaxis_range Optional [ Tuple [ float , float ]] range of values for the histogram's vertical axis None histogram_xbins Optional [ Dict [ str , Any ]] number of bins for the histogram's horizontal axis None histogram_ybins Optional [ Dict [ str , Any ]] number of bins for the histogram's vertical axis None Returns: Type Description go . Figure Plotly figure. Source code in pyodi/plots/common.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def plot_scatter_with_histograms ( df : DataFrame , x : str = \"width\" , y : str = \"height\" , title : Optional [ str ] = None , show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), histogram : bool = True , label : str = \"category\" , colors : Optional [ List ] = None , legendgroup : Optional [ str ] = None , fig : Optional [ go . Figure ] = None , row : int = 1 , col : int = 1 , xaxis_range : Optional [ Tuple [ float , float ]] = None , yaxis_range : Optional [ Tuple [ float , float ]] = None , histogram_xbins : Optional [ Dict [ str , Any ]] = None , histogram_ybins : Optional [ Dict [ str , Any ]] = None , ** kwargs : Any , ) -> go . Figure : \"\"\"Allows to compare the relation between two variables of your COCO dataset. Args: df: COCO annotations generated DataFrame. x: Name of column that will be represented in x axis. Defaults to \"width\". y: Name of column that will be represented in y axis. Defaults to \"height\". title: Plot name. Defaults to None. show: Whether to show the figure or not. Defaults to True. output: Output path folder. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). histogram: Whether to draw a marginal histogram distribution of each axis or not. Defaults to True. label: Name of the column with class information in df_annotations. Defaults to 'category'. colors: List of rgb colors to use. If None default plotly colors are used. Defaults to None. legendgroup: When present legend is grouped by different categories (see https://plotly.com/python/legend/). fig: When figure is provided, trace is automatically added on it. Defaults to None. row: Subplot row to use when fig is provided. Defaults to 1. col: Subplot col to use when fig is provided. Defaults to 1. xaxis_range: range of values for the histogram's horizontal axis yaxis_range: range of values for the histogram's vertical axis histogram_xbins: number of bins for the histogram's horizontal axis histogram_ybins: number of bins for the histogram's vertical axis Returns: Plotly figure. \"\"\" logger . info ( \"Plotting Scatter with Histograms\" ) if not fig : fig = make_subplots ( rows = 1 , cols = 1 ) classes = [( 0 , None )] if label in df : classes = list ( enumerate ( sorted ( df [ label ] . unique ()))) for i , c in classes : if c : filtered_df = df [ df [ label ] == c ] else : filtered_df = df scatter = go . Scattergl ( x = filtered_df [ x ], y = filtered_df [ y ], mode = \"markers\" , name = str ( c or \"Images Shape\" ), text = filtered_df [ \"img_file_name\" ], marker = dict ( color = colors [ i % len ( colors )] if colors else None ), legendgroup = f \"legendgroup_ { i } \" if legendgroup else None , ** kwargs , ) fig . add_trace ( scatter , row = row , col = col ) if histogram : fig . add_histogram ( x = df [ x ], name = f \" { x } distribution\" , yaxis = \"y2\" , marker = dict ( color = \"rgb(246, 207, 113)\" ), histnorm = \"percent\" , xbins = histogram_xbins , ) fig . add_histogram ( y = df [ y ], name = f \" { y } distribution\" , xaxis = \"x2\" , marker = dict ( color = \"rgb(102, 197, 204)\" ), histnorm = \"percent\" , ybins = histogram_ybins , ) fig . layout = dict ( xaxis = dict ( domain = [ 0 , 0.84 ], showgrid = False , zeroline = False , range = xaxis_range ), yaxis = dict ( domain = [ 0 , 0.83 ], showgrid = False , zeroline = False , range = yaxis_range ), xaxis2 = dict ( domain = [ 0.85 , 1 ], showgrid = False , zeroline = False , range = ( 0 , 100 ) ), yaxis2 = dict ( domain = [ 0.85 , 1 ], showgrid = False , zeroline = False , range = ( 0 , 100 ) ), ) if title is None : title = f \" { x } vs { y } \" fig . update_layout ( title_text = title , xaxis_title = f \" { x } \" , yaxis_title = f \" { y } \" , title_font_size = 20 ) if show : fig . show () if output : save_figure ( fig , title , output , output_size ) return fig","title":"plot_scatter_with_histograms()"},{"location":"reference/plots/common/#pyodi.plots.common.save_figure","text":"Saves figure into png image file. Parameters: Name Type Description Default figure go . Figure Figure to save. required output_name str Output filename. required output_dir str Output directory. required output_size Tuple [ int , int ] Size of saved image. required Source code in pyodi/plots/common.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def save_figure ( figure : go . Figure , output_name : str , output_dir : str , output_size : Tuple [ int , int ] ) -> None : \"\"\"Saves figure into png image file. Args: figure: Figure to save. output_name: Output filename. output_dir: Output directory. output_size: Size of saved image. \"\"\" output = str ( Path ( output_dir ) / ( output_name . replace ( \" \" , \"_\" ) + \".png\" )) figure . update_layout ( width = output_size [ 0 ], height = output_size [ 1 ]) figure . write_image ( output , engine = \"kaleido\" )","title":"save_figure()"},{"location":"reference/plots/evaluation/","text":"plot_overlap_result ( df , max_bins = 30 , show = True , output = None , output_size = ( 1600 , 900 )) Generates plot for train config evaluation based on overlap. Parameters: Name Type Description Default df DataFrame COCO annotations generated DataFrame with overlap. required max_bins int Max bins to use in histograms. Defaults to 30. 30 show bool Whether to show the figure or not. Defaults to True. True output Optional [ str ] Output path folder. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) Source code in pyodi/plots/evaluation.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def plot_overlap_result ( df : DataFrame , max_bins : int = 30 , show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), ) -> None : \"\"\"Generates plot for train config evaluation based on overlap. Args: df: COCO annotations generated DataFrame with overlap. max_bins: Max bins to use in histograms. Defaults to 30. show: Whether to show the figure or not. Defaults to True. output: Output path folder. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). \"\"\" fig = make_subplots ( rows = 2 , cols = 2 , subplot_titles = ( \"Cumulative overlap distribution\" , \"Bounding Box Distribution\" , \"Scale and mean overlap\" , \"Log Ratio and mean overlap\" , ), ) fig . append_trace ( go . Histogram ( x = df [ \"overlaps\" ], histnorm = \"probability\" , cumulative_enabled = True , showlegend = False , ), row = 1 , col = 1 , ) fig . append_trace ( go . Scattergl ( x = df [ \"scaled_width\" ], y = df [ \"scaled_height\" ], mode = \"markers\" , showlegend = False , marker = dict ( color = df [ \"overlaps\" ], colorscale = \"Electric\" , cmin = 0 , cmax = 1 , showscale = True , colorbar = dict ( title = \"Overlap value\" , lenmode = \"fraction\" , len = 0.5 , y = 0.8 ), ), ), row = 1 , col = 2 , ) for i , column in enumerate ([ \"scaled_scale\" , \"log_scaled_ratio\" ], 1 ): fig . append_trace ( go . Histogram ( x = df [ column ], y = df [ \"overlaps\" ], histfunc = \"avg\" , nbinsx = max_bins , showlegend = False , ), row = 2 , col = i , ) fig [ \"layout\" ] . update ( title = \"Train config evaluation\" , xaxis = dict ( title = \"Overlap values\" ), xaxis2 = dict ( title = \"Scaled width\" ), xaxis3 = dict ( title = \"Scale\" ), xaxis4 = dict ( title = \"Log Ratio\" ), yaxis = dict ( title = \"Accumulated percentage\" ), yaxis2 = dict ( title = \"Scaled heigh\" ), yaxis3 = dict ( title = \"Mean overlap\" ), yaxis4 = dict ( title = \"Mean overlap\" ), legend = dict ( y = 0.5 ), ) if show : fig . show () if output : save_figure ( fig , \"overlap\" , output , output_size )","title":"evaluation"},{"location":"reference/plots/evaluation/#pyodi.plots.evaluation.plot_overlap_result","text":"Generates plot for train config evaluation based on overlap. Parameters: Name Type Description Default df DataFrame COCO annotations generated DataFrame with overlap. required max_bins int Max bins to use in histograms. Defaults to 30. 30 show bool Whether to show the figure or not. Defaults to True. True output Optional [ str ] Output path folder. Defaults to None. None output_size Tuple [ int , int ] Size of saved images. Defaults to (1600, 900). (1600, 900) Source code in pyodi/plots/evaluation.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def plot_overlap_result ( df : DataFrame , max_bins : int = 30 , show : bool = True , output : Optional [ str ] = None , output_size : Tuple [ int , int ] = ( 1600 , 900 ), ) -> None : \"\"\"Generates plot for train config evaluation based on overlap. Args: df: COCO annotations generated DataFrame with overlap. max_bins: Max bins to use in histograms. Defaults to 30. show: Whether to show the figure or not. Defaults to True. output: Output path folder. Defaults to None. output_size: Size of saved images. Defaults to (1600, 900). \"\"\" fig = make_subplots ( rows = 2 , cols = 2 , subplot_titles = ( \"Cumulative overlap distribution\" , \"Bounding Box Distribution\" , \"Scale and mean overlap\" , \"Log Ratio and mean overlap\" , ), ) fig . append_trace ( go . Histogram ( x = df [ \"overlaps\" ], histnorm = \"probability\" , cumulative_enabled = True , showlegend = False , ), row = 1 , col = 1 , ) fig . append_trace ( go . Scattergl ( x = df [ \"scaled_width\" ], y = df [ \"scaled_height\" ], mode = \"markers\" , showlegend = False , marker = dict ( color = df [ \"overlaps\" ], colorscale = \"Electric\" , cmin = 0 , cmax = 1 , showscale = True , colorbar = dict ( title = \"Overlap value\" , lenmode = \"fraction\" , len = 0.5 , y = 0.8 ), ), ), row = 1 , col = 2 , ) for i , column in enumerate ([ \"scaled_scale\" , \"log_scaled_ratio\" ], 1 ): fig . append_trace ( go . Histogram ( x = df [ column ], y = df [ \"overlaps\" ], histfunc = \"avg\" , nbinsx = max_bins , showlegend = False , ), row = 2 , col = i , ) fig [ \"layout\" ] . update ( title = \"Train config evaluation\" , xaxis = dict ( title = \"Overlap values\" ), xaxis2 = dict ( title = \"Scaled width\" ), xaxis3 = dict ( title = \"Scale\" ), xaxis4 = dict ( title = \"Log Ratio\" ), yaxis = dict ( title = \"Accumulated percentage\" ), yaxis2 = dict ( title = \"Scaled heigh\" ), yaxis3 = dict ( title = \"Mean overlap\" ), yaxis4 = dict ( title = \"Mean overlap\" ), legend = dict ( y = 0.5 ), ) if show : fig . show () if output : save_figure ( fig , \"overlap\" , output , output_size )","title":"plot_overlap_result()"}]}